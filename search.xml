<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[20180105 Hive查询partitioned view报错]]></title>
    <url>%2F2020%2F04%2F11%2F20180105-Hive%E6%9F%A5%E8%AF%A2partitioned-view%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[问题还原12hive (default)&gt; select * from view_xxx where dt=20180101 limit 10;FAILED: IndexOutOfBoundsException Index: 19, Size: 19 分析Hive在做列剪枝的时候，在生成的Operatore Tree中，SEL Operator收集当前Operator需要查询哪些字段，最终告诉底层的Table Scan Operator，只需要查询目标字段，而不是读取表的全部字段，从而完成列剪枝。 该bug最简单的复现场景是： 12345678910create table foo(`a` string) partitioned by (`b` string); create view bar partitioned on (b) asselect a,b from foo; select * from bar; select * from bar 生成的原始Operator Tree是： 1234567TS[0] |SEL[1] |SEL[2] |FS[3] SEL[1]用于确定从bar中实际上选择了哪些字段，在ColumnPrunerProcFactory.java调用bar的Table.getCols().get(index).getName()方法获取第index个列的名称时，由于getCols()不返回分区列，bar的分区列是b，除了b的字段是a，因此在这里返回的list是{“a”}， SEL[1]轮询查询的a,b字段，a的index是0，b的index是1，所以循环到b时，从{“a”}中获取index=1的值，报IndexOutOfBoundsException 解决方案 避免踩坑：不建分区视图（建分区不要指定partitioned on，底层表是分区表的话，hive会自动判断分区视图） 修复bug，新建一个list，将普通字段getCols()和分区字段getPartCols()都添加进去，从这个list中获取字段的name]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列化与反序列化一[简介]]]></title>
    <url>%2F2018%2F04%2F26%2F%E7%A8%8B%E5%BA%8F%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[序什么是序列化？ 序列化是将程序中的对象或是数据结构转换成持久化的存储（例如文件或缓存）或是可以通过网络传输的字节流，并且转换的结果可以在后续需要的时候，依据预先定义的序列化格式，能够重新构造出原始的对象，这个过程在语义上等同于对于原始对象的clone。 序列化就是把内存中的对象保存起来，反序列化就是把保存的东西给恢复成内存中的对象，本质上跟数据库的作用类似，只不过跟数据库相比，一方面序列化技术能存储更为复杂的数据类型，而数据库只能存储基本数据类型和数据结构，另一方面序列化对程序语言的依赖性更强，需要程序语言定义好序列化的东西是什么（对象的结构），在反序列的时候要知道这个元信息，才能将网络或是硬盘中的字节流转换为相应的对象。 需要注意的是，在面向对象编程语言中，序列化对象的结果中不包含这个对象所拥有的方法，只能描述对象是什么（field），不能描述对象能干什么（method）。 常见的序列化技术以一个java的Cat类为例，介绍常见序列化技术的基本用法 123456789101112131415161718192021package serialization;public class Cat &#123; private String name; private int age; public Cat() &#123;&#125; public Cat(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125;&#125; 一些序列化的技术得到大部分编程语言的支持，我称之为编程语言无关的序列化技术，常见的有： XMLJsonBsonMessagePackYAML其他一些序列化的技术只用在特定的编程语言中，以java为例，java中的序列化技术有： Java原生序列化jdk原生的序列化方案要求被序列化的类实现Serializable接口，该接口是标记接口，并无任何方法，仅用于提示JVM，先挖个坑，以后再学习java原生序列化的原理，先介绍如何使用。 实现了Serializable接口的Cat类： 12345678910111213141516171819202122232425262728293031323334package serialization;import java.io.Serializable;public class Cat implements Serializable &#123; /** * serialVersionUID 用于反序列化时验证版本是否一致 * 如果没有显示指定，jvm执行序列化时根据当前类hash值自动生成一个uid */ private static final long serialVersionUID = 1L ; private String name; private int age; public Cat() &#123;&#125; public Cat(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; @Override public String toString() &#123; return "Cat&#123;" + "name='" + name + '\'' + ", age=" + age + '&#125;'; &#125;&#125; 序列化的核心类是ObjectOutputStream，序列化代码： 12345678910111213141516package serialization;import java.io.FileOutputStream;import java.io.ObjectOutputStream;public class JdkSerialization &#123; public static void main(String[] args) throws Exception &#123; Cat tom = new Cat("tom", 3); FileOutputStream fos = new FileOutputStream("cat.out"); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(tom); oos.flush(); oos.close(); &#125;&#125; 查看cat.out: 123456aced 0005 7372 0011 7365 7269 616c 697a6174 696f 6e2e 4361 7400 0000 0000 00000102 0002 4900 0361 6765 4c00 046e 616d6574 0012 4c6a 6176 612f 6c61 6e67 2f537472 696e 673b 7870 0000 0003 7400 03746f6d 反序列化cat对象: 12345678910111213package serialization;import java.io.FileInputStream;import java.io.ObjectInputStream;public class JdkDeSerialization &#123; public static void main(String[] args) throws Exception &#123; FileInputStream fis = new FileInputStream("cat.out"); ObjectInputStream ois = new ObjectInputStream(fis); Cat tom = (Cat) ois.readObject(); System.out.println(tom); &#125;&#125; 输出结果为正确的cat对象: Cat{name=&#39;tom&#39;, age=3} Kryomaven依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.2&lt;/version&gt;&lt;/dependency&gt; 使用: 12345678910111213141516171819202122232425262728293031323334353637package serialization;import com.esotericsoftware.kryo.Kryo;import com.esotericsoftware.kryo.io.Input;import com.esotericsoftware.kryo.io.Output;import java.io.FileInputStream;import java.io.FileOutputStream;public class KryoTest &#123; private Kryo kryo; public KryoTest() &#123; this.kryo = new Kryo(); &#125; public void serialize(Cat cat) throws Exception &#123; Output output = new Output(new FileOutputStream("cat.out")); kryo.writeObject(output, cat); output.close(); &#125; public void deserialize() throws Exception &#123; Input input = new Input(new FileInputStream("cat.out")); Cat cat = kryo.readObject(input, Cat.class); input.close(); System.out.println(cat); &#125; public static void main(String[] args) throws Exception &#123; Cat tom = new Cat("tom", 3); KryoTest kryoTest = new KryoTest(); kryoTest.serialize(tom); kryoTest.deserialize(); &#125;&#125; Hessian2maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.caucho&lt;/groupId&gt; &lt;artifactId&gt;hessian&lt;/artifactId&gt; &lt;version&gt;4.0.7&lt;/version&gt;&lt;/dependency&gt; 使用 1234567891011121314151617181920212223242526272829303132333435package serialization;import com.caucho.hessian.io.Hessian2Input;import com.caucho.hessian.io.Hessian2Output;import com.caucho.hessian.io.SerializerFactory;import java.io.FileInputStream;import java.io.FileOutputStream;public class Hessian2Test &#123; private static SerializerFactory factory = new SerializerFactory(); public void serialize(Cat cat) throws Exception &#123; Hessian2Output output = new Hessian2Output(new FileOutputStream("cat.out")); output.setSerializerFactory(factory); output.writeObject(cat); output.close(); &#125; public void deserialize() throws Exception &#123; Hessian2Input input = new Hessian2Input(new FileInputStream("cat.out")); input.setSerializerFactory(factory); Cat cat = (Cat) input.readObject(); System.out.println(cat); &#125; public static void main(String[] args) throws Exception &#123; Cat tom = new Cat("tom", 3); Hessian2Test test = new Hessian2Test(); test.serialize(tom); test.deserialize(); &#125;&#125; ProtobufThriftAvro]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于最近]]></title>
    <url>%2F2018%2F04%2F24%2F%E5%85%B3%E4%BA%8E%E6%9C%80%E8%BF%91%2F</url>
    <content type="text"><![CDATA[测试]]></content>
      <tags>
        <tag>我, 其他</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20171207 数据校验失败]]></title>
    <url>%2F2017%2F12%2F07%2F20171207-%E6%95%B0%E6%8D%AE%E6%A0%A1%E9%AA%8C%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[问题还原Hive开启向量化执行时，查询结果不准确12345678910111213141516171819-- 1. result : 20171205 199107set hive.vectorized.execution.enabled=true;selectdt,sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as barfrom foowhere dt=20171205group by dt; -- 2. result : 20171205 0set hive.vectorized.execution.enabled=false;selectdt,sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as barfrom foowhere dt=20171205group by dt; 向量化执行打开时，id =’’ or id is null 的判断有bug 影响范围，同时满足一下条件时存在bug： ORC格式的表 向量化执行打开 判断条件存在 col = ‘’ or col is null 解决方案关闭向量化执行，hive-site.xml中设置hive.vectorized.execution.enabled=false]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20171205 查询LZO压缩的表，Map无法切割大文件]]></title>
    <url>%2F2017%2F12%2F05%2F20171205-%E6%9F%A5%E8%AF%A2LZO%E5%8E%8B%E7%BC%A9%E7%9A%84%E8%A1%A8%EF%BC%8CMap%E6%97%A0%E6%B3%95%E5%88%87%E5%89%B2%E5%A4%A7%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[问题还原查询LZO格式的表，如果单个HDFS文件很大，在读取表阶段，Hive没有切割大文件交给多个mapper处理，导致单个mapper处理很大的文件，速度极慢 复现12set hive.fetch.task.conversion=none;select * from ods_test_table where dt=&apos;20171201&apos;; 执行上述select语句，发现只有2个mapper处理: 查看HDFS文件 一个很大的数据文件(14.1G), 一个索引文件，一个空的标识文件。 分析先说如何解决该问题的结论： set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; hive.input.format的默认值是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, 将其设置为HiveInputFormat即可。 1.Hive 如何确定mapper数量？Hadoop体系下的MR作业使用InputFormat用来将HDFS文件分片(getSplit()方法)和对单标记录的读取为key-value对(getRecordReader())，所以InputFormat提供了数据切分的功能，inputformat是一个接口，对于每一种文件的输入格式都有一种对应的inputformat实现，如DeprecatedLzoTextInputFormat, OrcInputFormat, RCFileInputFormat,TextInputFormat. 可以认为，每种InputFormat实现的getSplit方法(该参数是JobConf对象)返回的InputSplit[] 数组，数组的元素个数就是读取该文件最终需要的mapper数. 2.Hive中的两种InputFormatHive默认的InputFormat是 CombineHiveInputFormat，这个inputformat用于当hdfs上存在小文件时，将多个小文件合并起来供一个mapper处理，从而节省mapper资源。一个备选是 HiveInputFormat ，相比前者没有合并小文件的功能。那这两个inputformat和前面说的各种文件类型所实现的各自的inputformat是什么关系呢？以HiveInputFormat为例，查看其getSplits()方法: 123456789101112131415161718for (Path dir : dirs) &#123; PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir); // 获取hdfs路径下，文件实际的input format class Class&lt;? extends InputFormat&gt; inputFormatClass = part.getInputFileFormatClass(); // 省略&#125; // 省略 currentInputFormatClass = inputFormatClass; // 将实际的input format 设置为当前的inputformat class// 省略 addSplitsForGroup(currentDirs, currentTableScan, newjob, getInputFormatFromCache(currentInputFormatClass, job), currentInputFormatClass, currentDirs.size()*(numSplits / dirs.length), currentTable, result); // 使用currentInputFormatClass获取splits，该方法最终调用文件类型实际的inputformat.getSplits()方法获取split数量 通过上面的代码可知，hive中的CombineHiveInputFormat和HiveInputFormat是对各种文件格式的inputformat的包装。最终生效的仍是不同文件格式的inputformat实现。 3.为什么HiveInputFormat能够切分大的LZO文件为小文件？上面的代码显示了HiveInputFormat最终调用文件类型所实现的inputformat的getSplits()方法，lzo的inputformat实现是 DeprecatedLzoTextInputFormat, 查看其getSplits()方法：123456789101112131415161718for (FileSplit fileSplit: splits) &#123; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); if (!LzoInputFormatCommon.isLzoFile(file.toString())) &#123; // non-LZO file, keep the input split as is. result.add(fileSplit); continue; &#125; // LZO file, try to split if the .index file was found LzoIndex index = indexes.get(file); if (index == null) &#123; throw new IOException(&quot;Index not found for &quot; + file); &#125; // 省略&#125; 上面代码的注释很清楚说明了，当.index文件存在，即lzo索引文件存在时，将split文件，因此使用DeprecatedLzoTextInputFormat原生的getSplits()方法，也就是设置hive的inputformat为HiveInputFormat能够拆分lzo文件。 4.为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？捋一下CombineHiveInputFormat类中的调用关系 CombineHiveInputFormat#getSplits() → CombineHiveInputFormat#getCombineSplits() → CombineFileInputFormatShim#getSplits() → HadoopShimsSecure#getSplits() 上面的CombineFileInputFormatShim是Hive为不同版本的Hadoop提供的称为HadoopShims的机制，用于兼容不同的Hadoop版本，相当于为不同的hadoop版本做一个一致的封装，上面的过程，还在Hive的代码范围内。 下面进入Hadoop代码： HadoopShimsSecure#getSplits() → org.apache.hadoop.mapred.lib.CombineFileInputFormat#getSplits() → org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits() 可以看到进入hadoop代码后，最终生效的是hadoop的CombineFileInputFormat的getSplits()方法，hive将拆分任务交给了hadoop。 继续上面的调用链： org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits() → org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getMoreSplits() 在getMoreSplits()方法中，调用 org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#isSplitable() 来判断当前路径的文件是否可切割，那么isSplitable()是如何判断文件是否可切割呢？ 通过 判断当前文件的压缩方式是不是 SplittableCompressionCodec 实例来决定当前文件是否可分! lzo的codec是LzopCodec，查看LzopCodec的声明: 12public class LzoCodec implements Configurable, CompressionCodec &#123;&#125; 并没有实现SplittableCompressionCodec，因此Hadoop判断该文件不可拆分！最终导致只有一个mapper处理. 一张《hadoop definitiion guide》的图： 显示lzo不支持split，但是lzo在实际的实现中，当存在index文件时，是可以拆分的，因此重写了isSplitable()方法，但是只有在调用了lzo实现的getSplits()方法才会发现该文件可以split，而通过hadoop的 CombineFileInputFormat实现，lzo的LzoCodec被判定为不可切分。 5. 为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？跟踪orc文件的读取过程，最终在isSplitable()方法中获取codec时，orc返回结果是null，而对于null，isSplitable()方法直接返回true。。。。。。 影响范围LZO格式的表，单个HDFS文件很大 解决方案进行下面的设置，使用lzo自身的getSplits和isSplitable实现 1set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
</search>
