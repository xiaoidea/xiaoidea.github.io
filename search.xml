<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[20190731 Kryo反序列化失败]]></title>
    <url>%2F2019%2F07%2F31%2F20190731-Kryo%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[问题还原SQl报错1234567891011121314151617181920212223242526272829303132Error: java.lang.RuntimeException: Failed to load plan: viewfs://xxx/-mr-10005/49f2f715-02e7-4f66-802b-0b080a9c6129/map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 124Serialization trace:sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:456) at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:303) at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:315) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:607) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:582) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:676) at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:169) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 124Serialization trace:sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670) at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readClass(SerializationUtilities.java:175) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:686) at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readObject(SerializationUtilities.java:200) at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializeObjectByKryo(SerializationUtilities.java:615) at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializePlan(SerializationUtilities.java:524) at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializePlan(SerializationUtilities.java:477) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:416) ... 13 more 解决过程问题产生的背景首先描述下上述错误产生的上下文： Hive是一个将SQL编译成MR程序的sql编译器，并不负责执行MR任务 一个典型的MR任务，需要一个实现org.apache.hadoop.mapred.Mapper接口的mapper类，一个实现org.apache.hadoop.mapred.Reducer的reducer类，还要一个控制类设置任务相关参数，并提交给集群执行 Hive也不例外，Hive提交任务的控制器是org.apache.hadoop.hive.ql.exec.mr.ExecDriver，Mapper类是统一的，都是org.apache.hadoop.hive.ql.exec.mr.ExecMapper，reducer类也是统一的，org.apache.hadoop.hive.ql.exec.mr.ExecReducer 下面都以mapper为例，reducer一样 ExecMapper通过内部的operator来区分具体每个sql的每个mapper干什么事情，内部持有一个AbstractMapOperator属性，执行时是调用AbstractMapOperator#process来干活 Hive内部使用MapWork来描述一个mapper的工作，（4）中的AbstractMapOperator具体干什么事情，是由MapWork决定的，hadoop集群上的节点是通过Utilities.getMapWork(job)获取一个MapWork对象 终于进入正题了，Utilities.getMapWork(job)是读取HDFS的map.xml文件，将文件内容反序列化为java对象，而map.xml是由Hive客户端写入HDFS上的，也就是说，HDFS作为媒介，保存Hive端编译好的mapper/reducer内部的执行细节，然后在yarn集群上每个container从hdfs上读取xml文件，将其反序列化为对象，然后执行。 map.xml和reducer.xml虽然以xml结尾，但这是历史原因，早期的hive用xml保存序列化的结果，但是（记得是从2.0开始）目前hive只有一种序列化方式，就是kryo，xml其实是二进制文件。这个xml文件比较难捕捉，任务执行完成后不管是否正常结束都会自动删除，只能用debug的方式在写完文件后，下载下来。下面放一个上面SQL的map.xml文件： map.xml 回头看下上面的错误就能看明白了，Hive客户端将map.xml文件写到HDFS上的一个路径，在hadoop端反序列化map.xml时，kryo反序列化失败了，导致hadoop端获取不到mapper任务 解决问题的尝试解决问题首先google，上述错误在google上信息很少，但也有几条。 https://github.com/EsotericSoftware/kryo/issues/307 这个issue跟我面临的问题基本一致，提到了kryo的仓库里，下面kyro的作者之一回复说可能是上一个序列化属性占了大量的空间导致取下一个kryo class id的时候取错了。但是在我的例子里，本身序列化的文件就不大，这条pass。 https://issues.apache.org/jira/browse/HIVE-11519 这是一个Hive的issue，里面提的问题也跟我遇到的一致，但是处于未解决状态，下面的回复引导了另一个issue：https://issues.apache.org/jira/browse/HIVE-12175 这个issue号称解决了Hive中大量的kryo序列化bug，但是fix version是2.0.0，我们用的hive版本是2.1.1，高于它，可以预见这个issue也没什么用 翻遍了google，有说hive on spark问题的，有说hive并行化执行bug的，有说集群中存在老的hive相关类的class的，能通过改配置调整的都试过全部无效 只能磕代码了。 第一个尝试是在本地复现反序列化的问题。将Hive编译的map.xml下载到本地，然后在Hive中新建一个测试类，尝试去反序列化该文件： 12345678910111213141516public class KryoDebug &#123; public static void main(String[] args) throws Exception &#123; abnormal(); &#125; public static void abnormal() throws Exception &#123; Kryo kryo = SerializationUtilities.borrowKryo(); String path = KryoDebug.class.getClassLoader().getResource(&quot;map.xml&quot;).getPath(); File mapFile = new File(path); FileInputStream inputStream = new FileInputStream(mapFile); MapWork mapWork = SerializationUtilities.deserializePlan(kryo, inputStream, MapWork.class); System.out.println(mapWork); &#125;&#125; 很遗憾，上面的代码执行是没问题的，在本地无法复现反序列化的问题，为了进一步确定本地没问题，将出错的SQL使用本地模式执行： 12345678SET mapreduce.framework.name=local;drop table if exists temp.xxx_debug2;create table temp.xxx_debug2 stored as orc asselectto_json(price_promotion)from ods_xxxwhere dt =&apos;20190722&apos; and hour=20; 使用本地模式执行上述SQL也是可以正常执行，因此排除了SQL bug、Hive bug、Hadoop bug等问题，怀疑是集群环境和本地环境不一致。 第二个尝试是去测试集群中各类的版本是否和Hive端一致. 首先是验证反序列的MapWork客户端和集群是否一致：在集群上跑输出来源和字段 123456789101112if (planClass.equals(MapWork.class)) &#123; String path = &quot;org.apache.hadoop.hive.ql.plan.MapWork&quot;; ClassLoader classLoader = Utilities.getSessionSpecifiedClassLoader(); LOG.info(&quot;MapWork反序列化:&quot; + classLoader.getResource(path.replace(&apos;.&apos;, &apos;/&apos;) + &quot;.class&quot;).toString()); Class&lt;MapWork&gt; clazz = MapWork.class; Field[] fields = clazz.getDeclaredFields(); StringBuilder sb = new StringBuilder(); for (Field field : fields) &#123; sb.append(field.getName()).append(&quot;###&quot;); &#125; LOG.info(&quot;MapWork反序列化属性:&quot; + sb.toString());&#125; 结果122019-07-31 17:42:25,266 INFO [main] org.apache.hadoop.hive.ql.exec.SerializationUtilities: MapWork反序列化:jar:file:/data5/hadoop2x/nm-local-dir/usercache/qhstats/appcache/application_1564438364333_147100/filecache/10/job.jar/job.jar!/org/apache/hadoop/hive/ql/plan/MapWork.class2019-07-31 17:42:25,266 INFO [main] org.apache.hadoop.hive.ql.exec.SerializationUtilities: MapWork反序列化属性:pathToAliases###pathToPartitionInfo###aliasToWork###aliasToPartnInfo###nameToSplitSample###bucketedColsByDirectory###sortedColsByDirectory###tmpHDFSPath###tmpPathForPartitionPruning###inputformat###indexIntermediateFile###numMapTasks###maxSplitSize###minSplitSize###minSplitSizePerNode###minSplitSizePerRack###samplingType###SAMPLING_ON_PREV_MR###SAMPLING_ON_START###leftInputJoin###baseSrc###mapAliases###mapperCannotSpanPartns###inputFormatSorted###useBucketizedHiveInputFormat###dummyTableScan###eventSourceTableDescMap###eventSourceColumnNameMap###eventSourceColumnTypeMap###eventSourcePartKeyExprMap###doSplitsGrouping###vectorizedRowBatch###includedBuckets###llapIoDesc###$assertionsDisabled### 可以看到class来自hive提交上去的jar包，比较了属性和本地class属性比较，也完全一致。排除了集群classpath下MapWork类冲突的问题 其次验证kryo版本是否一致，验证的结果是集群上使用的kryo也是来自hive提上去的。排除kryo class类冲突问题。 第三个尝试是针对反序列化错误的字段： sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)在反序列化sortedColsByDirectory字段发生错误，在com.esotericsoftware.kryo.pool.KryoFactory#create中让kryo不去序列化该字段： 重新编译hive后执行还是不行，包另一个字段出错了，而且这种方式是不可取的，不能说不序列化一个字段就不序列化了，sortedColsByDirectory在一些SQL中有用。 第四个尝试是怀疑SQL中用的to_json UDF有问题，UDF包中有hive相关类，在这里的尝试是将该UDF中所有用到的class，依赖全部改为provided（除了hive和hadoop中不存在的class），结果居然还是报同样的错。 第五个尝试是怀疑jdk版本的问题，hiveclient的jdk是8u60，hadoop集群jdk是8u131，于是将客户端jdk版本升级到8u131，仍然报错。 上述尝试全部失败后，实在没思路了，有一周时间没继续研究这个问题。 找到新的切入点既然是kryo反序列化的问题，还是得从kryo入手。 kyro本身在序列化的时候，可以通过执行下面指令将序列化的详细过程输出到stdout中 Log.TRACE(); 在Hive中加入上述代码，重新编译Hive并执行SQL，输出的结果大概是这样： 从错误日志： Encountered unregistered class ID: 124可以知道是124号class反序列化异常，看下124号序列话的是什么： 是org.apache.hadoop.hive.ql.plan.TableScanDesc，于是猜测是该类未在序列化时注册，于是在hive中注册kyro时，手动注册该类： org.apache.hadoop.hive.ql.exec.SerializationUtilities 重新编译并执行，问题依然存在，此时已经处于崩溃边缘。 看来从序列化日志是看不出什么东西了，那就从反序列日志入手： 本地的反序列日志容易获得，只需在本地运行上面的KryoDebug时，加上Log.TRACE();但是在集群上跑的时候，加上Log.TRACE();并不能在集群日志上打出反序列化日志，这是为什么？ 研究了一阵发现集群日志只会显示LOG输出的日志，kyro的序列化日志是直接输出到标准输出的，而stdout输出的东西不会显示在集群日志上，我又没办法登上集群机器上看标准输出的东西，最后想了个取巧的方案，将标准输出“移植”到LOG中： 1234567891011121314151617181920212223242526// 这个类用于将标准输出转移到LOG里public class CustomOutputStream extends OutputStream &#123; Logger logger; StringBuilder stringBuilder; public CustomOutputStream(Logger logger) &#123; this.logger = logger; stringBuilder = new StringBuilder(); &#125; @Override public final void write(int i) throws IOException &#123; char c = (char) i; if(c == &apos;\r&apos; || c == &apos;\n&apos;) &#123; if(stringBuilder.length() &gt; 0) &#123; logger.info(stringBuilder.toString()); stringBuilder = new StringBuilder(); &#125; &#125; else stringBuilder.append(c); &#125;&#125; // 全局设置一次重定向stdoutSystem.setOut(new PrintStream(new CustomOutputStream(LOG))); 这时候集群日志上终于可以显示Kryo日志了： 将集群上的kryo反序列化日志（失败的）和我本机的反序列化日志（成功的）进行对比，果然发现不同： 本地日志 集群日志对比两个日志，发现从Field _parent: class org.codehaus.jackson.sym.BytesToNameCanonicalizer这一行为分割，前面的日志两者一样，后面的日志开始不一样。 于是怀疑是这个类的问题，这是jackson里的类，udf中用到的。但是之前已经尝试过将udf中的类改为provided，并没有用。突然想起来会不会hive中的jackson和hadoop中的jackson冲突，让hadoop团队查了下hadoop中的jackson版本是1.9.13，然后我查了下hive的jackson版本，是1.9.2. 最后一次尝试！ 将hive中的jackson版本改为1.9.13，重新编译hive，执行sql。amazing happens！成功了。 所以最终定位到的问题是hive和hadoop中存在jackson类冲突，而udf用到了该类，导致在序列化时，用的hive端的jackson（1.9.2），而在反序列化时，用的hadoop端的jackson（1.9.13），至于为什么反序列时不用1.9.2的jackson，这谁tm知道呢，这就是同名类加载顺序不确定性恶心的地方了。 问题总结这是我处理过的最为棘手的一个Hive问题。难点在于： 定位问题难，kryo反序列报错，这个错可能导致的原因太多了，而且更为关键的是我本地无法复现反序列化错误。 集群上调试困难，代码是跑在hadoop集群上报错的，而我无法debug hadoop集群代码（毕竟你不能在公司生产集群上打个断点调试）收获是学习了kryo相关知识。 教训是class冲突真特么难分析，终于知道为什么公司强制每次push代码都检测工程依赖有没有重复的class了，万一因为class冲突导致错误是真的难发现。 而在我们的HIVE SQL运行环境里，似乎类冲突很难避免，因为hadoop/hive/udf jar都有各自的依赖包，在执行sql时，这些依赖搅成一团，难保不冲突。 该怎么做尽量减少这类问题？ UDF pom里绝对绝对不能随意添加依赖，在添加之前必须检测hive和hadoop中是否已经存在该依赖！如果都不存在，可以添加，如果有一个存在，尽量与已经存在的版本保持一致]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20190523 关于HiveCli启动慢原因的调查]]></title>
    <url>%2F2019%2F05%2F23%2F20190523-%E5%85%B3%E4%BA%8EHiveCli%E5%90%AF%E5%8A%A8%E6%85%A2%E5%8E%9F%E5%9B%A0%E7%9A%84%E8%B0%83%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[场景HiveClient 有时候会启动非常慢，具体表现是进入hive时当终端输出 Logging initialized using configuration in file:/home/q/hive/apache-hive-2.1.1-qhstats-bin/conf/hive-log4j2.properties Async: false 会隔上几秒甚至几十秒甚至几分钟才出现 hive (default)&gt;然后才能正常使用Hive 调查1. 打开debug日志查看启动Hive时卡在哪个步骤了 sudo -uqhstats hive_211 –hiveconf hive.root.logger=debug,console 观察启动后的日志 红框中两行日志相隔了一分多钟。 下方的日志很明显，是在创建HDFS目录。 所以元凶就是HDFS的问题，集群namenode响应慢。 2. 深入了解下Hive启动时需要创建哪些临时目录12345678910111213/** * Create dirs &amp; session paths for this session: * 1. HDFS scratch dir * 2. Local scratch dir * 3. Local downloaded resource dir * 4. HDFS session path * 5. hold a lock file in HDFS session dir to indicate the it is in use * 6. Local session path * 7. HDFS temp table space * @param userName * @throws IOException */private void createSessionDirs(String userName) throws IOException &#123; HDFS 临时写目录（存放执行过程中hdfs临时文件） 本地临时写目录（local task需要本地写） 本地下载资源目录（如udf jar包） hdfs会话目录 hdfs锁文件 本地会话目录 hdfs临时表空间可以看到当client启动时，需要创建很多hdfs/本地临时目录供当前会话使用 结论hive client启动慢的原因是hadoop集群namenode响应慢。]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20190429 ORC表一个有趣的问题]]></title>
    <url>%2F2019%2F04%2F29%2F20190429-ORC%E8%A1%A8%E4%B8%80%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[（其实这并不是bug，只是一个现象，或者说好久没更新这个系列wiki，随便写写） 现象： 两张Hive表关联的时候，mapper阶段卡在某个进度很久不动。 为什么？ orc是一种压缩率特别高的文件格式。因为其列存储的特点，当某些列存在很多一样的值时，比如NULL，压缩率特别极其高。 上面关联的一张表，其条数有1亿多，但是文件只有不到30M大小。 造成什么后果？ hive根据输入文件大小决定分配多少mapper，上面那个不到30M的文件，很有可能被分给一个mapper，那这个mapper就要处理1亿多条数据。。。 你也许觉得文件很小，处理很快，实际上，读取文件确实很快，可是别忘了mapper阶段需要对输入文件按照key排序，造成一个mapper要排序1亿多条数据（惊不惊喜？） 于是，这个mapper stuck了 我该怎么办呢？ 备选方案1：你可以设置一个mapper处理数据量小小小，比如设置成1M？这样那个30M的文件，会被分给30个mapper处理，可是~如果需要join的另一个表文件很大很大，比如1T，那你算算另一个表要用多少mapper吧（更加惊喜了） 备选方案2：把那个30M的表换个格式，简单点就用文本格式吧~体积一下子就膨胀了，单个mapper处理的数据量就变小啦。 备选方案3: 从数据源想想为啥压缩率这么高，是不是数据有问题。 当你mapper卡住的时候，怎么确定是不是这篇wiki描述的问题？ step1~查看那个卡住的mapper，看看它在干啥. 下面这个spilling map output表示mapper在排序（而且，这时候mapper在用硬盘做排序~很慢） step2~往上翻翻日志，看看这个mapper读取的文件，然后看看这个文件是不是很小？它对应的表的条数是不是很多？ 如果是的话那就没错了~ 顺便提下，当mapper日志中出现“spilling map output”时，你可以增加mapreduce.task.io.sort.mb这个参数的值，调整排序内存的大小，减少spill disk的次数加快速度]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20180803 修复metastore挂了重启之后，hiveserver无法重连的问题]]></title>
    <url>%2F2018%2F08%2F03%2F20180803-%E4%BF%AE%E5%A4%8Dmetastore%E6%8C%82%E4%BA%86%E9%87%8D%E5%90%AF%E4%B9%8B%E5%90%8E%EF%BC%8Chiveserver%E6%97%A0%E6%B3%95%E9%87%8D%E8%BF%9E%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述当外库的metastore服务挂了，手动重启服务，hiveserver无法探测到服务变得可用而自动重连，必须重启hiveserver才能重连到metastore 解决方案 为了防止每次连接都要新建metastore client对象（一次tcp连接过程），我们将首次连接好后的metastore client对象缓存起来，以后使用直接拿缓存即可，但是当底层网络连接断开后，缓存中的连接对象变得不可用，这是导致故障的原因。 Hive开发者为了解决这个问题，实际上使用的metastore client是RetryingMetaStoreClient，该对象是真正的metastore client的动态代理，带有尝试功能，当首次连接失败后，调用metastore client的reconnect方法，重建底层tcp连接。 外库的metastore client为了限制哪些方法可用，本身已经做了一层jdk动态代理，没法再使用RetryingMetaStoreClient去动态代理一个被代理过的对象，但是可以换一个思路，模仿RetryingMetaStoreClient，增加重试功能。 解决 org.apache.hadoop.hive.metastore.HiveForeignMetaStoreProxy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName(); if (NOT_IMPLEMENTED_METHODS.contains(methodName)) throw new MetaException(methodName + &quot; on a foreign database is not implemented&quot;); if (!ALLOWED_METHODS.contains(methodName)) throw new MetaException(methodName + &quot; on a foreign database is not allowed&quot;); int retry = 0; String virtualDBName = (String) args[0]; try &#123; while (retry++ &lt; RETRY_COUNT) &#123; try &#123; if (MAPTABLE_METHODS.contains(methodName)) &#123; assert args.length != 0; String mappedDBName = foreignMapping.get(virtualDBName); assert mappedDBName != null; args[0] = mappedDBName; Object ret = method.invoke(foreignMSC, args); if (ret instanceof Table) &#123; ((Table) ret).setDbName(virtualDBName); &#125; return ret; &#125; else &#123; return method.invoke(foreignMSC, args); &#125; &#125; catch (Exception e) &#123; args[0] = virtualDBName; if (retry &gt;= RETRY_COUNT) &#123; throw e; &#125; else &#123; foreignMSC.reconnect(); // client重连 Thread.sleep(1500); &#125; &#125; &#125; &#125; catch (UndeclaredThrowableException e) &#123; throw e.getCause().getCause(); &#125; catch (InvocationTargetException e) &#123; Throwable underlying = e.getCause(); if ((underlying instanceof TApplicationException) || (underlying instanceof TProtocolException) || (underlying instanceof TTransportException)) &#123; throw underlying; &#125; else if ((underlying instanceof MetaException) &amp;&amp; underlying.getMessage().matches(&quot;JDO[a-zA-Z]*Exception&quot;)) &#123; throw underlying; &#125; else &#123; throw underlying; &#125; &#125; return null;&#125;]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20180628 Hive On Spark任务初始化失败]]></title>
    <url>%2F2018%2F06%2F28%2F20180628-Hive-On-Spark%E4%BB%BB%E5%8A%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[问题还原在Hive中指定spark引擎，提交SQL后出现错误： FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client. 分析1. Hive on spark部署方式根据 Hive on Spark: Getting Started ，Hive on spark在Hive 2.2.0之前和之后的版本部署方式不同，当前生产环境的hive是2.1.1，部署spark环境需要编译spark得到 spark-assembly*.jar，然后将jar包放在hive的lib目录下，就完成了部署。（注意，hive on spark并不需要在hive服务器上安装spark客户端） 2. 当服务器上部署了Spark客户端，并设置了SPARK_HOME环境变量算法同学需要使用spark客户端提交spark任务，在服务器上部署了spark客户端，版本是2.1.2，并设置了SPARK_HOME环境变量，而2.1.2版本的spark和2.1.1版本的hive是不兼容的（2.1.1版本pom文件中指定spark版本是1.6） Hive中使用spark提交任务部分代码片段如下：123456789101112131415161718192021222324252627282930313233343536if (sparkHome != null) &#123; // 如果sparkHome不为空，将sparkHome目录下的bin/spark-submit加入执行参数 argv.add(new File(sparkHome, &quot;bin/spark-submit&quot;).getAbsolutePath());&#125; else &#123; LOG.info(&quot;No spark.home provided, calling SparkSubmit directly.&quot;); argv.add(new File(System.getProperty(&quot;java.home&quot;), &quot;bin/java&quot;).getAbsolutePath()); if (master.startsWith(&quot;local&quot;) || master.startsWith(&quot;mesos&quot;) || master.endsWith(&quot;-client&quot;) || master.startsWith(&quot;spark&quot;)) &#123; String mem = conf.get(&quot;spark.driver.memory&quot;); if (mem != null) &#123; argv.add(&quot;-Xms&quot; + mem); argv.add(&quot;-Xmx&quot; + mem); &#125; String cp = conf.get(&quot;spark.driver.extraClassPath&quot;); if (cp != null) &#123; argv.add(&quot;-classpath&quot;); argv.add(cp); &#125; String libPath = conf.get(&quot;spark.driver.extraLibPath&quot;); if (libPath != null) &#123; argv.add(&quot;-Djava.library.path=&quot; + libPath); &#125; String extra = conf.get(DRIVER_OPTS_KEY); if (extra != null) &#123; for (String opt : extra.split(&quot;[ ]&quot;)) &#123; if (!opt.trim().isEmpty()) &#123; argv.add(opt.trim()); &#125; &#125; &#125; &#125; // 如果sparkHome为空，直接将org.apache.spark.deploy.SparkSubmit加入执行变量，此时，将会使用classpath下的该类提交任务，也就是lib目录下的spark jar argv.add(&quot;org.apache.spark.deploy.SparkSubmit&quot;);&#125; 上面的注释清楚的说明了提交spark任务途径的优先级： 如sparkHome设置，使用sparkHome/bin/spark-submit提交任务。 如果sparkHome没设置，使用org.apache.spark.deploy.SparkSubmit提交任务，该类在spark-assembly.jar中。 sparkHome的设置方式是： 1234567String sparkHome = conf.get(SPARK_HOME_KEY);if (sparkHome == null) &#123; sparkHome = System.getenv(SPARK_HOME_ENV);&#125;if (sparkHome == null) &#123; sparkHome = System.getProperty(SPARK_HOME_KEY);&#125; 优先级如下： 先取conf对象中的 spark.home 设置，可以在hive-site.xml中设置（当前未设置） 取 SPARK_HOME 环境变量 取 spark.home系统属性 所以，当设置了SPARK_HOME环境变量时，hive将使用服务器上部署的spark客户端提交任务，而spark客户端版本2.1.2和hive版本不兼容，导致了错误 解决为了保证hive on spark不受到其他spark版本影响，做了以下双重保证： 不设置SPARK_HOME环境变量 假如不小心设置了，在Hive中抹去SPARK_HOME环境变量 对于1， 将 job工程和机器上/etc/profile.d/z-hadoop.sh中的 export SPARK_HOME=/home/q/spark/spark-default 设置删除，取而代之的是，在job工程中封装函数，需要的时候再指定spark_home1234567# hive on spark 目前只支持spark1.6 spark-default版本是2.1.2# 为同时支持hive on spark和直接提交spark任务，封装此函数而不直接export SPARK_HOMEfunction SPARK_SUBMIT &#123; export SPARK_HOME=/home/q/spark/spark-default $&#123;SPARK_HOME&#125;/bin/spark-submit $@ unset SPARK_HOME&#125; 对于2，在hive-config.sh中，抹去SPARK_HOME]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列化与反序列化一[简介]]]></title>
    <url>%2F2018%2F04%2F26%2F%E7%A8%8B%E5%BA%8F%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[序什么是序列化？ 序列化是将程序中的对象或是数据结构转换成持久化的存储（例如文件或缓存）或是可以通过网络传输的字节流，并且转换的结果可以在后续需要的时候，依据预先定义的序列化格式，能够重新构造出原始的对象，这个过程在语义上等同于对于原始对象的clone。 序列化就是把内存中的对象保存起来，反序列化就是把保存的东西给恢复成内存中的对象，本质上跟数据库的作用类似，只不过跟数据库相比，一方面序列化技术能存储更为复杂的数据类型，而数据库只能存储基本数据类型和数据结构，另一方面序列化对程序语言的依赖性更强，需要程序语言定义好序列化的东西是什么（对象的结构），在反序列的时候要知道这个元信息，才能将网络或是硬盘中的字节流转换为相应的对象。 需要注意的是，在面向对象编程语言中，序列化对象的结果中不包含这个对象所拥有的方法，只能描述对象是什么（field），不能描述对象能干什么（method）。 常见的序列化技术以一个java的Cat类为例，介绍常见序列化技术的基本用法 123456789101112131415161718192021package serialization;public class Cat &#123; private String name; private int age; public Cat() &#123;&#125; public Cat(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125;&#125; 一些序列化的技术得到大部分编程语言的支持，我称之为编程语言无关的序列化技术，常见的有： XMLJsonBsonMessagePackYAML其他一些序列化的技术只用在特定的编程语言中，以java为例，java中的序列化技术有： Java原生序列化jdk原生的序列化方案要求被序列化的类实现Serializable接口，该接口是标记接口，并无任何方法，仅用于提示JVM，先挖个坑，以后再学习java原生序列化的原理，先介绍如何使用。 实现了Serializable接口的Cat类： 12345678910111213141516171819202122232425262728293031323334package serialization;import java.io.Serializable;public class Cat implements Serializable &#123; /** * serialVersionUID 用于反序列化时验证版本是否一致 * 如果没有显示指定，jvm执行序列化时根据当前类hash值自动生成一个uid */ private static final long serialVersionUID = 1L ; private String name; private int age; public Cat() &#123;&#125; public Cat(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; @Override public String toString() &#123; return "Cat&#123;" + "name='" + name + '\'' + ", age=" + age + '&#125;'; &#125;&#125; 序列化的核心类是ObjectOutputStream，序列化代码： 12345678910111213141516package serialization;import java.io.FileOutputStream;import java.io.ObjectOutputStream;public class JdkSerialization &#123; public static void main(String[] args) throws Exception &#123; Cat tom = new Cat("tom", 3); FileOutputStream fos = new FileOutputStream("cat.out"); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(tom); oos.flush(); oos.close(); &#125;&#125; 查看cat.out: 123456aced 0005 7372 0011 7365 7269 616c 697a6174 696f 6e2e 4361 7400 0000 0000 00000102 0002 4900 0361 6765 4c00 046e 616d6574 0012 4c6a 6176 612f 6c61 6e67 2f537472 696e 673b 7870 0000 0003 7400 03746f6d 反序列化cat对象: 12345678910111213package serialization;import java.io.FileInputStream;import java.io.ObjectInputStream;public class JdkDeSerialization &#123; public static void main(String[] args) throws Exception &#123; FileInputStream fis = new FileInputStream("cat.out"); ObjectInputStream ois = new ObjectInputStream(fis); Cat tom = (Cat) ois.readObject(); System.out.println(tom); &#125;&#125; 输出结果为正确的cat对象: Cat{name=&#39;tom&#39;, age=3} Kryomaven依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.2&lt;/version&gt;&lt;/dependency&gt; 使用: 12345678910111213141516171819202122232425262728293031323334353637package serialization;import com.esotericsoftware.kryo.Kryo;import com.esotericsoftware.kryo.io.Input;import com.esotericsoftware.kryo.io.Output;import java.io.FileInputStream;import java.io.FileOutputStream;public class KryoTest &#123; private Kryo kryo; public KryoTest() &#123; this.kryo = new Kryo(); &#125; public void serialize(Cat cat) throws Exception &#123; Output output = new Output(new FileOutputStream("cat.out")); kryo.writeObject(output, cat); output.close(); &#125; public void deserialize() throws Exception &#123; Input input = new Input(new FileInputStream("cat.out")); Cat cat = kryo.readObject(input, Cat.class); input.close(); System.out.println(cat); &#125; public static void main(String[] args) throws Exception &#123; Cat tom = new Cat("tom", 3); KryoTest kryoTest = new KryoTest(); kryoTest.serialize(tom); kryoTest.deserialize(); &#125;&#125; Hessian2maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.caucho&lt;/groupId&gt; &lt;artifactId&gt;hessian&lt;/artifactId&gt; &lt;version&gt;4.0.7&lt;/version&gt;&lt;/dependency&gt; 使用 1234567891011121314151617181920212223242526272829303132333435package serialization;import com.caucho.hessian.io.Hessian2Input;import com.caucho.hessian.io.Hessian2Output;import com.caucho.hessian.io.SerializerFactory;import java.io.FileInputStream;import java.io.FileOutputStream;public class Hessian2Test &#123; private static SerializerFactory factory = new SerializerFactory(); public void serialize(Cat cat) throws Exception &#123; Hessian2Output output = new Hessian2Output(new FileOutputStream("cat.out")); output.setSerializerFactory(factory); output.writeObject(cat); output.close(); &#125; public void deserialize() throws Exception &#123; Hessian2Input input = new Hessian2Input(new FileInputStream("cat.out")); input.setSerializerFactory(factory); Cat cat = (Cat) input.readObject(); System.out.println(cat); &#125; public static void main(String[] args) throws Exception &#123; Cat tom = new Cat("tom", 3); Hessian2Test test = new Hessian2Test(); test.serialize(tom); test.deserialize(); &#125;&#125; ProtobufThriftAvro]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Profile]]></title>
    <url>%2F2018%2F04%2F24%2FProfile%2F</url>
    <content type="text"><![CDATA[戴恒宇大数据开发工程师，数据仓库工程师 联系方式| Tel: 18512547673 | Email: dhytheone@163.com| My WebSite : http://dhy.party 技术栈 编程语言 Java 熟练 Shell/Python 日常运维脚本 后端开发 熟悉Spring/Spring Boot/MyBatis 熟悉常用并发编程组件 离线数据开发 熟悉离线数据仓库建模 Hive熟练，向社区反馈过多个bug，提交过patch被采纳 理解hadoop mapreduce/hdfs，能解决日常问题 实时数据开发 熟悉Flink 有建设实时数仓经验及实时数据应用案例 其他 有kafka/hbase/es使用经验 项目经验 离线数据开发、Hive二次开发与维护 离线数仓：负责BU“用户服务”主题域离线数仓开发，完成自底向上从ods层原始数据到报表层的数据开发。 Hive二次开发与维护 负责部门Hive集群的运维 二次开发Hive支持多metastore功能，解决日常hive bug，部分bug反馈社区，Apache Profile: hengyu.dai 数据系统开发 元数据系统 离线数据仓库元数据系统开发，管理离线数仓中的表资源，主要功能包括：表的多维度查询，血缘关系，表的各方面监控。实现的关键词：SQL解析、Hive Hook。 即席查询 基于Hive Client开发的支持Hive/Presto/Hive on spark的即席查询系统，相比于底层使用hiveserver的开源HUE等产品，大幅提高稳定性。 实时数仓开发 从零搭建基于Flink的实时数仓体系，负责订单与资金模块的数据源接入，实时数据清洗，实时数据落地与应用（HDFS/HBase）。 实时数仓周边服务：Redis实时维度表关联服务，实时数据完整性保障。 数据应用 用户画像系统 数据赋能业务，通过离线数仓+实时计算清洗出的用户特征信息，定义用户画像，提供在线接口输出用户画像，用于各种基于用户身份的营销策略，例如发券、促销、新客优惠等。 实时大屏 重要活动时，实时计算活动期间的各项运营指标，在BI系统上实时更新，实时数据大屏。 工作经历 2016-至今 去哪儿网 目的地事业部 数据开发 教育经历 2009-2013 中山大学 软件学院 学士学位 2013-2016 南京大学 计算机科学与技术 硕士学位]]></content>
      <tags>
        <tag>我, 其他</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20180201 查询LZO表出现额外的NULL和乱码]]></title>
    <url>%2F2018%2F02%2F01%2F20180201-%E6%9F%A5%E8%AF%A2LZO%E8%A1%A8%E5%87%BA%E7%8E%B0%E9%A2%9D%E5%A4%96%E7%9A%84NULL%E5%92%8C%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[问题还原以下查询shotel_id字段为null的SQL（实际上表中不存在NULL值）123set hive.fetch.task.conversion=none;select * from dw_wwwwhere dt=&apos;20180115&apos; and shotel_id is null limit 10; 结果是 分析从以下三个可能原因入手： 1. LZO压缩导致的乱码和NULL最初怀疑是lzo的压缩和解压过程中引入了脏数据，验证过程如下： 跑一个分区的数据，数据不用lzo压缩，具体的SQL是：12345create table tmp_lzo_fix_bugrow format delimitedfields terminated by &apos;,&apos;asselect xxx -- select语句和生成乱码表的创建过程一样，使用20180115的数据 通过以上SQL，创建了一个用逗号分隔的文本格式的表tmp_lzo_fix_bug，其数据应该与dw_xxx的20180115分区一致。生成后执行SQL验证tmp_lzo_fix_bug是否存在NULL 123set hive.fetch.task.conversion=none;select * from tmp_lzo_fix_bug where shotel_id is null limit 10; 以上SQL查询结果为空，说明tmp_lzo_fix_bug表中不存在为NULL的数据，也即未压缩的文本文件中的数据是正常的。 既然未压缩前数据正常，那么接下来验证是不是压缩导致了数据损坏。将20180115分区的lzo文件 000000_0.lzo下载下来，并解压 使用lzop对其解压，将结果命名为decompress：1lzop -dv 000000_0.lzo -o decompress 接下来只需要验证decompress文件中是否存在NULL和乱码，就能证明是否是lzo带来的问题，由于解压后的文件非常大，不能直接用vim/cat打开。 由于在HIVE中，NULL在文本文件中的表示是 \N， 因此只需要看文件中的 \N 即可 1cat decompress | grep -a -F &quot;\N&quot; 执行以上命令没有任何数据匹配，说明压缩后的lzo文件被解压后没有带来异常数据。 因此，由LZO压缩导致的乱码和NULL被排除。 2.由hadoop集群未正确安装lzo压缩/解压软件或者lzo软件版本不对导致这个jira反映了类似的问题：https://issues.apache.org/jira/browse/HIVE-1138 作者最终发现是由于未正确配置 io.compression.codecs 导致了乱码和NULL，验证了这个设置，已经正确设置了lzo codec。 然而在本地(pg5)上，由上一步验证了pg5上的lzo安装是正确的，并且上面的出现NULL的测试SQL中必须要有 下面的设置，才会出现NULL和乱码的情况 1set hive.fetch.task.conversion=none; 上述设置是强制HIVE走hadoop集群而不是本地的fetch task直接从HDFS上取数据。因此怀疑是hadoop集群机器上的lzo软件问题。 如何验证以上猜测？ 强制SQL使用本地hadoop。 1SET mapreduce.framework.name=local; 加入上述设置使用本地hadoop来重新生成一份lzo压缩的表分区数据。 最终的验证结果是，这样生成的数据仍然存在NULL和乱码。 因此这个hadoop集群的原因也被排除。 3.Hive自身的原因，读取lzo文件过程引入脏数据受到这篇文章启发：https://github.com/twitter/hadoop-lzo/issues/49 这篇文章反映，Hive表没有设置正确的inputformat/outputformat同样会导致这个问题： 12STORED AS INPUTFORMAT &quot;com.hadoop.mapred.DeprecatedLzoTextInputFormat&quot; OUTPUTFORMAT &quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&quot; 如果未正确设置outputformat为org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat，那么HIVE会将lzo的index文件当成数据文件来读取，所以引入了脏数据（即index文件） 然而查看这个表的格式，发现inputformat和outputformat都是正确设置了的。 接下来对Hive读取数据的过程调试，发现在实际读取文件时，确实将index文件当做数据来读了： 尝试设置：1234set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;set hive.fetch.task.conversion=none;select * from dw_xxxwhere dt=&apos;20180115&apos; and shotel_id is null limit 10; 执行上述SQL果然没了NULL和乱码，跟踪读取文件： 确实没有将index文件加进来。 那么最终的原因找到了，CombineHiveInputFormat读取文件时错误的将索引文件当成数据文件读入了。 影响范围与解决方案影响范围必须满足一下两个条件： 表格式是lzo 表中数据存在索引文件，即在job中使用了lzo_index对数据建了索引 解决方案是，凡是在读取lzo的时候，都加上：1set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; 结合以前的lzo无法分割的issue，所有使用lzo表的地方都要加上上述设置 备注 既然lzo文件与CombineHiveInputFormat之间有这么多兼容问题，能不能将org.apache.hadoop.hive.ql.io.HiveInputFormat设置为Hive默认的hive.input.format？ 答：不能，CombineHiveInputFormat在绝大部分情况下是有利的，能够合并小文件输入交给一个mapper处理，如果设置为HiveInputFormat，部分job由于有太多小文件，小文件列表会占用过多内存，导致OOM. 2 . 那遇到LZO的表该怎么办？ 两个办法，要么加上set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;设置，要么将表改为orc格式，orc格式在空间效率和时间效率上都远优于lzo压缩的表。 因此，强烈建议表的格式采用orc！]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20180109 mapper超物理内存限制]]></title>
    <url>%2F2018%2F01%2F09%2F20180109-mapper%E8%B6%85%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[问题还原显示容器使用的内存(2.1G)已经超过物理内存限制(2G) 分析hadoop中关于mapper/reducer几个内存参数的设置。注：以下每个设置都存在两个等效的名字，前者是hadoop0.x和1.x版本的名称，已废弃，后者是hadoop2.x版本的名称，虽然hadoop为了兼容性前者依然有效，但是建议全部使用后者。 mapred.job.map.memory.mb 或 mapreduce.map.memory.mb该参数是Hadoop允许为一个mapper分配的最大内存上限，单位是M, 如果内存不够，将会报类似下面的错误： Containerpid=container_1406552545451_0009_01_000002,containerID=container_234132_0001_01_000001 is running beyond physical memory limits. Current usage: 569.1 MB of 512 MB physical memory used; 970.1 MB of 1.0 GB virtual memory used. Killing container. mapred.map.child.java.opts 或 mapreduce.map.java.opts该参数是对java进程的内存参数设置，常用来设置一个mapper进程最大的堆内存上限(-Xmx)，使用 ‘-Xmx数字M’设置，如果堆内存不够，将报下面的错误： Error: java.lang.RuntimeException: java.lang.OutOfMemoryError mapred.job.reduce.memory.mb 或 mapreduce.reduce.memory.mb与mapper类似，该设置是针对reducer的最大内存上限设置 mapred.reduce.child.java.opts 或 mapreduce.reduce.java.opts跟mapper类似，该设置是针对reduce进程最大堆内存上限设置。 这些设置之间有什么关系？上面说了两类设置，最大内存上限和最大堆内存上限。前者是针对container容器的设置，后者是针对java进程的JVM内存设置。java进程在container容器中运行， 所以理所应当的最大内存上限要大于最大堆内存上限，根据JVM内存模型，(最大内存 - 最大堆内存) 至少要大于 方法区 需要的内存大小。 建议 JVM最大堆内存 = Container最大内存 * 75% （当然，如果设置这两者相等，而运行中两者的值都超过了需要的内存大小，也不会报错） 应该如何设置？mapreduce.map.memory.mb=5120 mapreduce.reduce.memory.mb=8192 mapreduce.map.java.opts=’-Xmx8192M’ mapreduce.reduce.java.opts=’-Xmx8192M’ google上述设置发现这两项是CDH版本的Hadoop针对JVM内存的设置，在这里不知道有没有生效。（前面介绍的几项设置是hadoop标准版的设置） BTW:除了上面几项设置，还有一项设置能够在单个mapper处理的数据量较大时，加快mapper的速度。 mapreduce.task.io.sort.mb 该设置的默认值是100， 单位是M，作用是map阶段spill过程中，用于对mapper结果进行排序的内存大小（归并排序） 当内存不够时，会使用硬盘辅助排序，这就减慢了速度，必要时可以通过mapper的日志，如果发现在排序上耗时很多，可以将该设置增加到500. 不常用。 shuffle报错1234567891011121314151617Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56) at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46) at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:511) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:333) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193) 处理办法 Cause 原因：reduce会在map执行到一定比例启动多个fetch线程去拉取map的输出结果，放到reduce的内存、磁盘中，然后进行merge。当数据量大时，拉取到内存的数据就会引起OOM，所以此时要减少fetch占内存的百分比，将fetch的数据直接放在磁盘上。 有关参数：mapreduce.reduce.shuffle.memory.limit.percent 将mapreduce.reduce.shuffle.memory.limit.percent设置为0.15，执行成功（默认0.25）]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20180109 HS2查询SQL报NPE以及Explain SQL的ClassCastException]]></title>
    <url>%2F2018%2F01%2F09%2F20180109-HS2%E6%9F%A5%E8%AF%A2SQL%E6%8A%A5NPE%E4%BB%A5%E5%8F%8AExplain-SQL%E7%9A%84ClassCastException%2F</url>
    <content type="text"><![CDATA[问题还原一段SQL查询语句，在HiveCli上查询正常，在邮件服务器上通过HiveServer2查询报错. 报错信息12345678910111213141516171819202122232425262728java.lang.NullPointerException at org.apache.hadoop.hive.shims.Hadoop23Shims$1.listStatus(Hadoop23Shims.java:134) at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:217) at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:75) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getSplits(HadoopShimsSecure.java:319) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getCombineSplits(CombineHiveInputFormat.java:425) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:532) at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:518) at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557) at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:416) at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:138) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) 分析1. 从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat：1SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; 问题消失。 2. 错误是怎么触发的？下面是对这个bug的调试过程 首先发现，出错的位置是第4个job，如下图 使用explain语句查看第4个job是干什么的（直接explain的时候报错，遇到了一个 Correlation Optimizer的bug，将在以后说明这个问题，关闭 Correlation Optimizer以后再执行explain），explain的结果如下： 发现第4个job是一个MR过程，并且是由group by造成的MR，group by的聚合是count, sum, count, sum。对比原SQL，发现这个MR是最后的一个阶段。因此考虑将出错SQL内部的查询建成一张临时表，在临时表的基础上执行相应的聚合，查看会不会报错. 遗憾的是，将中间过程拆分后，不管是在HIVECLI 还是HiveServer2上执行都不报错。 上面的方法失败，只能远程调试HiveServer2了首先直接跟踪到出错的位置：上述while循环这段代码的意思是，扫描输入路径，将不是文件的东西，或者size为0的文件，或者schema以nullscan开头的文件，排除出输入文件。问题就发生在 stat.getPath().toUri().getScheme().equals(“nullscan”) 这个判断中，从截图可以看到，循环的位置 stat.getPath().toUri().getScheme() 值是null，那么调用equals方法就会报NPE错误了。作为对比，看一下使用HiveCli执行到这里时的情况： 从上图可以看到，使用HiveCli执行到这里时，stat.getPath().toUri().getScheme() 的值为hdfs。那么这时会判断 “hdfs”.equals(“nullscan”)，不会报错，并且下面的 it.remove()也不会执行到(判断这个文件是正常的文件，加入到当前MR job的) 到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）首先看一下导致上述bug的 “nullscan” 是什么， nullscan 应该是hadoop标识一个文件处于非正常状态，告诉job不要去扫描读取这个文件的标识。 上述发生错误的代码，Hive的本意是，调用listStats(job)方法获取需要扫描的文件列表（super.listStatus方法是Hadoop源码，不在Hive中），然后判断每个文件的schema是不是”nullscan”调试super.listStatus代码，发现传入方法时，获取的文件列表，schema确实是nullscan: 可是当该方法返回时，hadoop会将nullscan处理掉， 因此，这里Hive误以为返回值是带nullscan的schema，但是其实经过hadoop处理后，已经不是一个完整的URI了，因此获取schema结果是null，再使用equals方法，就报错。 以上是导致这个错误的直接原因。 进一步分析这不是这个bug的最终形态，因为如果仅仅在调用equals方法之前，加一个非Null判断，虽然能避免NPE，但无法真正的解决问题，本质上这个问题是一个MR job的输入路径错了，不该是nullscan的文件。 应该是一个正常的HDFS文件。 从上面的截图可以看到，通过HIVECLI执行时，输入文件是 LocatedFileStatus，其完整路径是 hdfs://xxx…. 对应一个HDFS的文件。 通过HiveServer2执行时，输入文件是nullscan://null/default…..，是一个错误的标识。 为什么HiveServer2执行时的输入路径不对？ 输入路径是在JobConf对象的properties中的mapred.input.dir配置的，查看hiveserver2提交当前作业的job对象，发现在提交给Hadoop执行时，其输入路径就已经给错了，下面是job配置文件中mapred.input.dir的值]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20180105 Hive查询partitioned view报错]]></title>
    <url>%2F2018%2F01%2F05%2F20180105-Hive%E6%9F%A5%E8%AF%A2partitioned-view%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[问题还原12hive (default)&gt; select * from view_xxx where dt=20180101 limit 10;FAILED: IndexOutOfBoundsException Index: 19, Size: 19 分析Hive在做列剪枝的时候，在生成的Operatore Tree中，SEL Operator收集当前Operator需要查询哪些字段，最终告诉底层的Table Scan Operator，只需要查询目标字段，而不是读取表的全部字段，从而完成列剪枝。 该bug最简单的复现场景是： 12345678910create table foo(`a` string) partitioned by (`b` string); create view bar partitioned on (b) asselect a,b from foo; select * from bar; select * from bar 生成的原始Operator Tree是： 1234567TS[0] |SEL[1] |SEL[2] |FS[3] SEL[1]用于确定从bar中实际上选择了哪些字段，在ColumnPrunerProcFactory.java调用bar的Table.getCols().get(index).getName()方法获取第index个列的名称时，由于getCols()不返回分区列，bar的分区列是b，除了b的字段是a，因此在这里返回的list是{“a”}， SEL[1]轮询查询的a,b字段，a的index是0，b的index是1，所以循环到b时，从{“a”}中获取index=1的值，报IndexOutOfBoundsException 解决方案 避免踩坑：不建分区视图（建分区不要指定partitioned on，底层表是分区表的话，hive会自动判断分区视图） 修复bug，新建一个list，将普通字段getCols()和分区字段getPartCols()都添加进去，从这个list中获取字段的name]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20171207 数据校验失败]]></title>
    <url>%2F2017%2F12%2F07%2F20171207-%E6%95%B0%E6%8D%AE%E6%A0%A1%E9%AA%8C%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[问题还原Hive开启向量化执行时，查询结果不准确12345678910111213141516171819-- 1. result : 20171205 199107set hive.vectorized.execution.enabled=true;selectdt,sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as barfrom foowhere dt=20171205group by dt; -- 2. result : 20171205 0set hive.vectorized.execution.enabled=false;selectdt,sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as barfrom foowhere dt=20171205group by dt; 向量化执行打开时，id =’’ or id is null 的判断有bug 影响范围，同时满足一下条件时存在bug： ORC格式的表 向量化执行打开 判断条件存在 col = ‘’ or col is null 解决方案关闭向量化执行，hive-site.xml中设置hive.vectorized.execution.enabled=false]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20171205 查询LZO压缩的表，Map无法切割大文件]]></title>
    <url>%2F2017%2F12%2F05%2F20171205-%E6%9F%A5%E8%AF%A2LZO%E5%8E%8B%E7%BC%A9%E7%9A%84%E8%A1%A8%EF%BC%8CMap%E6%97%A0%E6%B3%95%E5%88%87%E5%89%B2%E5%A4%A7%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[问题还原查询LZO格式的表，如果单个HDFS文件很大，在读取表阶段，Hive没有切割大文件交给多个mapper处理，导致单个mapper处理很大的文件，速度极慢 复现12set hive.fetch.task.conversion=none;select * from ods_test_table where dt=&apos;20171201&apos;; 执行上述select语句，发现只有2个mapper处理: 查看HDFS文件 一个很大的数据文件(14.1G), 一个索引文件，一个空的标识文件。 分析先说如何解决该问题的结论： set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; hive.input.format的默认值是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, 将其设置为HiveInputFormat即可。 1.Hive 如何确定mapper数量？Hadoop体系下的MR作业使用InputFormat用来将HDFS文件分片(getSplit()方法)和对单标记录的读取为key-value对(getRecordReader())，所以InputFormat提供了数据切分的功能，inputformat是一个接口，对于每一种文件的输入格式都有一种对应的inputformat实现，如DeprecatedLzoTextInputFormat, OrcInputFormat, RCFileInputFormat,TextInputFormat. 可以认为，每种InputFormat实现的getSplit方法(该参数是JobConf对象)返回的InputSplit[] 数组，数组的元素个数就是读取该文件最终需要的mapper数. 2.Hive中的两种InputFormatHive默认的InputFormat是 CombineHiveInputFormat，这个inputformat用于当hdfs上存在小文件时，将多个小文件合并起来供一个mapper处理，从而节省mapper资源。一个备选是 HiveInputFormat ，相比前者没有合并小文件的功能。那这两个inputformat和前面说的各种文件类型所实现的各自的inputformat是什么关系呢？以HiveInputFormat为例，查看其getSplits()方法: 123456789101112131415161718for (Path dir : dirs) &#123; PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir); // 获取hdfs路径下，文件实际的input format class Class&lt;? extends InputFormat&gt; inputFormatClass = part.getInputFileFormatClass(); // 省略&#125; // 省略 currentInputFormatClass = inputFormatClass; // 将实际的input format 设置为当前的inputformat class// 省略 addSplitsForGroup(currentDirs, currentTableScan, newjob, getInputFormatFromCache(currentInputFormatClass, job), currentInputFormatClass, currentDirs.size()*(numSplits / dirs.length), currentTable, result); // 使用currentInputFormatClass获取splits，该方法最终调用文件类型实际的inputformat.getSplits()方法获取split数量 通过上面的代码可知，hive中的CombineHiveInputFormat和HiveInputFormat是对各种文件格式的inputformat的包装。最终生效的仍是不同文件格式的inputformat实现。 3.为什么HiveInputFormat能够切分大的LZO文件为小文件？上面的代码显示了HiveInputFormat最终调用文件类型所实现的inputformat的getSplits()方法，lzo的inputformat实现是 DeprecatedLzoTextInputFormat, 查看其getSplits()方法：123456789101112131415161718for (FileSplit fileSplit: splits) &#123; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); if (!LzoInputFormatCommon.isLzoFile(file.toString())) &#123; // non-LZO file, keep the input split as is. result.add(fileSplit); continue; &#125; // LZO file, try to split if the .index file was found LzoIndex index = indexes.get(file); if (index == null) &#123; throw new IOException(&quot;Index not found for &quot; + file); &#125; // 省略&#125; 上面代码的注释很清楚说明了，当.index文件存在，即lzo索引文件存在时，将split文件，因此使用DeprecatedLzoTextInputFormat原生的getSplits()方法，也就是设置hive的inputformat为HiveInputFormat能够拆分lzo文件。 4.为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？捋一下CombineHiveInputFormat类中的调用关系 CombineHiveInputFormat#getSplits() → CombineHiveInputFormat#getCombineSplits() → CombineFileInputFormatShim#getSplits() → HadoopShimsSecure#getSplits() 上面的CombineFileInputFormatShim是Hive为不同版本的Hadoop提供的称为HadoopShims的机制，用于兼容不同的Hadoop版本，相当于为不同的hadoop版本做一个一致的封装，上面的过程，还在Hive的代码范围内。 下面进入Hadoop代码： HadoopShimsSecure#getSplits() → org.apache.hadoop.mapred.lib.CombineFileInputFormat#getSplits() → org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits() 可以看到进入hadoop代码后，最终生效的是hadoop的CombineFileInputFormat的getSplits()方法，hive将拆分任务交给了hadoop。 继续上面的调用链： org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits() → org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getMoreSplits() 在getMoreSplits()方法中，调用 org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#isSplitable() 来判断当前路径的文件是否可切割，那么isSplitable()是如何判断文件是否可切割呢？ 通过 判断当前文件的压缩方式是不是 SplittableCompressionCodec 实例来决定当前文件是否可分! lzo的codec是LzopCodec，查看LzopCodec的声明: 12public class LzoCodec implements Configurable, CompressionCodec &#123;&#125; 并没有实现SplittableCompressionCodec，因此Hadoop判断该文件不可拆分！最终导致只有一个mapper处理. 一张《hadoop definitiion guide》的图： 显示lzo不支持split，但是lzo在实际的实现中，当存在index文件时，是可以拆分的，因此重写了isSplitable()方法，但是只有在调用了lzo实现的getSplits()方法才会发现该文件可以split，而通过hadoop的 CombineFileInputFormat实现，lzo的LzoCodec被判定为不可切分。 5. 为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？跟踪orc文件的读取过程，最终在isSplitable()方法中获取codec时，orc返回结果是null，而对于null，isSplitable()方法直接返回true。。。。。。 影响范围LZO格式的表，单个HDFS文件很大 解决方案进行下面的设置，使用lzo自身的getSplits和isSplitable实现 1set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
</search>
