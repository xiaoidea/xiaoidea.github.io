<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>20171205 查询LZO压缩的表，Map无法切割大文件</title>
    <url>/2017/12/05/20171205-%E6%9F%A5%E8%AF%A2LZO%E5%8E%8B%E7%BC%A9%E7%9A%84%E8%A1%A8%EF%BC%8CMap%E6%97%A0%E6%B3%95%E5%88%87%E5%89%B2%E5%A4%A7%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>查询LZO格式的表，如果单个HDFS文件很大，在读取表阶段，Hive没有切割大文件交给多个mapper处理，导致单个mapper处理很大的文件，速度极慢</p>
<p>复现<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from ods_test_table where dt=&apos;20171201&apos;;</span><br></pre></td></tr></table></figure></p>
<p>执行上述select语句，发现只有2个mapper处理:</p>
<p><img src="/uploads/Jietu20200411-112858.jpg" alt></p>
<p>查看HDFS文件</p>
<p><img src="/uploads/Jietu20200411-113505.jpg" alt></p>
<p>一个很大的数据文件(14.1G), 一个索引文件，一个空的标识文件。</p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>先说如何解决该问题的结论：</p>
<ul>
<li>set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</li>
</ul>
<p>hive.input.format的默认值是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, 将其设置为HiveInputFormat即可。</p>
<h2 id="1-Hive-如何确定mapper数量？"><a href="#1-Hive-如何确定mapper数量？" class="headerlink" title="1.Hive 如何确定mapper数量？"></a>1.Hive 如何确定mapper数量？</h2><p>Hadoop体系下的MR作业使用InputFormat用来将HDFS文件分片(getSplit()方法)和对单标记录的读取为key-value对(getRecordReader())，所以InputFormat提供了数据切分的功能，inputformat是一个接口，对于每一种文件的输入格式都有一种对应的inputformat实现，如DeprecatedLzoTextInputFormat, OrcInputFormat, RCFileInputFormat,TextInputFormat. 可以认为，每种InputFormat实现的getSplit方法(该参数是JobConf对象)返回的InputSplit[] 数组，数组的元素个数就是读取该文件最终需要的mapper数.</p>
<h2 id="2-Hive中的两种InputFormat"><a href="#2-Hive中的两种InputFormat" class="headerlink" title="2.Hive中的两种InputFormat"></a>2.Hive中的两种InputFormat</h2><p>Hive默认的InputFormat是 CombineHiveInputFormat，这个inputformat用于当hdfs上存在小文件时，将多个小文件合并起来供一个mapper处理，从而节省mapper资源。一个备选是 HiveInputFormat ，相比前者没有合并小文件的功能。那这两个inputformat和前面说的各种文件类型所实现的各自的inputformat是什么关系呢？以HiveInputFormat为例，查看其getSplits()方法:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (Path dir : dirs) &#123;</span><br><span class="line">      PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);</span><br><span class="line">      // 获取hdfs路径下，文件实际的input format class</span><br><span class="line">      Class&lt;? extends InputFormat&gt; inputFormatClass = part.getInputFileFormatClass();</span><br><span class="line">      // 省略</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">// 省略</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">currentInputFormatClass = inputFormatClass;   // 将实际的input format 设置为当前的inputformat class</span><br><span class="line">// 省略</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">addSplitsForGroup(currentDirs, currentTableScan, newjob,</span><br><span class="line">    getInputFormatFromCache(currentInputFormatClass, job),</span><br><span class="line">    currentInputFormatClass, currentDirs.size()*(numSplits / dirs.length),</span><br><span class="line">    currentTable, result);    // 使用currentInputFormatClass获取splits，该方法最终调用文件类型实际的inputformat.getSplits()方法获取split数量</span><br></pre></td></tr></table></figure>
<p>通过上面的代码可知，hive中的CombineHiveInputFormat和HiveInputFormat是对各种文件格式的inputformat的包装。最终生效的仍是不同文件格式的inputformat实现。</p>
<h2 id="3-为什么HiveInputFormat能够切分大的LZO文件为小文件？"><a href="#3-为什么HiveInputFormat能够切分大的LZO文件为小文件？" class="headerlink" title="3.为什么HiveInputFormat能够切分大的LZO文件为小文件？"></a>3.为什么HiveInputFormat能够切分大的LZO文件为小文件？</h2><p>上面的代码显示了HiveInputFormat最终调用文件类型所实现的inputformat的getSplits()方法，lzo的inputformat实现是 DeprecatedLzoTextInputFormat, 查看其getSplits()方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (FileSplit fileSplit: splits) &#123;</span><br><span class="line">      Path file = fileSplit.getPath();</span><br><span class="line">      FileSystem fs = file.getFileSystem(conf);</span><br><span class="line"> </span><br><span class="line">      if (!LzoInputFormatCommon.isLzoFile(file.toString())) &#123;</span><br><span class="line">        // non-LZO file, keep the input split as is.</span><br><span class="line">        result.add(fileSplit);</span><br><span class="line">        continue;</span><br><span class="line">      &#125;</span><br><span class="line"> </span><br><span class="line">      // LZO file, try to split if the .index file was found</span><br><span class="line">      LzoIndex index = indexes.get(file);</span><br><span class="line">      if (index == null) &#123;</span><br><span class="line">        throw new IOException(&quot;Index not found for &quot; + file);</span><br><span class="line">      &#125;</span><br><span class="line"> </span><br><span class="line">      // 省略</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面代码的注释很清楚说明了，当.index文件存在，即lzo索引文件存在时，将split文件，因此使用DeprecatedLzoTextInputFormat原生的getSplits()方法，也就是设置hive的inputformat为HiveInputFormat能够拆分lzo文件。</p>
<h2 id="4-为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？"><a href="#4-为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？" class="headerlink" title="4.为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？"></a>4.为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？</h2><p>捋一下CombineHiveInputFormat类中的调用关系</p>
<p>CombineHiveInputFormat#getSplits()  →  CombineHiveInputFormat#getCombineSplits() →  CombineFileInputFormatShim#getSplits() →  HadoopShimsSecure#getSplits()</p>
<p>上面的CombineFileInputFormatShim是Hive为不同版本的Hadoop提供的称为HadoopShims的机制，用于兼容不同的Hadoop版本，相当于为不同的hadoop版本做一个一致的封装，上面的过程，还在Hive的代码范围内。</p>
<p>下面进入Hadoop代码：</p>
<p>HadoopShimsSecure#getSplits() →  org.apache.hadoop.mapred.lib.CombineFileInputFormat#getSplits() →  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits()</p>
<p>可以看到进入hadoop代码后，最终生效的是hadoop的CombineFileInputFormat的getSplits()方法，hive将拆分任务交给了hadoop。</p>
<p>继续上面的调用链：</p>
<p>org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits() → org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getMoreSplits()</p>
<p>在getMoreSplits()方法中，调用 org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#isSplitable() 来判断当前路径的文件是否可切割，那么isSplitable()是如何判断文件是否可切割呢？</p>
<p><img src="/uploads/Jietu20200411-114112.jpg" alt></p>
<p>通过 判断当前文件的压缩方式是不是 SplittableCompressionCodec 实例来决定当前文件是否可分!</p>
<p>lzo的codec是LzopCodec，查看LzopCodec的声明:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class LzoCodec implements Configurable, CompressionCodec &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>并没有实现SplittableCompressionCodec，因此Hadoop判断该文件不可拆分！最终导致只有一个mapper处理.</p>
<p>一张《hadoop definitiion guide》的图：</p>
<p><img src="/uploads/Jietu20200411-114220.jpg" alt><br>显示lzo不支持split，但是lzo在实际的实现中，当存在index文件时，是可以拆分的，因此重写了isSplitable()方法，但是只有在调用了lzo实现的getSplits()方法才会发现该文件可以split，而通过hadoop的</p>
<p>CombineFileInputFormat实现，lzo的LzoCodec被判定为不可切分。</p>
<h2 id="5-为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？"><a href="#5-为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？" class="headerlink" title="5. 为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？"></a>5. 为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？</h2><p>跟踪orc文件的读取过程，最终在isSplitable()方法中获取codec时，orc返回结果是null，而对于null，isSplitable()方法直接返回true。。。。。。</p>
<p><img src="/uploads/Jietu20200411-114315.jpg" alt></p>
<h1 id="影响范围"><a href="#影响范围" class="headerlink" title="影响范围"></a>影响范围</h1><p>LZO格式的表，单个HDFS文件很大</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>进行下面的设置，使用lzo自身的getSplits和isSplitable实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20171207 数据校验失败</title>
    <url>/2017/12/07/20171207-%E6%95%B0%E6%8D%AE%E6%A0%A1%E9%AA%8C%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>Hive开启向量化执行时，查询结果不准确<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 1. result : 20171205   199107</span><br><span class="line">set hive.vectorized.execution.enabled=true;</span><br><span class="line">select</span><br><span class="line">dt,</span><br><span class="line">sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as bar</span><br><span class="line">from foo</span><br><span class="line">where dt=20171205</span><br><span class="line">group by dt</span><br><span class="line">;</span><br><span class="line"> </span><br><span class="line">-- 2. result : 20171205    0</span><br><span class="line">set hive.vectorized.execution.enabled=false;</span><br><span class="line">select</span><br><span class="line">dt,</span><br><span class="line">sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as bar</span><br><span class="line">from foo</span><br><span class="line">where dt=20171205</span><br><span class="line">group by dt</span><br><span class="line">;</span><br></pre></td></tr></table></figure></p>
<p>向量化执行打开时，id =’’ or id is null 的判断有bug</p>
<p>影响范围，同时满足一下条件时存在bug：</p>
<ol>
<li>ORC格式的表</li>
<li>向量化执行打开</li>
<li>判断条件存在 col = ‘’ or col is null</li>
</ol>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>关闭向量化执行，hive-site.xml中设置hive.vectorized.execution.enabled=false</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20180105 Hive查询partitioned view报错</title>
    <url>/2018/01/05/20180105-Hive%E6%9F%A5%E8%AF%A2partitioned-view%E6%8A%A5%E9%94%99/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from view_xxx where dt=20180101 limit 10;</span><br><span class="line">FAILED: IndexOutOfBoundsException Index: 19, Size: 19</span><br></pre></td></tr></table></figure>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>Hive在做列剪枝的时候，在生成的Operatore Tree中，SEL Operator收集当前Operator需要查询哪些字段，最终告诉底层的Table Scan Operator，只需要查询目标字段，而不是读取表的全部字段，从而完成列剪枝。</p>
<p>该bug最简单的复现场景是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table foo</span><br><span class="line">(</span><br><span class="line">`a` string</span><br><span class="line">) partitioned by (`b` string)</span><br><span class="line">;</span><br><span class="line"> </span><br><span class="line">create view bar partitioned on (b) as</span><br><span class="line">select a,b from foo;</span><br><span class="line"> </span><br><span class="line">select * from bar;</span><br></pre></td></tr></table></figure>
<p>select * from bar 生成的原始Operator Tree是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TS[0]</span><br><span class="line">   |</span><br><span class="line">SEL[1]</span><br><span class="line">   |</span><br><span class="line">SEL[2]</span><br><span class="line">   |</span><br><span class="line">FS[3]</span><br></pre></td></tr></table></figure>
<p>SEL[1]用于确定从bar中实际上选择了哪些字段，在ColumnPrunerProcFactory.java调用bar的Table.getCols().get(index).getName()方法获取第index个列的名称时，由于getCols()不返回分区列，bar的分区列是b，除了b的字段是a，因此在这里返回的list是{“a”}， SEL[1]轮询查询的a,b字段，a的index是0，b的index是1，所以循环到b时，从{“a”}中获取index=1的值，报IndexOutOfBoundsException</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><ol>
<li>避免踩坑：不建分区视图（建分区不要指定partitioned on，底层表是分区表的话，hive会自动判断分区视图）</li>
<li>修复bug，新建一个list，将普通字段getCols()和分区字段getPartCols()都添加进去，从这个list中获取字段的name</li>
</ol>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20180109 HS2查询SQL报NPE以及Explain SQL的ClassCastException</title>
    <url>/2018/01/09/20180109-HS2%E6%9F%A5%E8%AF%A2SQL%E6%8A%A5NPE%E4%BB%A5%E5%8F%8AExplain-SQL%E7%9A%84ClassCastException/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>一段SQL查询语句，在HiveCli上查询正常，在邮件服务器上通过HiveServer2查询报错.</p>
<p>报错信息<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">    at org.apache.hadoop.hive.shims.Hadoop23Shims$1.listStatus(Hadoop23Shims.java:134)</span><br><span class="line">    at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:217)</span><br><span class="line">    at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:75)</span><br><span class="line">    at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getSplits(HadoopShimsSecure.java:319)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getCombineSplits(CombineHiveInputFormat.java:425)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:532)</span><br><span class="line">    at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:518)</span><br><span class="line">    at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)</span><br><span class="line">    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)</span><br><span class="line">    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268)</span><br><span class="line">    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:416)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:138)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79)</span><br></pre></td></tr></table></figure></p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="1-从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat："><a href="#1-从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat：" class="headerlink" title="1. 从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat："></a>1. 从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat：</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure>
<p>问题消失。</p>
<h2 id="2-错误是怎么触发的？"><a href="#2-错误是怎么触发的？" class="headerlink" title="2. 错误是怎么触发的？"></a>2. 错误是怎么触发的？</h2><p>下面是对这个bug的调试过程</p>
<h3 id="首先发现，出错的位置是第4个job，如下图"><a href="#首先发现，出错的位置是第4个job，如下图" class="headerlink" title="首先发现，出错的位置是第4个job，如下图"></a>首先发现，出错的位置是第4个job，如下图</h3><p><img src="/uploads/Jietu20200411-121510.jpg" alt></p>
<p>使用explain语句查看第4个job是干什么的（直接explain的时候报错，遇到了一个 Correlation Optimizer的bug，将在以后说明这个问题，关闭 Correlation Optimizer以后再执行explain），explain的结果如下：</p>
<p><img src="/uploads/Jietu20200411-121654.jpg" alt></p>
<p>发现第4个job是一个MR过程，并且是由group by造成的MR，group by的聚合是count, sum, count, sum。对比原SQL，发现这个MR是最后的一个阶段。因此考虑将出错SQL内部的查询建成一张临时表，在临时表的基础上执行相应的聚合，查看会不会报错.</p>
<p>遗憾的是，将中间过程拆分后，不管是在HIVECLI 还是HiveServer2上执行都不报错。</p>
<h3 id="上面的方法失败，只能远程调试HiveServer2了"><a href="#上面的方法失败，只能远程调试HiveServer2了" class="headerlink" title="上面的方法失败，只能远程调试HiveServer2了"></a>上面的方法失败，只能远程调试HiveServer2了</h3><p>首先直接跟踪到出错的位置：<br><img src="/uploads/Jietu20200411-121901.jpg" alt><br>上述while循环这段代码的意思是，扫描输入路径，将不是文件的东西，或者size为0的文件，或者schema以nullscan开头的文件，排除出输入文件。<br>问题就发生在 stat.getPath().toUri().getScheme().equals(“nullscan”) 这个判断中，从截图可以看到，循环的位置 stat.getPath().toUri().getScheme() 值是null，那么调用equals方法就会报NPE错误了。<br>作为对比，看一下使用HiveCli执行到这里时的情况：<br><img src="/uploads/Jietu20200411-122010.jpg" alt></p>
<p>从上图可以看到，使用HiveCli执行到这里时，stat.getPath().toUri().getScheme() 的值为hdfs。那么这时会判断 “hdfs”.equals(“nullscan”)，不会报错，并且下面的 it.remove()也不会执行到(判断这个文件是正常的文件，加入到当前MR job的)</p>
<h3 id="到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）"><a href="#到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）" class="headerlink" title="到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）"></a>到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）</h3><p>首先看一下导致上述bug的 “nullscan” 是什么， nullscan 应该是hadoop标识一个文件处于非正常状态，告诉job不要去扫描读取这个文件的标识。<br><img src="/uploads/Jietu20200411-122121.jpg" alt></p>
<p>上述发生错误的代码，Hive的本意是，调用listStats(job)方法获取需要扫描的文件列表（super.listStatus方法是Hadoop源码，不在Hive中），然后判断每个文件的schema是不是”nullscan”<br>调试super.listStatus代码，发现传入方法时，获取的文件列表，schema确实是nullscan:</p>
<p><img src="/uploads/Jietu20200411-122237.jpg" alt></p>
<p>可是当该方法返回时，hadoop会将nullscan处理掉，</p>
<p><img src="/uploads/Jietu20200411-122318.jpg" alt></p>
<p>因此，这里Hive误以为返回值是带nullscan的schema，但是其实经过hadoop处理后，已经不是一个完整的URI了，因此获取schema结果是null，再使用equals方法，就报错。</p>
<p>以上是导致这个错误的直接原因。</p>
<h3 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h3><p>这不是这个bug的最终形态，因为如果仅仅在调用equals方法之前，加一个非Null判断，虽然能避免NPE，但无法真正的解决问题，本质上这个问题是一个MR job的输入路径错了，不该是nullscan的文件。</p>
<p>应该是一个正常的HDFS文件。 </p>
<p>从上面的截图可以看到，通过HIVECLI执行时，输入文件是 LocatedFileStatus，其完整路径是 hdfs://xxx…. 对应一个HDFS的文件。</p>
<p>通过HiveServer2执行时，输入文件是nullscan://null/default…..，是一个错误的标识。</p>
<p>为什么HiveServer2执行时的输入路径不对？</p>
<p>输入路径是在JobConf对象的properties中的mapred.input.dir配置的，查看hiveserver2提交当前作业的job对象，发现在提交给Hadoop执行时，其输入路径就已经给错了，下面是job配置文件中mapred.input.dir的值</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20180109 mapper超物理内存限制</title>
    <url>/2018/01/09/20180109-mapper%E8%B6%85%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E9%99%90%E5%88%B6/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p><img src="/uploads/Jietu20200411-122757.jpg" alt><br>显示容器使用的内存(2.1G)已经超过物理内存限制(2G)</p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="hadoop中关于mapper-reducer几个内存参数的设置。"><a href="#hadoop中关于mapper-reducer几个内存参数的设置。" class="headerlink" title="hadoop中关于mapper/reducer几个内存参数的设置。"></a>hadoop中关于mapper/reducer几个内存参数的设置。</h2><p>注：以下每个设置都存在两个等效的名字，前者是hadoop0.x和1.x版本的名称，已废弃，后者是hadoop2.x版本的名称，虽然hadoop为了兼容性前者依然有效，但是建议全部使用后者。</p>
<h2 id="mapred-job-map-memory-mb-或-mapreduce-map-memory-mb"><a href="#mapred-job-map-memory-mb-或-mapreduce-map-memory-mb" class="headerlink" title="mapred.job.map.memory.mb 或 mapreduce.map.memory.mb"></a>mapred.job.map.memory.mb 或 mapreduce.map.memory.mb</h2><p>该参数是Hadoop允许为一个mapper分配的最大内存上限，单位是M, 如果内存不够，将会报类似下面的错误：</p>
<blockquote>
<p>Containerpid=container_1406552545451_0009_01_000002,containerID=container_234132_0001_01_000001 is running beyond physical memory limits. Current usage: 569.1 MB of 512 MB physical memory used; 970.1 MB of 1.0 GB virtual memory used. Killing container.</p>
</blockquote>
<h2 id="mapred-map-child-java-opts-或-mapreduce-map-java-opts"><a href="#mapred-map-child-java-opts-或-mapreduce-map-java-opts" class="headerlink" title="mapred.map.child.java.opts 或 mapreduce.map.java.opts"></a>mapred.map.child.java.opts 或 mapreduce.map.java.opts</h2><p>该参数是对java进程的内存参数设置，常用来设置一个mapper进程最大的堆内存上限(-Xmx)，使用 ‘-Xmx数字M’设置，如果堆内存不够，将报下面的错误：</p>
<blockquote>
<p>Error: java.lang.RuntimeException: java.lang.OutOfMemoryError</p>
</blockquote>
<h2 id="mapred-job-reduce-memory-mb-或-mapreduce-reduce-memory-mb"><a href="#mapred-job-reduce-memory-mb-或-mapreduce-reduce-memory-mb" class="headerlink" title="mapred.job.reduce.memory.mb 或  mapreduce.reduce.memory.mb"></a>mapred.job.reduce.memory.mb 或  mapreduce.reduce.memory.mb</h2><p>与mapper类似，该设置是针对reducer的最大内存上限设置</p>
<h2 id="mapred-reduce-child-java-opts-或-mapreduce-reduce-java-opts"><a href="#mapred-reduce-child-java-opts-或-mapreduce-reduce-java-opts" class="headerlink" title="mapred.reduce.child.java.opts 或 mapreduce.reduce.java.opts"></a>mapred.reduce.child.java.opts 或 mapreduce.reduce.java.opts</h2><p>跟mapper类似，该设置是针对reduce进程最大堆内存上限设置。</p>
<h1 id="这些设置之间有什么关系？"><a href="#这些设置之间有什么关系？" class="headerlink" title="这些设置之间有什么关系？"></a>这些设置之间有什么关系？</h1><p>上面说了两类设置，最大内存上限和最大堆内存上限。前者是针对container容器的设置，后者是针对java进程的JVM内存设置。java进程在container容器中运行，</p>
<p>所以理所应当的最大内存上限要大于最大堆内存上限，根据JVM内存模型，(最大内存 - 最大堆内存) 至少要大于 方法区 需要的内存大小。</p>
<p>建议  JVM最大堆内存 = Container最大内存 * 75% （当然，如果设置这两者相等，而运行中两者的值都超过了需要的内存大小，也不会报错）</p>
<h1 id="应该如何设置？"><a href="#应该如何设置？" class="headerlink" title="应该如何设置？"></a>应该如何设置？</h1><p>mapreduce.map.memory.mb=5120</p>
<p>mapreduce.reduce.memory.mb=8192</p>
<p>mapreduce.map.java.opts=’-Xmx8192M’</p>
<p>mapreduce.reduce.java.opts=’-Xmx8192M’</p>
<p>google上述设置发现这两项是CDH版本的Hadoop针对JVM内存的设置，在这里不知道有没有生效。（前面介绍的几项设置是hadoop标准版的设置）</p>
<p>BTW:<br>除了上面几项设置，还有一项设置能够在单个mapper处理的数据量较大时，加快mapper的速度。</p>
<p>mapreduce.task.io.sort.mb</p>
<p>该设置的默认值是100， 单位是M，作用是map阶段spill过程中，用于对mapper结果进行排序的内存大小（归并排序）</p>
<p>当内存不够时，会使用硬盘辅助排序，这就减慢了速度，必要时可以通过mapper的日志，如果发现在排序上耗时很多，可以将该设置增加到500. </p>
<p>不常用。</p>
<h1 id="shuffle报错"><a href="#shuffle报错" class="headerlink" title="shuffle报错"></a>shuffle报错</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:511)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:333)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)</span><br></pre></td></tr></table></figure>
<p>处理办法</p>
<blockquote>
<p>Cause 原因：reduce会在map执行到一定比例启动多个fetch线程去拉取map的输出结果，放到reduce的内存、磁盘中，然后进行merge。当数据量大时，拉取到内存的数据就会引起OOM，所以此时要减少fetch占内存的百分比，将fetch的数据直接放在磁盘上。<br>  有关参数：mapreduce.reduce.shuffle.memory.limit.percent</p>
</blockquote>
<p>将mapreduce.reduce.shuffle.memory.limit.percent设置为0.15，执行成功（默认0.25）</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20180201 查询LZO表出现额外的NULL和乱码</title>
    <url>/2018/02/01/20180201-%E6%9F%A5%E8%AF%A2LZO%E8%A1%A8%E5%87%BA%E7%8E%B0%E9%A2%9D%E5%A4%96%E7%9A%84NULL%E5%92%8C%E4%B9%B1%E7%A0%81/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>以下查询shotel_id字段为null的SQL（实际上表中不存在NULL值）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from dw_www</span><br><span class="line">where dt=&apos;20180115&apos; and shotel_id is null limit 10;</span><br></pre></td></tr></table></figure></p>
<p>结果是</p>
<p><img src="/uploads/Jietu20200411-123922.jpg" alt></p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>从以下三个可能原因入手：</p>
<h2 id="1-LZO压缩导致的乱码和NULL"><a href="#1-LZO压缩导致的乱码和NULL" class="headerlink" title="1. LZO压缩导致的乱码和NULL"></a>1. LZO压缩导致的乱码和NULL</h2><p>最初怀疑是lzo的压缩和解压过程中引入了脏数据，验证过程如下：</p>
<ol>
<li>跑一个分区的数据，数据不用lzo压缩，具体的SQL是：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table tmp_lzo_fix_bug</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &apos;,&apos;</span><br><span class="line">as</span><br><span class="line">select xxx -- select语句和生成乱码表的创建过程一样，使用20180115的数据</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>通过以上SQL，创建了一个用逗号分隔的文本格式的表tmp_lzo_fix_bug，其数据应该与dw_xxx的20180115分区一致。生成后执行SQL验证tmp_lzo_fix_bug是否存在NULL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from tmp_lzo_fix_bug where shotel_id is null limit 10;</span><br></pre></td></tr></table></figure>
<p>以上SQL查询结果为空，说明tmp_lzo_fix_bug表中不存在为NULL的数据，也即未压缩的文本文件中的数据是正常的。</p>
<ol start="2">
<li>既然未压缩前数据正常，那么接下来验证是不是压缩导致了数据损坏。<br>将20180115分区的lzo文件 000000_0.lzo下载下来，并解压<br><img src="/uploads/Jietu20200411-124214.jpg" alt></li>
</ol>
<p>使用lzop对其解压，将结果命名为decompress：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lzop -dv 000000_0.lzo -o decompress</span><br></pre></td></tr></table></figure></p>
<p>接下来只需要验证decompress文件中是否存在NULL和乱码，就能证明是否是lzo带来的问题，由于解压后的文件非常大，不能直接用vim/cat打开。</p>
<p>由于在HIVE中，NULL在文本文件中的表示是 \N， 因此只需要看文件中的 \N 即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat decompress | grep -a -F &quot;\N&quot;</span><br></pre></td></tr></table></figure>
<p>执行以上命令没有任何数据匹配，说明压缩后的lzo文件被解压后没有带来异常数据。</p>
<p>因此，由LZO压缩导致的乱码和NULL被排除。</p>
<h2 id="2-由hadoop集群未正确安装lzo压缩-解压软件或者lzo软件版本不对导致"><a href="#2-由hadoop集群未正确安装lzo压缩-解压软件或者lzo软件版本不对导致" class="headerlink" title="2.由hadoop集群未正确安装lzo压缩/解压软件或者lzo软件版本不对导致"></a>2.由hadoop集群未正确安装lzo压缩/解压软件或者lzo软件版本不对导致</h2><p>这个jira反映了类似的问题：<a href="https://issues.apache.org/jira/browse/HIVE-1138" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-1138</a></p>
<p>作者最终发现是由于未正确配置 io.compression.codecs 导致了乱码和NULL，验证了这个设置，已经正确设置了lzo codec。</p>
<p>然而在本地(pg5)上，由上一步验证了pg5上的lzo安装是正确的，并且上面的出现NULL的测试SQL中必须要有 下面的设置，才会出现NULL和乱码的情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br></pre></td></tr></table></figure>
<p>上述设置是强制HIVE走hadoop集群而不是本地的fetch task直接从HDFS上取数据。因此怀疑是hadoop集群机器上的lzo软件问题。</p>
<p>如何验证以上猜测？ </p>
<p>强制SQL使用本地hadoop。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET mapreduce.framework.name=local;</span><br></pre></td></tr></table></figure>
<p>加入上述设置使用本地hadoop来重新生成一份lzo压缩的表分区数据。</p>
<p>最终的验证结果是，这样生成的数据仍然存在NULL和乱码。</p>
<p>因此这个hadoop集群的原因也被排除。</p>
<h2 id="3-Hive自身的原因，读取lzo文件过程引入脏数据"><a href="#3-Hive自身的原因，读取lzo文件过程引入脏数据" class="headerlink" title="3.Hive自身的原因，读取lzo文件过程引入脏数据"></a>3.Hive自身的原因，读取lzo文件过程引入脏数据</h2><p>受到这篇文章启发：<a href="https://github.com/twitter/hadoop-lzo/issues/49" target="_blank" rel="noopener">https://github.com/twitter/hadoop-lzo/issues/49</a></p>
<p>这篇文章反映，Hive表没有设置正确的inputformat/outputformat同样会导致这个问题：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">STORED AS INPUTFORMAT &quot;com.hadoop.mapred.DeprecatedLzoTextInputFormat&quot;</span><br><span class="line">      OUTPUTFORMAT &quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&quot;</span><br></pre></td></tr></table></figure>
<p>如果未正确设置outputformat为org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat，那么HIVE会将lzo的index文件当成数据文件来读取，所以引入了脏数据（即index文件）</p>
<p>然而查看这个表的格式，发现inputformat和outputformat都是正确设置了的。</p>
<p>接下来对Hive读取数据的过程调试，发现在实际读取文件时，确实将index文件当做数据来读了：</p>
<p><img src="/uploads/Jietu20200411-124516.jpg" alt></p>
<p>尝试设置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from dw_xxx</span><br><span class="line">where dt=&apos;20180115&apos; and shotel_id is null limit 10;</span><br></pre></td></tr></table></figure></p>
<p>执行上述SQL果然没了NULL和乱码，跟踪读取文件：</p>
<p><img src="/uploads/Jietu20200411-124632.jpg" alt></p>
<p>确实没有将index文件加进来。</p>
<p>那么最终的原因找到了，CombineHiveInputFormat读取文件时错误的将索引文件当成数据文件读入了。</p>
<h1 id="影响范围与解决方案"><a href="#影响范围与解决方案" class="headerlink" title="影响范围与解决方案"></a>影响范围与解决方案</h1><p>影响范围必须满足一下两个条件：</p>
<ul>
<li>表格式是lzo</li>
<li>表中数据存在索引文件，即在job中使用了lzo_index对数据建了索引</li>
</ul>
<p>解决方案是，凡是在读取lzo的时候，都加上：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure></p>
<p>结合以前的lzo无法分割的issue，所有使用lzo表的地方都要加上上述设置</p>
<h1 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h1><ol>
<li>既然lzo文件与CombineHiveInputFormat之间有这么多兼容问题，能不能将org.apache.hadoop.hive.ql.io.HiveInputFormat设置为Hive默认的hive.input.format？</li>
</ol>
<p>答：不能，CombineHiveInputFormat在绝大部分情况下是有利的，能够合并小文件输入交给一个mapper处理，如果设置为HiveInputFormat，部分job由于有太多小文件，小文件列表会占用过多内存，导致OOM.</p>
<p>2 . 那遇到LZO的表该怎么办？</p>
<p>两个办法，要么加上set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;设置，要么将表改为orc格式，orc格式在空间效率和时间效率上都远优于lzo压缩的表。</p>
<p>因此，强烈建议表的格式采用orc！</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20180628 Hive On Spark任务初始化失败</title>
    <url>/2018/06/28/20180628-Hive-On-Spark%E4%BB%BB%E5%8A%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>在Hive中指定spark引擎，提交SQL后出现错误：</p>
<blockquote>
<p>FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.</p>
</blockquote>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="1-Hive-on-spark部署方式"><a href="#1-Hive-on-spark部署方式" class="headerlink" title="1. Hive on spark部署方式"></a>1. Hive on spark部署方式</h2><p>根据 Hive on Spark: Getting Started ，Hive on spark在Hive 2.2.0之前和之后的版本部署方式不同，当前生产环境的hive是2.1.1，部署spark环境需要编译spark得到 spark-assembly*.jar，然后将jar包放在hive的lib目录下，就完成了部署。（注意，hive on spark并不需要在hive服务器上安装spark客户端）</p>
<h2 id="2-当服务器上部署了Spark客户端，并设置了SPARK-HOME环境变量"><a href="#2-当服务器上部署了Spark客户端，并设置了SPARK-HOME环境变量" class="headerlink" title="2. 当服务器上部署了Spark客户端，并设置了SPARK_HOME环境变量"></a>2. 当服务器上部署了Spark客户端，并设置了SPARK_HOME环境变量</h2><p>算法同学需要使用spark客户端提交spark任务，在服务器上部署了spark客户端，版本是2.1.2，并设置了SPARK_HOME环境变量，而2.1.2版本的spark和2.1.1版本的hive是不兼容的（2.1.1版本pom文件中指定spark版本是1.6）</p>
<p>Hive中使用spark提交任务部分代码片段如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (sparkHome != null) &#123; // 如果sparkHome不为空，将sparkHome目录下的bin/spark-submit加入执行参数</span><br><span class="line">  argv.add(new File(sparkHome, &quot;bin/spark-submit&quot;).getAbsolutePath());</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  LOG.info(&quot;No spark.home provided, calling SparkSubmit directly.&quot;);</span><br><span class="line">  argv.add(new File(System.getProperty(&quot;java.home&quot;), &quot;bin/java&quot;).getAbsolutePath());</span><br><span class="line"> </span><br><span class="line">  if (master.startsWith(&quot;local&quot;) || master.startsWith(&quot;mesos&quot;) || master.endsWith(&quot;-client&quot;) || master.startsWith(&quot;spark&quot;)) &#123;</span><br><span class="line">    String mem = conf.get(&quot;spark.driver.memory&quot;);</span><br><span class="line">    if (mem != null) &#123;</span><br><span class="line">      argv.add(&quot;-Xms&quot; + mem);</span><br><span class="line">      argv.add(&quot;-Xmx&quot; + mem);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    String cp = conf.get(&quot;spark.driver.extraClassPath&quot;);</span><br><span class="line">    if (cp != null) &#123;</span><br><span class="line">      argv.add(&quot;-classpath&quot;);</span><br><span class="line">      argv.add(cp);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    String libPath = conf.get(&quot;spark.driver.extraLibPath&quot;);</span><br><span class="line">    if (libPath != null) &#123;</span><br><span class="line">      argv.add(&quot;-Djava.library.path=&quot; + libPath);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    String extra = conf.get(DRIVER_OPTS_KEY);</span><br><span class="line">    if (extra != null) &#123;</span><br><span class="line">      for (String opt : extra.split(&quot;[ ]&quot;)) &#123;</span><br><span class="line">        if (!opt.trim().isEmpty()) &#123;</span><br><span class="line">          argv.add(opt.trim());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 如果sparkHome为空，直接将org.apache.spark.deploy.SparkSubmit加入执行变量，此时，将会使用classpath下的该类提交任务，也就是lib目录下的spark jar</span><br><span class="line">  argv.add(&quot;org.apache.spark.deploy.SparkSubmit&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的注释清楚的说明了提交spark任务途径的优先级：</p>
<ul>
<li>如sparkHome设置，使用sparkHome/bin/spark-submit提交任务。</li>
<li>如果sparkHome没设置，使用org.apache.spark.deploy.SparkSubmit提交任务，该类在spark-assembly.jar中。</li>
</ul>
<p>sparkHome的设置方式是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">String sparkHome = conf.get(SPARK_HOME_KEY);</span><br><span class="line">if (sparkHome == null) &#123;</span><br><span class="line">  sparkHome = System.getenv(SPARK_HOME_ENV);</span><br><span class="line">&#125;</span><br><span class="line">if (sparkHome == null) &#123;</span><br><span class="line">  sparkHome = System.getProperty(SPARK_HOME_KEY);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>优先级如下：</p>
<ol>
<li>先取conf对象中的 spark.home 设置，可以在hive-site.xml中设置（当前未设置）</li>
<li>取 SPARK_HOME 环境变量</li>
<li>取 spark.home系统属性</li>
</ol>
<p>所以，当设置了SPARK_HOME环境变量时，hive将使用服务器上部署的spark客户端提交任务，而spark客户端版本2.1.2和hive版本不兼容，导致了错误</p>
<h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>为了保证hive on spark不受到其他spark版本影响，做了以下双重保证：</p>
<ol>
<li>不设置SPARK_HOME环境变量</li>
<li>假如不小心设置了，在Hive中抹去SPARK_HOME环境变量</li>
</ol>
<p>对于1， 将 job工程和机器上/etc/profile.d/z-hadoop.sh中的</p>
<p>export SPARK_HOME=/home/q/spark/spark-default</p>
<p>设置删除，取而代之的是，在job工程中封装函数，需要的时候再指定spark_home<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hive on spark 目前只支持spark1.6 spark-default版本是2.1.2</span><br><span class="line"># 为同时支持hive on spark和直接提交spark任务，封装此函数而不直接export SPARK_HOME</span><br><span class="line">function SPARK_SUBMIT &#123;</span><br><span class="line">    export SPARK_HOME=/home/q/spark/spark-default</span><br><span class="line">    $&#123;SPARK_HOME&#125;/bin/spark-submit $@</span><br><span class="line">    unset SPARK_HOME</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对于2，在hive-config.sh中，抹去SPARK_HOME</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20180803 修复metastore挂了重启之后，hiveserver无法重连的问题</title>
    <url>/2018/08/03/20180803-%E4%BF%AE%E5%A4%8Dmetastore%E6%8C%82%E4%BA%86%E9%87%8D%E5%90%AF%E4%B9%8B%E5%90%8E%EF%BC%8Chiveserver%E6%97%A0%E6%B3%95%E9%87%8D%E8%BF%9E%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>当外库的metastore服务挂了，手动重启服务，hiveserver无法探测到服务变得可用而自动重连，必须重启hiveserver才能重连到metastore</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><ol>
<li>为了防止每次连接都要新建metastore client对象（一次tcp连接过程），我们将首次连接好后的metastore client对象缓存起来，以后使用直接拿缓存即可，但是当底层网络连接断开后，缓存中的连接对象变得不可用，这是导致故障的原因。</li>
<li>Hive开发者为了解决这个问题，实际上使用的metastore client是RetryingMetaStoreClient，该对象是真正的metastore client的动态代理，带有尝试功能，当首次连接失败后，调用metastore client的reconnect方法，重建底层tcp连接。</li>
<li>外库的metastore client为了限制哪些方法可用，本身已经做了一层jdk动态代理，没法再使用RetryingMetaStoreClient去动态代理一个被代理过的对象，但是可以换一个思路，模仿RetryingMetaStoreClient，增加重试功能。</li>
</ol>
<h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><blockquote>
<p>org.apache.hadoop.hive.metastore.HiveForeignMetaStoreProxy</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;</span><br><span class="line">    String methodName = method.getName();</span><br><span class="line"> </span><br><span class="line">    if (NOT_IMPLEMENTED_METHODS.contains(methodName))</span><br><span class="line">        throw new MetaException(methodName + &quot; on a foreign database is not implemented&quot;);</span><br><span class="line"> </span><br><span class="line">    if (!ALLOWED_METHODS.contains(methodName))</span><br><span class="line">        throw new MetaException(methodName + &quot; on a foreign database is not allowed&quot;);</span><br><span class="line"> </span><br><span class="line">    int retry = 0;</span><br><span class="line">    String virtualDBName = (String) args[0];</span><br><span class="line"> </span><br><span class="line">    try &#123;</span><br><span class="line">        while (retry++ &lt; RETRY_COUNT) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                if (MAPTABLE_METHODS.contains(methodName)) &#123;</span><br><span class="line">                    assert args.length != 0;</span><br><span class="line">                    String mappedDBName = foreignMapping.get(virtualDBName);</span><br><span class="line">                    assert mappedDBName != null;</span><br><span class="line">                    args[0] = mappedDBName;</span><br><span class="line"> </span><br><span class="line">                    Object ret = method.invoke(foreignMSC, args);</span><br><span class="line">                    if (ret instanceof Table) &#123;</span><br><span class="line">                        ((Table) ret).setDbName(virtualDBName);</span><br><span class="line">                    &#125;</span><br><span class="line">                    return ret;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    return method.invoke(foreignMSC, args);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                args[0] = virtualDBName;</span><br><span class="line">                if (retry &gt;= RETRY_COUNT) &#123;</span><br><span class="line">                    throw e;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    foreignMSC.reconnect(); // client重连</span><br><span class="line">                    Thread.sleep(1500);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; catch (UndeclaredThrowableException e) &#123;</span><br><span class="line">        throw e.getCause().getCause();</span><br><span class="line">    &#125; catch (InvocationTargetException e) &#123;</span><br><span class="line">        Throwable underlying = e.getCause();</span><br><span class="line">        if ((underlying instanceof TApplicationException) ||</span><br><span class="line">                (underlying instanceof TProtocolException) ||</span><br><span class="line">                (underlying instanceof TTransportException)) &#123;</span><br><span class="line">            throw underlying;</span><br><span class="line">        &#125; else if ((underlying instanceof MetaException) &amp;&amp;</span><br><span class="line">                underlying.getMessage().matches(&quot;JDO[a-zA-Z]*Exception&quot;)) &#123;</span><br><span class="line">            throw underlying;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            throw underlying;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    return null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20190429 ORC表一个有趣的问题</title>
    <url>/2019/04/29/20190429-ORC%E8%A1%A8%E4%B8%80%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>（其实这并不是bug，只是一个现象，或者说好久没更新这个系列wiki，随便写写）</p>
<p>现象：</p>
<p>两张Hive表关联的时候，mapper阶段卡在某个进度很久不动。</p>
<p>为什么？</p>
<p>orc是一种压缩率特别高的文件格式。因为其列存储的特点，当某些列存在很多一样的值时，比如NULL，压缩率特别极其高。</p>
<p>上面关联的一张表，其条数有1亿多，但是文件只有不到30M大小。</p>
<p>造成什么后果？</p>
<p>hive根据输入文件大小决定分配多少mapper，上面那个不到30M的文件，很有可能被分给一个mapper，那这个mapper就要处理1亿多条数据。。。</p>
<p>你也许觉得文件很小，处理很快，实际上，读取文件确实很快，可是别忘了mapper阶段需要对输入文件按照key排序，造成一个mapper要排序1亿多条数据（惊不惊喜？）</p>
<p>于是，这个mapper stuck了</p>
<p>我该怎么办呢？</p>
<p>备选方案1：你可以设置一个mapper处理数据量小小小，比如设置成1M？这样那个30M的文件，会被分给30个mapper处理，可是~如果需要join的另一个表文件很大很大，比如1T，那你算算另一个表要用多少mapper吧（更加惊喜了）</p>
<p>备选方案2：把那个30M的表换个格式，简单点就用文本格式吧~体积一下子就膨胀了，单个mapper处理的数据量就变小啦。</p>
<p>备选方案3:  从数据源想想为啥压缩率这么高，是不是数据有问题。</p>
<p>当你mapper卡住的时候，怎么确定是不是这篇wiki描述的问题？</p>
<p>step1~查看那个卡住的mapper，看看它在干啥.</p>
<p>下面这个spilling map output表示mapper在排序（而且，这时候mapper在用硬盘做排序~很慢）</p>
<p><img src="/uploads/Jietu20200411-125841.jpg" alt></p>
<p>step2~往上翻翻日志，看看这个mapper读取的文件，然后看看这个文件是不是很小？它对应的表的条数是不是很多？</p>
<p>如果是的话那就没错了~</p>
<p><img src="/uploads/Jietu20200411-125934.jpg" alt></p>
<p>顺便提下，当mapper日志中出现“spilling map output”时，你可以增加mapreduce.task.io.sort.mb这个参数的值，调整排序内存的大小，减少spill disk的次数加快速度</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20190731 Kryo反序列化失败</title>
    <url>/2019/07/31/20190731-Kryo%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>SQl报错<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error: java.lang.RuntimeException: Failed to load plan: viewfs://xxx/-mr-10005/49f2f715-02e7-4f66-802b-0b080a9c6129/map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 124</span><br><span class="line">Serialization trace:</span><br><span class="line">sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:456)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:303)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:315)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:607)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:582)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:676)</span><br><span class="line">    at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:169)</span><br><span class="line">    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)</span><br><span class="line">    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 124</span><br><span class="line">Serialization trace:</span><br><span class="line">sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readClass(SerializationUtilities.java:175)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:686)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readObject(SerializationUtilities.java:200)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializeObjectByKryo(SerializationUtilities.java:615)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializePlan(SerializationUtilities.java:524)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializePlan(SerializationUtilities.java:477)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:416)</span><br><span class="line">    ... 13 more</span><br></pre></td></tr></table></figure></p>
<h1 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h1><h2 id="问题产生的背景"><a href="#问题产生的背景" class="headerlink" title="问题产生的背景"></a>问题产生的背景</h2><p>首先描述下上述错误产生的上下文：</p>
<ol>
<li><p>Hive是一个将SQL编译成MR程序的sql编译器，并不负责执行MR任务</p>
</li>
<li><p>一个典型的MR任务，需要一个实现org.apache.hadoop.mapred.Mapper接口的mapper类，一个实现org.apache.hadoop.mapred.Reducer的reducer类，还要一个控制类设置任务相关参数，并提交给集群执行</p>
</li>
<li><p>Hive也不例外，Hive提交任务的控制器是org.apache.hadoop.hive.ql.exec.mr.ExecDriver，Mapper类是统一的，都是org.apache.hadoop.hive.ql.exec.mr.ExecMapper，reducer类也是统一的，org.apache.hadoop.hive.ql.exec.mr.ExecReducer</p>
</li>
</ol>
<p>下面都以mapper为例，reducer一样</p>
<ol start="4">
<li><p>ExecMapper通过内部的operator来区分具体每个sql的每个mapper干什么事情，内部持有一个AbstractMapOperator属性，执行时是调用AbstractMapOperator#process来干活</p>
</li>
<li><p>Hive内部使用MapWork来描述一个mapper的工作，（4）中的AbstractMapOperator具体干什么事情，是由MapWork决定的，hadoop集群上的节点是通过Utilities.getMapWork(job)获取一个MapWork对象</p>
</li>
<li><p>终于进入正题了，Utilities.getMapWork(job)是读取HDFS的map.xml文件，将文件内容反序列化为java对象，而map.xml是由Hive客户端写入HDFS上的，也就是说，HDFS作为媒介，保存Hive端编译好的mapper/reducer内部的执行细节，然后在yarn集群上每个container从hdfs上读取xml文件，将其反序列化为对象，然后执行。</p>
</li>
<li><p>map.xml和reducer.xml虽然以xml结尾，但这是历史原因，早期的hive用xml保存序列化的结果，但是（记得是从2.0开始）目前hive只有一种序列化方式，就是kryo，xml其实是二进制文件。这个xml文件比较难捕捉，任务执行完成后不管是否正常结束都会自动删除，只能用debug的方式在写完文件后，下载下来。下面放一个上面SQL的map.xml文件：</p>
</li>
</ol>
<p>map.xml</p>
<ol start="8">
<li>回头看下上面的错误就能看明白了，Hive客户端将map.xml文件写到HDFS上的一个路径，在hadoop端反序列化map.xml时，kryo反序列化失败了，导致hadoop端获取不到mapper任务</li>
</ol>
<h2 id="解决问题的尝试"><a href="#解决问题的尝试" class="headerlink" title="解决问题的尝试"></a>解决问题的尝试</h2><p>解决问题首先google，上述错误在google上信息很少，但也有几条。</p>
<ol>
<li><a href="https://github.com/EsotericSoftware/kryo/issues/307" target="_blank" rel="noopener">https://github.com/EsotericSoftware/kryo/issues/307</a></li>
</ol>
<p>这个issue跟我面临的问题基本一致，提到了kryo的仓库里，下面kyro的作者之一回复说可能是上一个序列化属性占了大量的空间导致取下一个kryo class id的时候取错了。但是在我的例子里，本身序列化的文件就不大，这条pass。</p>
<ol start="2">
<li><a href="https://issues.apache.org/jira/browse/HIVE-11519" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-11519</a></li>
</ol>
<p>这是一个Hive的issue，里面提的问题也跟我遇到的一致，但是处于未解决状态，下面的回复引导了另一个issue：<a href="https://issues.apache.org/jira/browse/HIVE-12175" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-12175</a> 这个issue号称解决了Hive中大量的kryo序列化bug，但是fix version是2.0.0，我们用的hive版本是2.1.1，高于它，可以预见这个issue也没什么用</p>
<ol start="3">
<li>翻遍了google，有说hive on spark问题的，有说hive并行化执行bug的，有说集群中存在老的hive相关类的class的，能通过改配置调整的都试过全部无效</li>
</ol>
<p>只能磕代码了。</p>
<p>第一个尝试是在本地复现反序列化的问题。将Hive编译的map.xml下载到本地，然后在Hive中新建一个测试类，尝试去反序列化该文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">public class KryoDebug &#123;</span><br><span class="line"> </span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        abnormal();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    public static void abnormal() throws Exception &#123;</span><br><span class="line">        Kryo kryo = SerializationUtilities.borrowKryo();</span><br><span class="line">        String path = KryoDebug.class.getClassLoader().getResource(&quot;map.xml&quot;).getPath();</span><br><span class="line">        File mapFile = new File(path);</span><br><span class="line">        FileInputStream inputStream = new FileInputStream(mapFile);</span><br><span class="line">        MapWork mapWork = SerializationUtilities.deserializePlan(kryo, inputStream, MapWork.class);</span><br><span class="line">        System.out.println(mapWork);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>很遗憾，上面的代码执行是没问题的，在本地无法复现反序列化的问题，为了进一步确定本地没问题，将出错的SQL使用本地模式执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET mapreduce.framework.name=local;</span><br><span class="line">drop table if exists temp.xxx_debug2;</span><br><span class="line">create table temp.xxx_debug2 stored as orc as</span><br><span class="line">select</span><br><span class="line">to_json(price_promotion)</span><br><span class="line">from ods_xxx</span><br><span class="line">where dt =&apos;20190722&apos; and hour=20</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<p>使用本地模式执行上述SQL也是可以正常执行，因此排除了SQL bug、Hive bug、Hadoop bug等问题，怀疑是集群环境和本地环境不一致。</p>
<p>第二个尝试是去测试集群中各类的版本是否和Hive端一致.</p>
<p>首先是验证反序列的MapWork客户端和集群是否一致：在集群上跑输出来源和字段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (planClass.equals(MapWork.class)) &#123;</span><br><span class="line">  String path = &quot;org.apache.hadoop.hive.ql.plan.MapWork&quot;;</span><br><span class="line">  ClassLoader classLoader = Utilities.getSessionSpecifiedClassLoader();</span><br><span class="line">  LOG.info(&quot;MapWork反序列化:&quot; + classLoader.getResource(path.replace(&apos;.&apos;, &apos;/&apos;) + &quot;.class&quot;).toString());</span><br><span class="line">  Class&lt;MapWork&gt; clazz = MapWork.class;</span><br><span class="line">  Field[] fields = clazz.getDeclaredFields();</span><br><span class="line">  StringBuilder sb = new StringBuilder();</span><br><span class="line">  for (Field field : fields) &#123;</span><br><span class="line">    sb.append(field.getName()).append(&quot;###&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  LOG.info(&quot;MapWork反序列化属性:&quot; + sb.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结果<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-07-31 17:42:25,266 INFO [main] org.apache.hadoop.hive.ql.exec.SerializationUtilities: MapWork反序列化:jar:file:/data5/hadoop2x/nm-local-dir/usercache/qhstats/appcache/application_1564438364333_147100/filecache/10/job.jar/job.jar!/org/apache/hadoop/hive/ql/plan/MapWork.class</span><br><span class="line">2019-07-31 17:42:25,266 INFO [main] org.apache.hadoop.hive.ql.exec.SerializationUtilities: MapWork反序列化属性:pathToAliases###pathToPartitionInfo###aliasToWork###aliasToPartnInfo###nameToSplitSample###bucketedColsByDirectory###sortedColsByDirectory###tmpHDFSPath###tmpPathForPartitionPruning###inputformat###indexIntermediateFile###numMapTasks###maxSplitSize###minSplitSize###minSplitSizePerNode###minSplitSizePerRack###samplingType###SAMPLING_ON_PREV_MR###SAMPLING_ON_START###leftInputJoin###baseSrc###mapAliases###mapperCannotSpanPartns###inputFormatSorted###useBucketizedHiveInputFormat###dummyTableScan###eventSourceTableDescMap###eventSourceColumnNameMap###eventSourceColumnTypeMap###eventSourcePartKeyExprMap###doSplitsGrouping###vectorizedRowBatch###includedBuckets###llapIoDesc###$assertionsDisabled###</span><br></pre></td></tr></table></figure></p>
<p>可以看到class来自hive提交上去的jar包，比较了属性和本地class属性比较，也完全一致。排除了集群classpath下MapWork类冲突的问题</p>
<p>其次验证kryo版本是否一致，验证的结果是集群上使用的kryo也是来自hive提上去的。排除kryo class类冲突问题。</p>
<p>第三个尝试是针对反序列化错误的字段：</p>
<p>sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)<br>在反序列化sortedColsByDirectory字段发生错误，在com.esotericsoftware.kryo.pool.KryoFactory#create中让kryo不去序列化该字段：</p>
<p><img src="/uploads/Jietu20200411-131223.jpg" alt></p>
<p>重新编译hive后执行还是不行，包另一个字段出错了，而且这种方式是不可取的，不能说不序列化一个字段就不序列化了，sortedColsByDirectory在一些SQL中有用。</p>
<p>第四个尝试是怀疑SQL中用的to_json UDF有问题，UDF包中有hive相关类，在这里的尝试是将该UDF中所有用到的class，依赖全部改为provided（除了hive和hadoop中不存在的class），结果居然还是报同样的错。</p>
<p>第五个尝试是怀疑jdk版本的问题，hiveclient的jdk是8u60，hadoop集群jdk是8u131，于是将客户端jdk版本升级到8u131，仍然报错。</p>
<p>上述尝试全部失败后，实在没思路了，有一周时间没继续研究这个问题。</p>
<h2 id="找到新的切入点"><a href="#找到新的切入点" class="headerlink" title="找到新的切入点"></a>找到新的切入点</h2><p>既然是kryo反序列化的问题，还是得从kryo入手。</p>
<p>kyro本身在序列化的时候，可以通过执行下面指令将序列化的详细过程输出到stdout中</p>
<blockquote>
<p>Log.TRACE();</p>
</blockquote>
<p>在Hive中加入上述代码，重新编译Hive并执行SQL，输出的结果大概是这样：</p>
<p><img src="/uploads/Jietu20200411-131351.jpg" alt></p>
<p>从错误日志：</p>
<p>Encountered unregistered class ID: 124<br>可以知道是124号class反序列化异常，看下124号序列话的是什么：</p>
<p><img src="/uploads/Jietu20200411-131438.jpg" alt></p>
<p>是org.apache.hadoop.hive.ql.plan.TableScanDesc，于是猜测是该类未在序列化时注册，于是在hive中注册kyro时，手动注册该类：</p>
<p>org.apache.hadoop.hive.ql.exec.SerializationUtilities</p>
<p><img src="/uploads/Jietu20200411-131517.jpg" alt></p>
<p>重新编译并执行，问题依然存在，此时已经处于崩溃边缘。</p>
<p>看来从序列化日志是看不出什么东西了，那就从反序列日志入手：</p>
<p>本地的反序列日志容易获得，只需在本地运行上面的KryoDebug时，加上Log.TRACE();但是在集群上跑的时候，加上Log.TRACE();并不能在集群日志上打出反序列化日志，这是为什么？</p>
<p>研究了一阵发现集群日志只会显示LOG输出的日志，kyro的序列化日志是直接输出到标准输出的，而stdout输出的东西不会显示在集群日志上，我又没办法登上集群机器上看标准输出的东西，最后想了个取巧的方案，将标准输出“移植”到LOG中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 这个类用于将标准输出转移到LOG里</span><br><span class="line">public class CustomOutputStream extends OutputStream &#123;</span><br><span class="line">    Logger logger;</span><br><span class="line">    StringBuilder stringBuilder;</span><br><span class="line"> </span><br><span class="line">    public CustomOutputStream(Logger logger)</span><br><span class="line">    &#123;</span><br><span class="line">        this.logger = logger;</span><br><span class="line">        stringBuilder = new StringBuilder();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    @Override</span><br><span class="line">    public final void write(int i) throws IOException &#123;</span><br><span class="line">        char c = (char) i;</span><br><span class="line">        if(c == &apos;\r&apos; || c == &apos;\n&apos;) &#123;</span><br><span class="line">            if(stringBuilder.length() &gt; 0) &#123;</span><br><span class="line">                logger.info(stringBuilder.toString());</span><br><span class="line">                stringBuilder = new StringBuilder();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else</span><br><span class="line">            stringBuilder.append(c);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">// 全局设置一次重定向stdout</span><br><span class="line">System.setOut(new PrintStream(new CustomOutputStream(LOG)));</span><br></pre></td></tr></table></figure>
<p>这时候集群日志上终于可以显示Kryo日志了：</p>
<p><img src="/uploads/Jietu20200411-131613.jpg" alt></p>
<p>将集群上的kryo反序列化日志（失败的）和我本机的反序列化日志（成功的）进行对比，果然发现不同：</p>
<p>本地日志<br><img src="/uploads/Jietu20200411-131645.jpg" alt></p>
<p>集群日志<br><img src="/uploads/Jietu20200411-131716.jpg" alt><br>对比两个日志，发现从Field _parent: class org.codehaus.jackson.sym.BytesToNameCanonicalizer这一行为分割，前面的日志两者一样，后面的日志开始不一样。</p>
<p>于是怀疑是这个类的问题，这是jackson里的类，udf中用到的。但是之前已经尝试过将udf中的类改为provided，并没有用。突然想起来会不会hive中的jackson和hadoop中的jackson冲突，让hadoop团队查了下hadoop中的jackson版本是1.9.13，然后我查了下hive的jackson版本，是1.9.2.</p>
<p>最后一次尝试！</p>
<p>将hive中的jackson版本改为1.9.13，重新编译hive，执行sql。amazing happens！成功了。</p>
<p>所以最终定位到的问题是hive和hadoop中存在jackson类冲突，而udf用到了该类，导致在序列化时，用的hive端的jackson（1.9.2），而在反序列化时，用的hadoop端的jackson（1.9.13），至于为什么反序列时不用1.9.2的jackson，这谁tm知道呢，这就是同名类加载顺序不确定性恶心的地方了。</p>
<h1 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h1><p>这是我处理过的最为棘手的一个Hive问题。难点在于：</p>
<ul>
<li>定位问题难，kryo反序列报错，这个错可能导致的原因太多了，而且更为关键的是我本地无法复现反序列化错误。</li>
<li>集群上调试困难，代码是跑在hadoop集群上报错的，而我无法debug hadoop集群代码（毕竟你不能在公司生产集群上打个断点调试）<br>收获是学习了kryo相关知识。</li>
</ul>
<p>教训是class冲突真特么难分析，终于知道为什么公司强制每次push代码都检测工程依赖有没有重复的class了，万一因为class冲突导致错误是真的难发现。</p>
<p>而在我们的HIVE SQL运行环境里，似乎类冲突很难避免，因为hadoop/hive/udf jar都有各自的依赖包，在执行sql时，这些依赖搅成一团，难保不冲突。</p>
<p>该怎么做尽量减少这类问题？</p>
<p>UDF pom里绝对绝对不能随意添加依赖，在添加之前必须检测hive和hadoop中是否已经存在该依赖！如果都不存在，可以添加，如果有一个存在，尽量与已经存在的版本保持一致</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>20190523 关于HiveCli启动慢原因的调查</title>
    <url>/2019/05/23/20190523-%E5%85%B3%E4%BA%8EHiveCli%E5%90%AF%E5%8A%A8%E6%85%A2%E5%8E%9F%E5%9B%A0%E7%9A%84%E8%B0%83%E6%9F%A5/</url>
    <content><![CDATA[<h1 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h1><p>HiveClient 有时候会启动非常慢，具体表现是进入hive时当终端输出</p>
<blockquote>
<p>Logging initialized using configuration in file:/home/q/hive/apache-hive-2.1.1-qhstats-bin/conf/hive-log4j2.properties Async: false</p>
</blockquote>
<p>会隔上几秒甚至几十秒甚至几分钟才出现</p>
<blockquote>
<p>hive (default)&gt;<br>然后才能正常使用Hive</p>
</blockquote>
<h1 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h1><h2 id="1-打开debug日志查看启动Hive时卡在哪个步骤了"><a href="#1-打开debug日志查看启动Hive时卡在哪个步骤了" class="headerlink" title="1. 打开debug日志查看启动Hive时卡在哪个步骤了"></a>1. 打开debug日志查看启动Hive时卡在哪个步骤了</h2><blockquote>
<p>sudo -uqhstats hive_211 –hiveconf hive.root.logger=debug,console</p>
</blockquote>
<p>观察启动后的日志</p>
<p><img src="/uploads/Jietu20200411-130501.jpg" alt></p>
<p>红框中两行日志相隔了一分多钟。</p>
<p>下方的日志很明显，是在创建HDFS目录。</p>
<p>所以元凶就是HDFS的问题，集群namenode响应慢。</p>
<h2 id="2-深入了解下Hive启动时需要创建哪些临时目录"><a href="#2-深入了解下Hive启动时需要创建哪些临时目录" class="headerlink" title="2. 深入了解下Hive启动时需要创建哪些临时目录"></a>2. 深入了解下Hive启动时需要创建哪些临时目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Create dirs &amp; session paths for this session:</span><br><span class="line"> * 1. HDFS scratch dir</span><br><span class="line"> * 2. Local scratch dir</span><br><span class="line"> * 3. Local downloaded resource dir</span><br><span class="line"> * 4. HDFS session path</span><br><span class="line"> * 5. hold a lock file in HDFS session dir to indicate the it is in use</span><br><span class="line"> * 6. Local session path</span><br><span class="line"> * 7. HDFS temp table space</span><br><span class="line"> * @param userName</span><br><span class="line"> * @throws IOException</span><br><span class="line"> */</span><br><span class="line">private void createSessionDirs(String userName) throws IOException &#123;</span><br></pre></td></tr></table></figure>
<ul>
<li>HDFS 临时写目录（存放执行过程中hdfs临时文件）</li>
<li>本地临时写目录（local task需要本地写）</li>
<li>本地下载资源目录（如udf jar包）</li>
<li>hdfs会话目录</li>
<li>hdfs锁文件</li>
<li>本地会话目录</li>
<li>hdfs临时表空间<br>可以看到当client启动时，需要创建很多hdfs/本地临时目录供当前会话使用</li>
</ul>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>hive client启动慢的原因是hadoop集群namenode响应慢。</p>
]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink执行流程(一)：生成StreamGraph</title>
    <url>/2021/07/30/Flink%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B-%E4%B8%80/</url>
    <content><![CDATA[<h1 id="一-四种Graph"><a href="#一-四种Graph" class="headerlink" title="一. 四种Graph"></a>一. 四种Graph</h1><p>Flink在将用户代码提交到集群上执行过程中，根据所处的阶段，会生成四种不同类型的DAG图，分别是:</p>
<ul>
<li>StreamGraph<br>Client端生成，根据用户编写的Stream API代码生成的拓扑图。</li>
<li>JobGraph<br>Client端生成，StreamGraph经过优化后生成JobGraph。<br>其中主要的优化是Operator Chain，将符合条件的相邻节点chain在一起执行，Flink的一个Operator占用TaskManager的一个线程执行，将多个operator合并成一个operator的好处是避免网络io和序列化开销，同时减少线程数、降低线程切换成本。</li>
<li>ExecutionGraph<br>JobManager生成，根据JobGraph生成ExecutionGraph，是JobGraph的并行化版本。</li>
<li>PhysicalGraph<br>各个TaskManager实际执行运行图，没有具体的实现类</li>
</ul>
<p><img src="/uploads/flink-run.png" alt></p>
<p>其关系是 StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; PhysicalGraph<br>下面以一个最简单的代码示例，演示StreamGraph是如何生成的：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Integer&gt; source = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">    DataStream&lt;Integer&gt; transform = source.map(x -&gt; x * x);</span><br><span class="line">    KeyedStream&lt;Integer, Integer&gt; keyedStream = transform.keyBy(<span class="keyword">new</span> KeySelector&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    keyedStream.print();</span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="二-StreamGraph生成中的关键对象"><a href="#二-StreamGraph生成中的关键对象" class="headerlink" title="二. StreamGraph生成中的关键对象"></a>二. StreamGraph生成中的关键对象</h1><h2 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h2><blockquote>
<p>对应类：org.apache.flink.streaming.api.datastream.DataStream</p>
</blockquote>
<p>DataStream有两层含义，一是动态的数据流，二是用户编程接口。</p>
<ol>
<li>动态的数据流：DataStream对象底层持有一个Transformation对象，表示数据流。</li>
<li>用户编程接口：DataStream类定义诸如map、keyBy、addSink等方法，用户调用这些接口在动态的数据流上进行数据转换。</li>
</ol>
<h2 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h2><blockquote>
<p>对应类: org.apache.flink.api.dag.Transformation</p>
</blockquote>
<p>Transformation表示从一个或多个DataStream生成一个新的DataStream的转换，DataStream底层就是一个Transformation。</p>
<p>Transformation的类图如下：<br><img src="/uploads/20210730-transformations.png" alt></p>
<ul>
<li><code>LegacySourceTransformation/SourceTransformation</code>：生成Source流的Transformation</li>
<li><code>OneInputTransformation</code>：接受一个输入参数的Transformation</li>
<li><code>LegacySinkTransformation/SinkTransformation</code>：生成Sink流的Transformation</li>
</ul>
<h2 id="StreamOperator"><a href="#StreamOperator" class="headerlink" title="StreamOperator"></a>StreamOperator</h2><blockquote>
<p>对应类: org.apache.flink.streaming.api.operators.StreamOperator</p>
</blockquote>
<p>如果说Transformation可以代表一个数据流，那么StreamOperator则表示对数据流的转换操作</p>
<p>可以简单的理解为<code>Transformation(out) = Transformation(source) + StreamOperator</code>，但也存在特例，部分<code>Transformation</code>只包含逻辑上的数据流变化，不包含对于数据本身的加工，比如对数据流的partition。从<code>Transformation</code>的具体实现类可以看出，大部分实现都持有<code>Operator</code>属性，部分逻辑操作，比如<code>PartitionTransformation</code>, <code>FeedbackTransformation</code>, <code>UnionTransformation</code>不持有<code>Operator</code></p>
<p>下图中的AbstractUdfStreamOperator封装了用户的UDF代码，即我们日常开发的flink程序中的map/filter/sink等各类操作都被包装在AbstractUdfStreamOperator的某个子类中提交给Flink<br><img src="/uploads/20210730-StreamOperator.png" alt></p>
<h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><blockquote>
<p>对应类: org.apache.flink.api.common.functions.Function</p>
</blockquote>
<p>用户对数据流操作的具体函数实现，典型的如MapFunction/FilterFunction等</p>
<h2 id="StreamGraph-Transformation-StreamOperator-Function关系"><a href="#StreamGraph-Transformation-StreamOperator-Function关系" class="headerlink" title="StreamGraph/Transformation/StreamOperator/Function关系"></a>StreamGraph/Transformation/StreamOperator/Function关系</h2><ol>
<li><code>StreamGraph</code>基于<code>Transformation</code>的关系树生成。</li>
<li><code>Transformation</code>实例拥有<code>StreamOperator</code>属性，描述对Stream的转换方式。</li>
<li><code>StreamOperator</code>实例拥有<code>Function</code>属性，接受开发者的具体实现。</li>
</ol>
<h1 id="三-源码跟踪"><a href="#三-源码跟踪" class="headerlink" title="三. 源码跟踪"></a>三. 源码跟踪</h1><h2 id="创建环境"><a href="#创建环境" class="headerlink" title="创建环境"></a>创建环境</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#getExecutionEnvironment()</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> StreamExecutionEnvironment <span class="title">getExecutionEnvironment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> getExecutionEnvironment(<span class="keyword">new</span> Configuration());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#getExecutionEnvironment(org.apache.flink.configuration.Configuration)</span></span><br><span class="line"><span class="comment">// 本地执行时，调用StreamExecutionEnvironment.createLocalEnvironment(configuration)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> StreamExecutionEnvironment <span class="title">getExecutionEnvironment</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Utils.resolveFactory(threadLocalContextEnvironmentFactory, contextEnvironmentFactory)</span><br><span class="line">            .map(factory -&gt; factory.createExecutionEnvironment(configuration))</span><br><span class="line">            .orElseGet(() -&gt; StreamExecutionEnvironment.createLocalEnvironment(configuration));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#createLocalEnvironment(org.apache.flink.configuration.Configuration)</span></span><br><span class="line"><span class="comment">// 创建LocalStreamEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (configuration.getOptional(CoreOptions.DEFAULT_PARALLELISM).isPresent()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LocalStreamEnvironment(configuration);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        Configuration copyOfConfiguration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        copyOfConfiguration.addAll(configuration);</span><br><span class="line">        copyOfConfiguration.set(CoreOptions.DEFAULT_PARALLELISM, defaultLocalParallelism);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LocalStreamEnvironment(copyOfConfiguration);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注: LocalStreamEnvironment</span></span><br><span class="line"><span class="comment">// LocalStreamEnvironment继承自StreamExecutionEnvironment，StreamExecutionEnvironment中包含Transformation的list</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamExecutionEnvironment</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> List&lt;Transformation&lt;?&gt;&gt; transformations = <span class="keyword">new</span> ArrayList&lt;&gt;();  <span class="comment">// 持有transformations</span></span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="构建source-stream"><a href="#构建source-stream" class="headerlink" title="构建source stream"></a>构建source stream</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line">DataStream&lt;Integer&gt; source = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#fromElements(OUT...)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> &lt;OUT&gt; <span class="function">DataStreamSource&lt;OUT&gt; <span class="title">fromElements</span><span class="params">(OUT... data)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fromCollection(Arrays.asList(data), typeInfo);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#fromCollection(java.util.Collection&lt;OUT&gt;, org.apache.flink.api.common.typeinfo.TypeInformation&lt;OUT&gt;)</span></span><br><span class="line"><span class="keyword">public</span> &lt;OUT&gt; <span class="function">DataStreamSource&lt;OUT&gt; <span class="title">fromCollection</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Collection&lt;OUT&gt; data, TypeInformation&lt;OUT&gt; typeInfo)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    SourceFunction&lt;OUT&gt; function;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 这里实现了Function</span></span><br><span class="line">        <span class="comment">// FromElementsFunction实现SourceFunction接口，其run方法从输入参数中的data逐个的发出数据element</span></span><br><span class="line">        function = <span class="keyword">new</span> FromElementsFunction&lt;&gt;(typeInfo.createSerializer(getConfig()), data);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> addSource(function, <span class="string">"Collection Source"</span>, typeInfo, Boundedness.BOUNDED)</span><br><span class="line">            .setParallelism(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#addSource(org.apache.flink.streaming.api.functions.source.SourceFunction&lt;OUT&gt;, java.lang.String, org.apache.flink.api.common.typeinfo.TypeInformation&lt;OUT&gt;, org.apache.flink.api.connector.source.Boundedness)</span></span><br><span class="line"><span class="keyword">private</span> &lt;OUT&gt; <span class="function">DataStreamSource&lt;OUT&gt; <span class="title">addSource</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> SourceFunction&lt;OUT&gt; function,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> String sourceName,</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nullable <span class="keyword">final</span> TypeInformation&lt;OUT&gt; typeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Boundedness boundedness)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里把FromElementsFunction用StreamSource包起来</span></span><br><span class="line">    <span class="comment">// StreamSource是AbstractUdfStreamOperator实现类，即StreamOperator的实现类</span></span><br><span class="line">    <span class="keyword">final</span> StreamSource&lt;OUT, ?&gt; sourceOperator = <span class="keyword">new</span> StreamSource&lt;&gt;(function);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DataStreamSource&lt;&gt;(</span><br><span class="line">            <span class="keyword">this</span>, resolvedTypeInfo, sourceOperator, isParallel, sourceName, boundedness);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注: AbstractUdfStreamOperator中持有UDF对象</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractUdfStreamOperator</span>&lt;<span class="title">OUT</span>, <span class="title">F</span> <span class="keyword">extends</span> <span class="title">Function</span>&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">extends</span> <span class="title">AbstractStreamOperator</span>&lt;<span class="title">OUT</span>&gt; <span class="keyword">implements</span> <span class="title">OutputTypeConfigurable</span>&lt;<span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The user function. */</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> F userFunction;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">AbstractUdfStreamOperator</span><span class="params">(F userFunction)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userFunction = requireNonNull(userFunction);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. org.apache.flink.streaming.api.datastream.DataStreamSource#DataStreamSource(org.apache.flink.streaming.api.environment.StreamExecutionEnvironment, org.apache.flink.api.common.typeinfo.TypeInformation&lt;T&gt;, org.apache.flink.streaming.api.operators.StreamSource&lt;T,?&gt;, boolean, java.lang.String, org.apache.flink.api.connector.source.Boundedness)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">DataStreamSource</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamExecutionEnvironment environment,</span></span></span><br><span class="line"><span class="function"><span class="params">        TypeInformation&lt;T&gt; outTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamSource&lt;T, ?&gt; operator,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> isParallel,</span></span></span><br><span class="line"><span class="function"><span class="params">        String sourceName,</span></span></span><br><span class="line"><span class="function"><span class="params">        Boundedness boundedness)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(</span><br><span class="line">            environment,</span><br><span class="line">            <span class="comment">// 这里构建了LegacySourceTransformation（Transformation的实现类）</span></span><br><span class="line">            <span class="comment">// 传入StreamOperator（上面构造的StreamSource）</span></span><br><span class="line">            <span class="keyword">new</span> LegacySourceTransformation&lt;&gt;(</span><br><span class="line">                    sourceName,</span><br><span class="line">                    operator,</span><br><span class="line">                    outTypeInfo,</span><br><span class="line">                    environment.getParallelism(),</span><br><span class="line">                    boundedness));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.isParallel = isParallel;</span><br><span class="line">    <span class="keyword">if</span> (!isParallel) &#123;</span><br><span class="line">        setParallelism(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建LegacySourceTransformation</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LegacySourceTransformation</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">PhysicalTransformation</span>&lt;<span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">implements</span> <span class="title">WithBoundedness</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> StreamOperatorFactory&lt;T&gt; operatorFactory;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LegacySourceTransformation</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            String name,</span></span></span><br><span class="line"><span class="function"><span class="params">            StreamSource&lt;T, ?&gt; operator,</span></span></span><br><span class="line"><span class="function"><span class="params">            TypeInformation&lt;T&gt; outputType,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">            Boundedness boundedness)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 使用Operator工厂将传入的StreamSource存储起来</span></span><br><span class="line">        <span class="keyword">this</span>(name, SimpleOperatorFactory.of(operator), outputType, parallelism, boundedness);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结一下，<code>env.fromElements(1, 2, 3, 4, 5)</code>做了这些事：</p>
<ol>
<li>创建<code>Function</code>：用一个内置的<code>FromElementsFunction</code>作为<code>DataStream</code>的源。</li>
<li>创建<code>StreamOperator</code>：把<code>FromElementsFunction</code>包装进<code>StreamSource</code>中。</li>
<li>创建<code>Transformation</code>，把<code>StreamSource</code>包装进<code>LegacySourceTransformation</code>中。</li>
</ol>
<p>到此为止，刚才创建的Transformation还没有与其他地方产生联系，继续往下分析代码。</p>
<h2 id="map转换"><a href="#map转换" class="headerlink" title="map转换"></a>map转换</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line">DataStream&lt;Integer&gt; transform = source.map(x -&gt; x * x);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.datastream.DataStream#map(org.apache.flink.api.common.functions.MapFunction&lt;T,R&gt;)</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span><span class="params">(MapFunction&lt;T, R&gt; mapper)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 参数mapper是lambda表达式 x -&gt; x * x, 获取返回值类型</span></span><br><span class="line">    TypeInformation&lt;R&gt; outType =</span><br><span class="line">            TypeExtractor.getMapReturnTypes(</span><br><span class="line">                    clean(mapper), getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> map(mapper, outType);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.datastream.DataStream#map(org.apache.flink.api.common.functions.MapFunction&lt;T,R&gt;, org.apache.flink.api.common.typeinfo.TypeInformation&lt;R&gt;)</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            MapFunction&lt;T, R&gt; mapper, TypeInformation&lt;R&gt; outputType)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 这里将mapper function用StreamMap这个StreamOperator包装起来</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Map"</span>, outputType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. org.apache.flink.streaming.api.datastream.DataStream#doTransform</span></span><br><span class="line"><span class="keyword">protected</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">doTransform</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        String operatorName,</span></span></span><br><span class="line"><span class="function"><span class="params">        TypeInformation&lt;R&gt; outTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamOperatorFactory&lt;R&gt; operatorFactory)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    OneInputTransformation&lt;T, R&gt; resultTransform =</span><br><span class="line">            <span class="keyword">new</span> OneInputTransformation&lt;&gt;( <span class="comment">// 用OneInputTransformation将StreamMap包装起来（operatorFactory），并且传入this.transformation作为新建的OneInputTransformation的input</span></span><br><span class="line">                    <span class="keyword">this</span>.transformation,</span><br><span class="line">                    operatorName,</span><br><span class="line">                    operatorFactory,</span><br><span class="line">                    outTypeInfo,</span><br><span class="line">                    environment.getParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将resultTransform转换为SingleOutputStreamOperator（SingleOutputStreamOperator是DataStream）</span></span><br><span class="line">    SingleOutputStreamOperator&lt;R&gt; returnStream =</span><br><span class="line">            <span class="keyword">new</span> SingleOutputStreamOperator(environment, resultTransform);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将resultTransform加入到environment的transformations列表中</span></span><br><span class="line">    getExecutionEnvironment().addOperator(resultTransform);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> returnStream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结一下，<code>source.map(x -&gt; x * x)</code>做了这些事：</p>
<ol>
<li>基于lambda function创建<code>StreamOperator</code>（<code>StreamMap</code>）和<code>Transformation</code>（<code>OneInputTransformation</code>）。</li>
<li>将上个创建的<code>LegacySourceTransformation</code>作为input，用于构建本轮创建的<code>OneInputTransformation</code>。</li>
<li>将<code>Transformation</code>加入到transformations列表中。</li>
</ol>
<h2 id="数据partition"><a href="#数据partition" class="headerlink" title="数据partition"></a>数据partition</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line">KeyedStream&lt;Integer, Integer&gt; keyedStream = transform.keyBy(<span class="keyword">new</span> KeySelector&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.datastream.KeyedStream#KeyedStream(org.apache.flink.streaming.api.datastream.DataStream&lt;T&gt;, org.apache.flink.api.java.functions.KeySelector&lt;T,KEY&gt;, org.apache.flink.api.common.typeinfo.TypeInformation&lt;KEY&gt;)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KeyedStream</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        DataStream&lt;T&gt; dataStream,</span></span></span><br><span class="line"><span class="function"><span class="params">        KeySelector&lt;T, KEY&gt; keySelector,</span></span></span><br><span class="line"><span class="function"><span class="params">        TypeInformation&lt;KEY&gt; keyType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(</span><br><span class="line">            dataStream,</span><br><span class="line">            <span class="keyword">new</span> PartitionTransformation&lt;&gt;( <span class="comment">// 创建PartitionTransformation</span></span><br><span class="line">                    dataStream.getTransformation(),</span><br><span class="line">                    <span class="keyword">new</span> KeyGroupStreamPartitioner&lt;&gt;(</span><br><span class="line">                            keySelector,</span><br><span class="line">                            StreamGraphGenerator.DEFAULT_LOWER_BOUND_MAX_PARALLELISM)),</span><br><span class="line">            keySelector,</span><br><span class="line">            keyType);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.datastream.KeyedStream#KeyedStream(org.apache.flink.streaming.api.datastream.DataStream&lt;T&gt;, org.apache.flink.streaming.api.transformations.PartitionTransformation&lt;T&gt;, org.apache.flink.api.java.functions.KeySelector&lt;T,KEY&gt;, org.apache.flink.api.common.typeinfo.TypeInformation&lt;KEY&gt;)</span></span><br><span class="line"><span class="comment">// 3. org.apache.flink.streaming.api.datastream.DataStream#DataStream</span></span><br><span class="line">KeyedStream(</span><br><span class="line">        DataStream&lt;T&gt; stream,</span><br><span class="line">        PartitionTransformation&lt;T&gt; partitionTransformation,</span><br><span class="line">        KeySelector&lt;T, KEY&gt; keySelector,</span><br><span class="line">        TypeInformation&lt;KEY&gt; keyType) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">super</span>(stream.getExecutionEnvironment(), partitionTransformation);</span><br><span class="line">    <span class="keyword">this</span>.keySelector = clean(keySelector); <span class="comment">// 保存返回流的keySelector/environment/transformation等信息</span></span><br><span class="line">    <span class="keyword">this</span>.keyType = validateKeyType(keyType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DataStream</span><span class="params">(StreamExecutionEnvironment environment, Transformation&lt;T&gt; transformation)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.environment =</span><br><span class="line">            Preconditions.checkNotNull(environment, <span class="string">"Execution Environment must not be null."</span>);</span><br><span class="line">    <span class="keyword">this</span>.transformation =</span><br><span class="line">            Preconditions.checkNotNull(</span><br><span class="line">                    transformation, <span class="string">"Stream Transformation must not be null."</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结一下<code>transform.keyBy</code>做了这些事：</p>
<ol>
<li>根据source stream对应的<code>Transformation</code>（即<code>source.map(x -&gt; x * x)</code>的结果）和当前传入的<code>KeySelector</code>构建<code>PartitionTransformation</code></li>
</ol>
<h2 id="Print-Sink"><a href="#Print-Sink" class="headerlink" title="Print Sink"></a>Print Sink</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line">transform.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.datastream.DataStream#print()</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStreamSink&lt;T&gt; <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// PrintSinkFunction是一类sink function，其invoke()方法将数据打印到标准输出上</span></span><br><span class="line">    PrintSinkFunction&lt;T&gt; printFunction = <span class="keyword">new</span> PrintSinkFunction&lt;&gt;();</span><br><span class="line">    <span class="keyword">return</span> addSink(printFunction).name(<span class="string">"Print to Std. Out"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.datastream.DataStream#addSink</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStreamSink&lt;T&gt; <span class="title">addSink</span><span class="params">(SinkFunction&lt;T&gt; sinkFunction)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// StreamSink是AbstractUdfStreamOperator的实现类</span></span><br><span class="line">    StreamSink&lt;T&gt; sinkOperator = <span class="keyword">new</span> StreamSink&lt;&gt;(clean(sinkFunction));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataStreamSink的构造方法初始化LegacySinkTransformation</span></span><br><span class="line">    DataStreamSink&lt;T&gt; sink = <span class="keyword">new</span> DataStreamSink&lt;&gt;(<span class="keyword">this</span>, sinkOperator);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将sink的LegacySinkTransformation对象加入到env的transformations中</span></span><br><span class="line">    getExecutionEnvironment().addOperator(sink.getTransformation());</span><br><span class="line">    <span class="keyword">return</span> sink;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注：DataStreamSink的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">DataStreamSink</span><span class="params">(DataStream&lt;T&gt; inputStream, StreamSink&lt;T&gt; operator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.transformation =</span><br><span class="line">            (PhysicalTransformation&lt;T&gt;)</span><br><span class="line">                    <span class="keyword">new</span> LegacySinkTransformation&lt;&gt;( <span class="comment">// 传入input的Transformation</span></span><br><span class="line">                            inputStream.getTransformation(),</span><br><span class="line">                            <span class="string">"Unnamed"</span>,</span><br><span class="line">                            operator,</span><br><span class="line">                            inputStream.getExecutionEnvironment().getParallelism());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结一下, <code>transform.print()</code>做了这些事:</p>
<ol>
<li>创建<code>PrintSinkFunction</code>对象，基于<code>PrintSinkFunction</code>对象创建<code>StreamSink</code>这个sink operator</li>
<li>基于StreamSink创建<code>LegacySinkTransformation</code>，其input是上轮创建的<code>KeyedStream</code>。</li>
<li>将<code>LegacySinkTransformation</code>加入到env中</li>
</ol>
<h2 id="提交执行"><a href="#提交执行" class="headerlink" title="提交执行"></a>提交执行</h2><p>前文提到过，<code>StreamGraph</code>是基于<code>Transformation</code>的关系树生成，那么<code>Transformation</code>树是以怎样的数据结构表示呢？首先，所有的<code>Transformation</code>是存储在<code>StreamExecutionEnvironment</code>对象中的<code>List&lt;Transformation&lt;?&gt;&gt; transformations</code>，其次，每个<code>Transformation</code>都有一个全局唯一的<code>idCounter</code>，<code>idCounter</code>=1表示树的根节点，最后，除source外，每个<code>Transformation</code>都有一个（或多个）<code>input Transformation</code>，用于表示其父节点，有了这些信息，就能构建出完整的一颗<code>Transformation</code>树。</p>
<p>根据上面的分析，四行代码创建了四个Transformation，其中第2、4个被加入到env中。看下当前env中Transformation的情况：<br><img src="/uploads/Xnip2021-08-05_14-52-03.jpg" alt></p>
<p><img src="/uploads/Xnip2021-08-05_14-54-33.jpg" alt></p>
<p>经过以上步骤创建的对象关系如下：<br><img src="/uploads/Xnip2021-08-05_15-14-46.jpg" alt></p>
<p>接下来看一下如何基于上面的transformations生成<code>StreamGraph</code>：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户代码</span></span><br><span class="line">env.execute();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#execute()</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> execute(getJobName());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#execute(java.lang.String)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Preconditions.checkNotNull(jobName, <span class="string">"Streaming Job name should not be null."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> execute(getStreamGraph(jobName));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#getStreamGraph(java.lang.String)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">getStreamGraph</span><span class="params">(String jobName)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> getStreamGraph(jobName, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#getStreamGraph(java.lang.String, boolean)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">getStreamGraph</span><span class="params">(String jobName, <span class="keyword">boolean</span> clearTransformations)</span> </span>&#123;</span><br><span class="line">    StreamGraph streamGraph = getStreamGraphGenerator().setJobName(jobName).generate();</span><br><span class="line">    <span class="keyword">if</span> (clearTransformations) &#123;</span><br><span class="line">        <span class="keyword">this</span>.transformations.clear();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> streamGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的代码说明最终是通过调用<code>StreamGraphGenerator</code>的<code>generate()</code>生成<code>StreamGraph</code>,<code>StreamGraph</code>到底是什么呢？ </p>
<p>官方对<code>StreamGraph</code>的定义是：</p>
<blockquote>
<p>Class representing the streaming topology. It contains all the information necessary to build the jobgraph for the execution.（用于表示流拓扑结构的类，包含所有用于构建运行时jobGraph的必要信息）</p>
</blockquote>
<p>包含的“必要信息”举例如下：</p>
<ul>
<li>配置信息：如执行配置<code>ExecutionConfig</code>，checkpoint配置<code>CheckpointConfig</code>，savepoint恢复配置<code>SavepointRestoreSettings</code>，时间属性<code>TimeCharacteristic</code>，状态后端<code>StateBackend</code>等。</li>
<li>拓扑结构：source节点id<code>Set&lt;Integer&gt; sources</code>，sink节点id<code>Set&lt;Integer&gt; sinks</code>，Graph的节点信息<code>Map&lt;Integer, StreamNode&gt; streamNodes</code>等。</li>
</ul>
<p>描述拓扑结构的最重要部分是<code>Map&lt;Integer, StreamNode&gt; streamNodes</code>，其key是<code>Transformation</code>的<code>idCounter</code>，value是<code>StreamNode</code>对象。</p>
<p><code>StreamNode</code>是<code>StreamGraph</code>的节点，关键属性有：</p>
<ol>
<li><code>List&lt;StreamEdge&gt; inEdges</code>, <code>List&lt;StreamEdge&gt; outEdges</code>：该节点输入和输出<ul>
<li><code>StreamEdge</code>表示<code>StreamGraph</code>的边，其主要属性是有：<code>sourceId</code>&amp;<code>targetId</code>：用于标识该边连接的source节点和target节点，<code>outputPartitioner</code>：表示两节点之间按什么规则传输数据。</li>
</ul>
</li>
<li><code>StreamOperatorFactory&lt;?&gt; operatorFactory</code>: 工厂对象，用于提供在该节点上处理数据的operator</li>
</ol>
<p>继续看<code>StreamGraph</code>是如何生成的，<code>StreamGraphGenerator</code>的<code>generate()</code>方法如下:<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">generate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    streamGraph = <span class="keyword">new</span> StreamGraph(executionConfig, checkpointConfig, savepointRestoreSettings); <span class="comment">// 创建一个空的StreamGraph</span></span><br><span class="line">    shouldExecuteInBatchMode = shouldExecuteInBatchMode(runtimeExecutionMode); <span class="comment">// 决定是否是批模式</span></span><br><span class="line">    configureStreamGraph(streamGraph); <span class="comment">// 一些其他配置，如是否Chaining，TimeCharacteristic，StateBackend等</span></span><br><span class="line"></span><br><span class="line">    alreadyTransformed = <span class="keyword">new</span> HashMap&lt;&gt;(); <span class="comment">// 记录已经执行完转换的Transformation</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Transformation&lt;?&gt; transformation : transformations) &#123; <span class="comment">// 对list中的transformation逐个执行transform方法</span></span><br><span class="line">        transform(transformation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> StreamGraph builtStreamGraph = streamGraph;</span><br><span class="line"></span><br><span class="line">    alreadyTransformed.clear();</span><br><span class="line">    alreadyTransformed = <span class="keyword">null</span>;</span><br><span class="line">    streamGraph = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> builtStreamGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>transform</code>的调用链路较深，这里截取关键片段如下：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1. org.apache.flink.streaming.api.graph.StreamGraphGenerator#transform</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(Transformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据Transformation的具体实现类，调用对应的TransformationTranslator的translate对该Transformation进行转换</span></span><br><span class="line">    <span class="keyword">final</span> TransformationTranslator&lt;?, Transformation&lt;?&gt;&gt; translator =</span><br><span class="line">            (TransformationTranslator&lt;?, Transformation&lt;?&gt;&gt;)</span><br><span class="line">                    translatorMap.get(transform.getClass());</span><br><span class="line"></span><br><span class="line">    Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">    <span class="keyword">if</span> (translator != <span class="keyword">null</span>) &#123;</span><br><span class="line">        transformedIds = translate(translator, transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        transformedIds = legacyTransform(transform);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line">    <span class="keyword">return</span> transformedIds;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. org.apache.flink.streaming.api.graph.StreamGraphGenerator#translate</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">translate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> TransformationTranslator&lt;?, Transformation&lt;?&gt;&gt; translator,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Transformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// getParentInputIds是一个递归调用，确保当前Transformation的父Transformation已经执行完translate</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;Collection&lt;Integer&gt;&gt; allInputIds = getParentInputIds(transform.getInputs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 调用具体的translator方法转换</span></span><br><span class="line">    <span class="keyword">return</span> shouldExecuteInBatchMode</span><br><span class="line">            ? translator.translateForBatch(transform, context)</span><br><span class="line">            : translator.translateForStreaming(transform, context);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. org.apache.flink.streaming.api.graph.TransformationTranslator#translateForStreaming</span></span><br><span class="line"><span class="comment">// 4. org.apache.flink.streaming.api.graph.SimpleTransformationTranslator#translateForStreamingInternal</span></span><br><span class="line"><span class="comment">// translateForStreamingInternal对不同的Transformation有不同的实现类，例如上述示例代码中的map操作会调用OneInputTransformationTranslator#translateForStreamingInternal</span></span><br><span class="line"><span class="comment">// 最终执行的方法是：AbstractOneInputTransformationTranslator#translateInternal</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Collection&lt;Integer&gt; <span class="title">translateInternal</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Transformation&lt;OUT&gt; transformation,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> StreamOperatorFactory&lt;OUT&gt; operatorFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> TypeInformation&lt;IN&gt; inputType,</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nullable <span class="keyword">final</span> KeySelector&lt;IN, ?&gt; stateKeySelector,</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nullable <span class="keyword">final</span> TypeInformation&lt;?&gt; stateKeyType,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Context context)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> StreamGraph streamGraph = context.getStreamGraph();</span><br><span class="line">    <span class="keyword">final</span> String slotSharingGroup = context.getSlotSharingGroup();</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> transformationId = transformation.getId();</span><br><span class="line">    <span class="keyword">final</span> ExecutionConfig executionConfig = streamGraph.getExecutionConfig();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为streamGraph添加StreamNode，过程中保留该StreamNode的operatorFactory</span></span><br><span class="line">    streamGraph.addOperator(</span><br><span class="line">            transformationId,</span><br><span class="line">            slotSharingGroup,</span><br><span class="line">            transformation.getCoLocationGroupKey(),</span><br><span class="line">            operatorFactory,</span><br><span class="line">            inputType,</span><br><span class="line">            transformation.getOutputType(),</span><br><span class="line">            transformation.getName());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (stateKeySelector != <span class="keyword">null</span>) &#123;</span><br><span class="line">        TypeSerializer&lt;?&gt; keySerializer = stateKeyType.createSerializer(executionConfig);</span><br><span class="line">        streamGraph.setOneInputStateKey(transformationId, stateKeySelector, keySerializer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为streamGraph添加StreamEdge</span></span><br><span class="line">    <span class="keyword">for</span> (Integer inputId : context.getStreamNodeIds(parentTransformations.get(<span class="number">0</span>))) &#123;</span><br><span class="line">        streamGraph.addEdge(inputId, transformationId, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Collections.singleton(transformationId);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// addOperator最终调用org.apache.flink.streaming.api.graph.StreamGraph#addNode方法为StreamGraph添加node</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> StreamNode <span class="title">addNode</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Integer vertexID,</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nullable String slotSharingGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nullable String coLocationGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">        Class&lt;? extends AbstractInvokable&gt; vertexClass,</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamOperatorFactory&lt;?&gt; operatorFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">        String operatorName)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (streamNodes.containsKey(vertexID)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Duplicate vertexID "</span> + vertexID);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    StreamNode vertex =</span><br><span class="line">            <span class="keyword">new</span> StreamNode(</span><br><span class="line">                    vertexID,</span><br><span class="line">                    slotSharingGroup,</span><br><span class="line">                    coLocationGroup,</span><br><span class="line">                    operatorFactory,</span><br><span class="line">                    operatorName,</span><br><span class="line">                    vertexClass);</span><br><span class="line"></span><br><span class="line">    streamNodes.put(vertexID, vertex);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vertex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对于<code>keyBy</code>对应的<code>PartitionTransformation</code>，其<code>translateInternal</code>调用的是<code>PartitionTransformationTranslator#translateInternal</code>方法，向streamGraph中加入一个虚拟节点（virtualPartitionNodes属性）,当下游添加边时，会把虚拟节点上的partition信息加入到边属性中。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">translateInternal</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> PartitionTransformation&lt;OUT&gt; transformation, <span class="keyword">final</span> Context context)</span> </span>&#123;</span><br><span class="line">    checkNotNull(transformation);</span><br><span class="line">    checkNotNull(context);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> StreamGraph streamGraph = context.getStreamGraph();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> List&lt;Transformation&lt;?&gt;&gt; parentTransformations = transformation.getInputs();</span><br><span class="line">    checkState(</span><br><span class="line">            parentTransformations.size() == <span class="number">1</span>,</span><br><span class="line">            <span class="string">"Expected exactly one input transformation but found "</span></span><br><span class="line">                    + parentTransformations.size());</span><br><span class="line">    <span class="keyword">final</span> Transformation&lt;?&gt; input = parentTransformations.get(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    List&lt;Integer&gt; resultIds = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Integer inputId : context.getStreamNodeIds(input)) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> virtualId = Transformation.getNewNodeId();</span><br><span class="line">        streamGraph.addVirtualPartitionNode( <span class="comment">// 向streamGraph中加入一个虚拟节点</span></span><br><span class="line">                inputId,</span><br><span class="line">                virtualId,</span><br><span class="line">                transformation.getPartitioner(),</span><br><span class="line">                transformation.getShuffleMode());</span><br><span class="line">        resultIds.add(virtualId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> resultIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>总结一下主要方法的作用：</p>
<ul>
<li><code>StreamGraphGenerator#generate</code>：生成<code>StreamGraph</code>的主方法，遍历env中记录的<code>Transformation</code>，逐个调用<code>StreamGraphGenerator#transform</code></li>
<li><code>StreamGraphGenerator#transform</code>: 对单个<code>Transformation</code>进行处理，从静态translatorMap中获取对应Transformation的处理类，并把处理类对象和该Transformation本身传入<code>StreamGraphGenerator#translate</code></li>
<li><code>StreamGraphGenerator#translate</code>: 透传调用具体的处理类，最终调用<code>XXXTransformationTranslator#translateInternal</code></li>
<li><code>XXXTransformationTranslator#translateInternal</code>: 负责创建节点，创建边，节点和边的联系等</li>
</ul>
<p>详细调用过程：</p>
<ol>
<li>在<code>StreamGraphGenerator#generate</code>遍历<code>Transformation</code> list，首先处理<code>tid=2</code>的transformation，调用<code>StreamGraphGenerator#transform</code>方法</li>
<li>从translatorMap中获取到对应的translator是<code>OneInputTransformationTranslator</code>,把该translator和<code>tid=2</code>的transformation作为参数传入<code>StreamGraphGenerator#translate</code>方法</li>
<li>StreamGraphGenerator#translate首先调用<code>StreamGraphGenerator#getParentInputIds</code>，传入<code>tid=2</code>的父节点，即<code>tid=1</code>的transformation，在此方法中会递归调用<code>StreamGraphGenerator#transform</code>，此时处理<code>tid=1</code>的transformation</li>
<li>从translatorMap中获取到对应的translator是<code>LegacySourceTransformationTranslator</code>,把该translator和<code>tid=1</code>的transformation作为参数传入<code>StreamGraphGenerator#translate</code>方法</li>
<li>StreamGraphGenerator#translate所调用的<code>StreamGraphGenerator#getParentInputIds</code>递归方法，发现该节点没有父节点，返回后，真正调用<code>LegacySourceTransformationTranslator#translateInternal</code>方法，传入<code>tid=1</code>的transformation为参数</li>
<li><code>LegacySourceTransformationTranslator#translateInternal</code>,调用过程中：<ol>
<li>根据src节点的信息创建一个<strong>StreamNode</strong>,<code>nid=1</code>,并把该node放进StreamGraph的<code>Map&lt;Integer, StreamNode&gt; streamNodes</code>属性中</li>
<li>在StreamGraph的<code>Set&lt;Integer&gt; sources</code>属性中加入source节点id</li>
</ol>
</li>
<li>回到第3步，<code>tid=1</code>的transformation处理完成，将该transform加入到<code>alreadyTransformed</code>标记已处理。第3步处理完成，回到第2步的<code>StreamGraphGenerator#translate</code>继续处理<code>tid=2</code>的transformation</li>
<li><code>tid=2</code>transformation的translate真正调用的是<code>AbstractOneInputTransformationTranslator#translateInternal</code>,调用过程中：<ol>
<li>根据该transformation的operator等信息创建一个<strong>StreamNode</strong>,<code>nid=2</code>，并把该node放进StreamGraph的<code>Map&lt;Integer, StreamNode&gt; streamNodes</code>属性中</li>
<li>对<code>nid=2</code>的父节点（即<code>nid=1</code>节点），调用StreamGraph#addEdge，最终调用<code>StreamGraph#addEdgeInternal</code>方法，在该方法中获取<code>nid=1</code>和<code>nid=2</code>的StreamNode，创建一条<strong>StreamEdge</strong>连接这两个StreamNode，并根据一定的规则，将StreamEdge分发数据的规则设置为<code>ForwardPartitioner</code>，最后把该StreamEdge分别加入<code>nid=1</code>和<code>nid=2</code>的StreamNode的出边和入边</li>
</ol>
</li>
<li>回到第1步，<code>tid=2</code>的transformation处理完成，将该transformation加入到<code>alreadyTransformed</code>标记已处理，env的transformation列表有2个元素，处理完<code>tid=2</code>之后，开始处理<code>tid=4</code>的<code>LegacySinkTransformation</code>,同样，依次调用<code>StreamGraphGenerator#transform</code>和<code>StreamGraphGenerator#translate</code></li>
<li>再次进入<code>StreamGraphGenerator#getParentInputIds</code>递归方法，发现<code>tid=4</code>的父节点是<code>tid=3</code>的<code>PartitionTransformation</code>，所以递归进入<code>StreamGraphGenerator#transform</code>开始处理<code>tid=3</code>的transformation，再次对<code>tid=3</code>调用<code>StreamGraphGenerator#transform</code>和<code>StreamGraphGenerator#translate</code></li>
<li>最终执行<code>PartitionTransformationTranslator#translateInternal</code>,调用过程中：<ol>
<li>获取<code>tid=3</code>节点的父节点，即<code>tid=2</code>的节点</li>
<li>创建一个<strong>虚拟节点</strong>virtualPartitionNode（<code>nid=5</code>,<code>tid=3</code>），该虚拟节点的<strong>inputId=2</strong>，并把StreamPartitioner的信息记录在虚拟节点中，表示inputId=2的节点到当前虚拟节点是通过哈希分发数据</li>
<li>将该虚拟节点加入到StreamGraph的记录虚拟节点的数据结构<code>Map&lt;Integer, Tuple3&lt;Integer, StreamPartitioner&lt;?&gt;, ShuffleMode&gt;&gt; virtualPartitionNodes</code></li>
</ol>
</li>
<li>回到第10步，<code>tid=3</code>的transformation处理完成，将该transformation加入到<code>alreadyTransformed</code>标记已处理</li>
<li>回到第9步，继续处理<code>tid=4</code>的transformation，父节点处理完成后，接着真正调用id=4的translator处理，具体执行方法是<code>LegacySinkTransformationTranslator#translateInternal</code>，调用过程中：<ol>
<li>获取<code>tid=4</code>节点的父节点，即<code>tid=3</code>的节点</li>
<li>根据该<code>LegacySinkTransformation</code>的信息创建一个<strong>StreamNode</strong>, <code>nid=4</code>, 并把该node放进StreamGraph的<code>Map&lt;Integer, StreamNode&gt; streamNodes</code>属性中</li>
<li>在StreamGraph的<code>Set&lt;Integer&gt; sinks</code>属性中加入到sink节点id</li>
<li>将父节点与当前节点传入<code>StreamGraph#addEdgeInternal</code>创建边，注意传入的父节点是对应的虚拟节点<code>nid=5</code></li>
<li><code>StreamGraph#addEdgeInternal</code>中发现<code>nid=5</code>是一个虚拟节点（在<code>virtualPartitionNodes</code>中），将该虚拟节点的真正上游（<strong>inputId=2</strong>）和partitioner取出来，递归调用<code>StreamGraph#addEdgeInternal</code></li>
<li>本次调用<code>StreamGraph#addEdgeInternal</code>的入参分别是<code>nid=2</code>和<code>nid=4</code>,并传入了partitioner参数</li>
<li>创建一条<strong>StreamEdge</strong>保留了上下游<code>nid=2</code>、<code>nid=4</code>、partitioner，并把改变分别加入到<code>nid=2</code>和<code>nid=4</code>出边和入边</li>
</ol>
</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文主要讲解如何从用户代码生成<code>StreamGraph</code>的过程。涉及到多个日常开发不会接触的概念，如<code>Transformation</code>, <code>StreamOperator</code>, <code>StreamNode</code>, <code>StreamEdge</code>等。</p>
<p><code>StreamGraph</code>可以通过<code>env.getStreamGraph().getStreamingPlanAsJSON()</code>获取json格式的输出，在 <a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">http://flink.apache.org/visualizer/</a> 进行可视化展示。</p>
<p>例如，上面例子的streamGraph为:</p>
<p>具体图形为：<br><img src="/uploads/Xnip2021-08-05_17-52-20.jpg" alt></p>
<p>参考文章：</p>
<ol>
<li><a href="https://wuchong.me/blog/2016/05/04/flink-internal-how-to-build-streamgraph/" target="_blank" rel="noopener">https://wuchong.me/blog/2016/05/04/flink-internal-how-to-build-streamgraph/</a></li>
<li><a href="https://matt33.com/2019/12/08/flink-stream-graph-2/" target="_blank" rel="noopener">https://matt33.com/2019/12/08/flink-stream-graph-2/</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Flink执行流程(三)：生成ExecutionGraph</title>
    <url>/2021/08/04/Flink%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B-%E4%B8%89/</url>
    <content><![CDATA[<p>前面的文章介绍了从用户代码生成StreamGraph、JobGraph的过程，这两部分都是在Client端完成的。Flink借助akka actor实现进程间的rpc调用，下一步是client将JobGraph对象发送给JobManager，JobManager将JobGraph转换成并行化的ExecutionGraph。</p>
<p>介绍如何生成ExecutionGraph之前，先引入ExecutionGraph中的几个概念：</p>
<ul>
<li><code>ExecutionJobVertex</code> : ExecutionGraph中的节点，与JobGraph中的<code>JobVertex</code>一一对应，表示一个task。</li>
<li><code>ExecutionVertex</code> : 表示一个subtask，一个<code>ExecutionJobVertex</code>对应一个或多个<code>ExecutionVertex</code>（取决于该task的并行度）。</li>
<li><code>Execution</code> : 表示一个<code>ExecutionVertex</code>的一次尝试执行，当任务失败时<code>ExecutionVertex</code>可能会被调起多次，每次执行被称为一个<code>Execution</code>，由<code>ExecutionAttemptID</code>唯一标识。</li>
</ul>
<p>再看一下主要数据结构:</p>
<ol>
<li><p><code>ExecutionGraph</code><br>主要由一个map和一个list表达，其元素都是节点<code>ExecutionJobVertex</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutionGraph</span> <span class="keyword">implements</span> <span class="title">AccessExecutionGraph</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 存储所有ExecutionJobVertex</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;JobVertexID, ExecutionJobVertex&gt; tasks;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照创建顺序存储ExecutionJobVertex</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;ExecutionJobVertex&gt; verticesInCreationOrder;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>ExecutionJobVertex</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutionJobVertex</span></span></span><br><span class="line"><span class="class">        <span class="keyword">implements</span> <span class="title">AccessExecutionJobVertex</span>, <span class="title">Archiveable</span>&lt;<span class="title">ArchivedExecutionJobVertex</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> JobVertex jobVertex;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ExecutionVertex[] taskVertices;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntermediateResult[] producedDataSets;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;IntermediateResult&gt; inputs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>主要包含：</p>
<ul>
<li>与该<code>ExecutionJobVertex</code>一一对应的<code>JobVertex</code></li>
<li>该<code>ExecutionJobVertex</code>对应的多个<code>ExecutionVertex[]</code>（多个subtask）</li>
<li>该<code>ExecutionJobVertex</code>产出的多个中间结果<code>IntermediateResult[]</code>（与<code>JobGraph</code>中的<code>IntermediateDataSet</code>类似）</li>
<li>该<code>ExecutionJobVertex</code>的多个输入数据源<code>List&lt;IntermediateResult&gt;</code></li>
</ul>
<ol start="3">
<li><code>ExecutionVertex</code><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutionVertex</span></span></span><br><span class="line"><span class="class">        <span class="keyword">implements</span> <span class="title">AccessExecutionVertex</span>, <span class="title">Archiveable</span>&lt;<span class="title">ArchivedExecutionVertex</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;IntermediateResultPartitionID, IntermediateResultPartition&gt; resultPartitions;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ExecutionEdge[][] inputEdges; <span class="comment">// 二维数组，第一维是上游来自哪个task，第二维是来自该task下的哪些subtask</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> subTaskIndex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>主要包含：</p>
<ul>
<li>该subtask产出的结果数据</li>
<li>输入边ExecutionEdge</li>
<li>index</li>
</ul>
<ol start="4">
<li><code>ExecutionEdge</code><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutionEdge</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntermediateResultPartition source;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ExecutionVertex target;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>ExecutionEdge</code>表示<code>ExecutionGraph</code>的边，主要包含该边产出的源数据source，及该边连接的下游节点target</p>
<ol start="5">
<li><code>IntermediateResult</code><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntermediateResult</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ExecutionJobVertex producer;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntermediateResultPartition[] partitions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>IntermediateResult</code>对应<code>JobGraph</code>中的<code>IntermediateDataSet</code>，用于表示一个task所产出的一个或多个数据流</p>
<ol start="6">
<li><code>IntermediateResultPartition</code><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntermediateResultPartition</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntermediateResult totalResult;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ExecutionVertex producer;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partitionNumber;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntermediateResultPartitionID partitionId;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;List&lt;ExecutionEdge&gt;&gt; consumers;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>IntermediateResult</code>在并行化之后，每个subtask所产出的就是一个<code>IntermediateResultPartition</code></p>
<p>以上实体的关系与<code>JobGraph</code>中类似：<br><img src="/uploads/Xnip2021-08-04_20-03-09.jpg" alt></p>
<h1 id="如何生成ExecutionGraph"><a href="#如何生成ExecutionGraph" class="headerlink" title="如何生成ExecutionGraph"></a>如何生成ExecutionGraph</h1><p>生成<code>ExecutionGraph</code>的代码在ExecutionGraphBuilder#buildGraph方法中，截取该方法中主要部分如下：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutionGraph <span class="title">buildGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nullable ExecutionGraph prior,</span></span></span><br><span class="line"><span class="function"><span class="params">        JobGraph jobGraph,</span></span></span><br><span class="line"><span class="function"><span class="params">        Configuration jobManagerConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">        ScheduledExecutorService futureExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">        Executor ioExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">        SlotProvider slotProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">        ClassLoader classLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointRecoveryFactory recoveryFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">        Time rpcTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">        RestartStrategy restartStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">        MetricGroup metrics,</span></span></span><br><span class="line"><span class="function"><span class="params">        BlobWriter blobWriter,</span></span></span><br><span class="line"><span class="function"><span class="params">        Time allocationTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">        Logger log,</span></span></span><br><span class="line"><span class="function"><span class="params">        ShuffleMaster&lt;?&gt; shuffleMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">        JobMasterPartitionTracker partitionTracker,</span></span></span><br><span class="line"><span class="function"><span class="params">        FailoverStrategy.Factory failoverStrategyFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">        ExecutionDeploymentListener executionDeploymentListener,</span></span></span><br><span class="line"><span class="function"><span class="params">        ExecutionStateUpdateListener executionStateUpdateListener,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> initializationTimestamp)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> JobExecutionException, JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 新建一个空的ExecutionGraph</span></span><br><span class="line">    <span class="keyword">final</span> ExecutionGraph executionGraph;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        executionGraph =</span><br><span class="line">                (prior != <span class="keyword">null</span>)</span><br><span class="line">                        ? prior</span><br><span class="line">                        : <span class="keyword">new</span> ExecutionGraph(</span><br><span class="line">                                jobInformation,</span><br><span class="line">                                futureExecutor,</span><br><span class="line">                                ioExecutor,</span><br><span class="line">                                rpcTimeout,</span><br><span class="line">                                restartStrategy,</span><br><span class="line">                                maxPriorAttemptsHistoryLength,</span><br><span class="line">                                failoverStrategyFactory,</span><br><span class="line">                                slotProvider,</span><br><span class="line">                                classLoader,</span><br><span class="line">                                blobWriter,</span><br><span class="line">                                allocationTimeout,</span><br><span class="line">                                partitionReleaseStrategyFactory,</span><br><span class="line">                                shuffleMaster,</span><br><span class="line">                                partitionTracker,</span><br><span class="line">                                jobGraph.getScheduleMode(),</span><br><span class="line">                                executionDeploymentListener,</span><br><span class="line">                                executionStateUpdateListener,</span><br><span class="line">                                initializationTimestamp);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">"Could not create the ExecutionGraph."</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按拓扑顺序，获取所有的JobVertex列表</span></span><br><span class="line">    List&lt;JobVertex&gt; sortedTopology = jobGraph.getVerticesSortedTopologicallyFromSources();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据JobVertex列表，生成execution graph</span></span><br><span class="line">    executionGraph.attachJobGraph(sortedTopology);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> executionGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的核心代码是<code>executionGraph.attachJobGraph(sortedTopology)</code>，截取该方法主要代码如下：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">attachJobGraph</span><span class="params">(List&lt;JobVertex&gt; topologiallySorted)</span> <span class="keyword">throws</span> JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历JobVertex</span></span><br><span class="line">    <span class="keyword">for</span> (JobVertex jobVertex : topologiallySorted) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据JobVertex创建ExecutionJobVertex</span></span><br><span class="line">        ExecutionJobVertex ejv =</span><br><span class="line">                <span class="keyword">new</span> ExecutionJobVertex(</span><br><span class="line">                        <span class="keyword">this</span>,</span><br><span class="line">                        jobVertex,</span><br><span class="line">                        <span class="number">1</span>,</span><br><span class="line">                        maxPriorAttemptsHistoryLength,</span><br><span class="line">                        rpcTimeout,</span><br><span class="line">                        globalModVersion,</span><br><span class="line">                        createTimestamp);</span><br><span class="line">        <span class="comment">// 将创建的ExecutionJobVertex与前置的IntermediateResult连接起来</span></span><br><span class="line">        ejv.connectToPredecessors(<span class="keyword">this</span>.intermediateResults);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>attachJobGraph</code>遍历JobVertex创建ExecutionJobVertex，主要的逻辑在<code>ExecutionJobVertex</code>的构造方法里，截取片段如下：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionJobVertex(</span><br><span class="line">            ExecutionGraph graph,</span><br><span class="line">            JobVertex jobVertex,</span><br><span class="line">            <span class="keyword">int</span> defaultParallelism,</span><br><span class="line">            <span class="keyword">int</span> maxPriorAttemptsHistoryLength,</span><br><span class="line">            Time timeout,</span><br><span class="line">            <span class="keyword">long</span> initialGlobalModVersion,</span><br><span class="line">            <span class="keyword">long</span> createTimestamp)</span><br><span class="line">            <span class="keyword">throws</span> JobException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建IntermediateResult数组</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; jobVertex.getProducedDataSets().size(); i++) &#123;</span><br><span class="line">            <span class="keyword">final</span> IntermediateDataSet result = jobVertex.getProducedDataSets().get(i);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.producedDataSets[i] =</span><br><span class="line">                    <span class="keyword">new</span> IntermediateResult(</span><br><span class="line">                            result.getId(), <span class="keyword">this</span>, numTaskVertices, result.getResultType());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据并行度，创建ExecutionVertex</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numTaskVertices; i++) &#123;</span><br><span class="line">            ExecutionVertex vertex =</span><br><span class="line">                    <span class="keyword">new</span> ExecutionVertex(</span><br><span class="line">                            <span class="keyword">this</span>,</span><br><span class="line">                            i,</span><br><span class="line">                            producedDataSets,</span><br><span class="line">                            timeout,</span><br><span class="line">                            initialGlobalModVersion,</span><br><span class="line">                            createTimestamp,</span><br><span class="line">                            maxPriorAttemptsHistoryLength);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.taskVertices[i] = vertex;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>这里会根据jobVertex产出的<code>IntermediateDataSet</code>生成对应的<code>IntermediateResult</code>，并且根据并行度的多少，创建对应的<code>ExecutionVertex</code>，在构造方法中都传入了<code>this</code>指针，所以会在构造过程中记录<code>ExecutionJobVertex</code>，<code>IntermediateResult</code>, <code>ExecutionVertex</code>三者的联系。</p>
<p>这一步会把<code>ExecutionJobVertex</code>和<code>ExecutionVertex</code>的下游联系建立起来，与上游的联系在<code>ejv.connectToPredecessors(this.intermediateResults);</code>中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connectToPredecessors</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;IntermediateDataSetID, IntermediateResult&gt; intermediateDataSets)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    List&lt;JobEdge&gt; inputs = jobVertex.getInputs();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> num = <span class="number">0</span>; num &lt; inputs.size(); num++) &#123;</span><br><span class="line">        JobEdge edge = inputs.get(num);</span><br><span class="line"></span><br><span class="line">        IntermediateResult ires = intermediateDataSets.get(edge.getSourceId());</span><br><span class="line">        <span class="keyword">if</span> (ires == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobException(</span><br><span class="line">                    <span class="string">"Cannot connect this job graph to the previous graph. No previous intermediate result found for ID "</span></span><br><span class="line">                            + edge.getSourceId());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加ExecutionJobVertex的输入</span></span><br><span class="line">        <span class="keyword">this</span>.inputs.add(ires);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> consumerIndex = ires.registerConsumer();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; parallelism; i++) &#123;</span><br><span class="line">            ExecutionVertex ev = taskVertices[i];</span><br><span class="line">            <span class="comment">// 添加ExecutionVertex的输入</span></span><br><span class="line">            ev.connectSource(num, ires, edge, consumerIndex);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要做的事情是</p>
<ol>
<li>遍历当前<code>ExecutionJobVertex</code>的输入边<code>JobEdge</code>，将每个输入边对应的<code>IntermediateResult</code>加入到<code>ExecutionJobVertex</code>的<code>List&lt;IntermediateResult&gt; inputs</code>中</li>
<li>对当前<code>ExecutionJobVertex</code>节点的每个subtask <code>ExecutionVertex</code>，调用<code>connectSource</code>方法为该subtask设置输入边<code>ExecutionEdge[][] inputEdges</code></li>
</ol>
<p>接着看<code>connectSource</code>方法的实现：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connectSource</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> inputNumber, IntermediateResult source, JobEdge edge, <span class="keyword">int</span> consumerNumber)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> DistributionPattern pattern = edge.getDistributionPattern();</span><br><span class="line">    <span class="keyword">final</span> IntermediateResultPartition[] sourcePartitions = source.getPartitions();</span><br><span class="line"></span><br><span class="line">    ExecutionEdge[] edges;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (pattern) &#123;</span><br><span class="line">        <span class="keyword">case</span> POINTWISE:</span><br><span class="line">            edges = connectPointwise(sourcePartitions, inputNumber);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> ALL_TO_ALL:</span><br><span class="line">            edges = connectAllToAll(sourcePartitions, inputNumber);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Unrecognized distribution pattern."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    inputEdges[inputNumber] = edges;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the consumers to the source</span></span><br><span class="line">    <span class="comment">// for now (until the receiver initiated handshake is in place), we need to register the</span></span><br><span class="line">    <span class="comment">// edges as the execution graph</span></span><br><span class="line">    <span class="keyword">for</span> (ExecutionEdge ee : edges) &#123;</span><br><span class="line">        ee.getSource().addConsumer(ee, consumerNumber);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里的主要逻辑是根据数据分发的模式，调用<code>connectPointwise</code>或<code>connectAllToAll</code>方法生成<code>ExecutionEdge</code>数据，并且赋值给<code>inputEdges</code>。</p>
<p>对于数据分发的模式，Flink中只有2种，<code>ALL_TO_ALL</code>和<code>POINTWISE</code>：</p>
<ul>
<li><code>ALL_TO_ALL</code>：多对多，上游的一个subtask可能把数据发送给所有下游subtask，下游的一个subtask可能接收到所有上游subtask发送的数据。</li>
<li><code>POINTWISE</code>：点对点，上游的一个subtask只能把数据发送给一个（或多个）subtask<br><img src="/uploads/Xnip2021-08-05_12-21-41.jpg" alt></li>
</ul>
<p>Flink根据分区器类型判断<code>ALL_TO_ALL</code>还是<code>POINTWISE</code>，可以看到，只有<code>ForwardPartitioner</code>和<code>RescalePartitioner</code>才是点对点分发。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator#isPointwisePartitioner</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isPointwisePartitioner</span><span class="params">(StreamPartitioner&lt;?&gt; partitioner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> partitioner <span class="keyword">instanceof</span> ForwardPartitioner</span><br><span class="line">            || partitioner <span class="keyword">instanceof</span> RescalePartitioner;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>关于Flink的分区器，其类图如下：<br><img src="/uploads/Xnip2021-08-05_14-01-52.jpg" alt></p>
<p>简单介绍下：</p>
<ul>
<li>ForwardPartitioner：上下游算子并行度一致时，一对一传输数据</li>
<li>ShufflePartitioner：随机选择一个下游算子实例进行发送</li>
<li>RebalancePartitioner：采用Round-Robin方式，循环依次发给下游</li>
<li>KeyGroupStreamPartitioner：根据用户的KeySelector，计算key的哈希值模下游并行度取余，计算下游节点</li>
<li>GlobalPartitioner：全部发往下游的某一个节点</li>
<li>BroadcastPartitioner：广播，一条数据下游所有节点都会收到</li>
<li>RescalePartitioner：基于上下游Operator的并行度，记录以循环的方式输出到下游Operator的每个实例</li>
</ul>
<p>参考文章：</p>
<ol>
<li><a href="http://matt33.com/2019/12/20/flink-execution-graph-4/" target="_blank" rel="noopener">http://matt33.com/2019/12/20/flink-execution-graph-4/</a></li>
<li><a href="https://developer.aliyun.com/article/225618" target="_blank" rel="noopener">https://developer.aliyun.com/article/225618</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Flink执行流程(二)：生成JobGraph</title>
    <url>/2021/08/02/Flink%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B-%E4%BA%8C/</url>
    <content><![CDATA[<p>上一篇文章介绍了用户代码是如何被转化为<code>StreamGraph</code>，本文介绍<code>StreamGraph</code>如何被转换为<code>JobGraph</code>，继续使用上节的代码示例：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Integer&gt; source = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">    DataStream&lt;Integer&gt; transform = source.map(x -&gt; x * x);</span><br><span class="line">    KeyedStream&lt;Integer, Integer&gt; keyedStream = transform.keyBy(<span class="keyword">new</span> KeySelector&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    keyedStream.print();</span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>以上程序的StreamGraph为：<br><img src="/uploads/Xnip2021-08-03_19-19-47.jpg" alt></p>
<p>首先对<code>JobGraph</code>做一个简单的介绍：</p>
<ol>
<li>一个<code>JobGraph</code>表示一个flink流处理程序，它是由client端产生，是Flink JobManager能接收的对象，任何上层API编写的flink程序（Table/SQL/Stream/Batch）都会被转换为<code>JobGraph</code>提交给JobManager。</li>
<li><code>JobGraph</code>使用<code>Map&lt;JobVertexID, JobVertex&gt; taskVertices</code>表示，其中<code>JobVertex</code>表示<code>JobGraph</code>的节点，在<code>JobVertex</code>对象中包含<code>ArrayList&lt;JobEdge&gt; inputs</code>表示该节点的入边。</li>
<li><code>JobGraph</code>相比于<code>StreamGraph</code>，会将可以chain在一起的节点组成一个节点。</li>
<li><code>JobGraph</code>中出现一个新概念，<code>IntermediateDataSet</code>，表示一个operator产生的中间结果，其作为桥梁连接<code>JobVertex</code>和<code>JobEdge</code></li>
</ol>
<p>下面介绍一下<code>JobVertex</code>，<code>JobEdge</code>，<code>IntermediateDataSet</code>三者的关系:<br><img src="/uploads/Xnip2021-08-02_17-18-58.jpg" alt></p>
<p>上图表示：</p>
<ol>
<li>（Source）<code>JobVertex</code>包含一个或多个<code>IntermediateDataSet</code>，表示该<code>JobVertex</code>可能产生多个结果集</li>
<li><code>IntermediateDataSet</code>中包含一个<code>JobVertex producer</code>指向产生该<code>IntermediateDataSet</code>的vertex</li>
<li><code>IntermediateDataSet</code>中包含一个或多个<code>JobEdge</code>，指向该<code>IntermediateDataSet</code>的消费方</li>
<li><code>JobEdge</code>中包含一个<code>IntermediateDataSet</code>表示该边的source数据</li>
<li><code>JobEdge</code>中包含一个<code>JobVertex target</code>表示该边指向的（Dest）<code>JobVertex</code></li>
</ol>
<h1 id="如何生成JobGraph"><a href="#如何生成JobGraph" class="headerlink" title="如何生成JobGraph"></a>如何生成JobGraph</h1><p>生成JobGraph的入口是org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator#createJobGraph()<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> JobGraph <span class="title">createJobGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    preValidate(); <span class="comment">// 校验是否是iterative任务，如果是，不能开启checkpoint，否则直接报错退出</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// make sure that all vertices start immediately</span></span><br><span class="line">    jobGraph.setScheduleMode(streamGraph.getScheduleMode());</span><br><span class="line">    jobGraph.enableApproximateLocalRecovery(</span><br><span class="line">            streamGraph.getCheckpointConfig().isApproximateLocalRecoveryEnabled());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为每个算子生成唯一的operator id</span></span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes =</span><br><span class="line">            defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 兼容历史版本</span></span><br><span class="line">    List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes = <span class="keyword">new</span> ArrayList&lt;&gt;(legacyStreamGraphHashers.size());</span><br><span class="line">    <span class="keyword">for</span> (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123;</span><br><span class="line">        legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 核心部分：递归将operator chain在一起，构造JobGraph（生成JobVertex/JobEdge）</span></span><br><span class="line">    setChaining(hashes, legacyHashes);</span><br><span class="line">    <span class="comment">// 设置JobGraph中JobVertex的入边</span></span><br><span class="line">    setPhysicalEdges();</span><br><span class="line">    <span class="comment">// 设置SlotSharingGroup</span></span><br><span class="line">    setSlotSharingAndCoLocation();</span><br><span class="line"></span><br><span class="line">    setManagedMemoryFraction(</span><br><span class="line">            Collections.unmodifiableMap(jobVertices),</span><br><span class="line">            Collections.unmodifiableMap(vertexConfigs),</span><br><span class="line">            Collections.unmodifiableMap(chainedConfigs),</span><br><span class="line">            id -&gt; streamGraph.getStreamNode(id).getManagedMemoryOperatorScopeUseCaseWeights(),</span><br><span class="line">            id -&gt; streamGraph.getStreamNode(id).getManagedMemorySlotScopeUseCases());</span><br><span class="line"></span><br><span class="line">    configureCheckpointing();</span><br><span class="line"></span><br><span class="line">    jobGraph.setSavepointRestoreSettings(streamGraph.getSavepointRestoreSettings());</span><br><span class="line"></span><br><span class="line">    JobGraphUtils.addUserArtifactEntries(streamGraph.getUserArtifacts(), jobGraph);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set the ExecutionConfig last when it has been finalized</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        jobGraph.setExecutionConfig(streamGraph.getExecutionConfig());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalConfigurationException(</span><br><span class="line">                <span class="string">"Could not serialize the ExecutionConfig."</span></span><br><span class="line">                        + <span class="string">"This indicates that non-serializable types (like custom serializers) were registered"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>以上代码中最核心的部分是<code>setChaining(hashes, legacyHashes);</code>，这里先介绍下<code>defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph)</code>方法，该方法为每个operator生成唯一的id。每个算子拥有唯一id的作用是flink作业在从state恢复时，根据该id来匹配状态和算子。看一下id是如何生成的：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">generateNodeHash</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamNode node,</span></span></span><br><span class="line"><span class="function"><span class="params">        HashFunction hashFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> isChainingEnabled,</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamGraph streamGraph)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取用户是否显示指定了uid</span></span><br><span class="line">    String userSpecifiedHash = node.getTransformationUID();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果用户没有指定uid，根据一定规则生成</span></span><br><span class="line">    <span class="keyword">if</span> (userSpecifiedHash == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Check that all input nodes have their hashes computed</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge inEdge : node.getInEdges()) &#123;</span><br><span class="line">            <span class="comment">// If the input node has not been visited yet, the current</span></span><br><span class="line">            <span class="comment">// node will be visited again at a later point when all input</span></span><br><span class="line">            <span class="comment">// nodes have been visited and their hashes set.</span></span><br><span class="line">            <span class="keyword">if</span> (!hashes.containsKey(inEdge.getSourceId())) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Hasher hasher = hashFunction.newHasher();</span><br><span class="line">        <span class="keyword">byte</span>[] hash =</span><br><span class="line">                generateDeterministicHash(node, hasher, hashes, isChainingEnabled, streamGraph);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (hashes.put(node.getId(), hash) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Sanity check</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line">                    <span class="string">"Unexpected state. Tried to add node hash "</span></span><br><span class="line">                            + <span class="string">"twice. This is probably a bug in the JobGraph generator."</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果用户指定了uid，generateUserSpecifiedHash(node, hasher)方法根据uid生成byte[]哈希值</span></span><br><span class="line">        Hasher hasher = hashFunction.newHasher();</span><br><span class="line">        <span class="keyword">byte</span>[] hash = generateUserSpecifiedHash(node, hasher);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">byte</span>[] previousHash : hashes.values()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (Arrays.equals(previousHash, hash)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                        <span class="string">"Hash collision on user-specified ID "</span></span><br><span class="line">                                + <span class="string">"\""</span></span><br><span class="line">                                + userSpecifiedHash</span><br><span class="line">                                + <span class="string">"\". "</span></span><br><span class="line">                                + <span class="string">"Most likely cause is a non-unique ID. Please check that all IDs "</span></span><br><span class="line">                                + <span class="string">"specified via `uid(String)` are unique."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (hashes.put(node.getId(), hash) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Sanity check</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line">                    <span class="string">"Unexpected state. Tried to add node hash "</span></span><br><span class="line">                            + <span class="string">"twice. This is probably a bug in the JobGraph generator."</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果用户为显示指定uid，默认的operator id由以下规则生成：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">byte</span>[] generateDeterministicHash(</span><br><span class="line">        StreamNode node,</span><br><span class="line">        Hasher hasher,</span><br><span class="line">        Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes,</span><br><span class="line">        <span class="keyword">boolean</span> isChainingEnabled,</span><br><span class="line">        StreamGraph streamGraph) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前节点在图中的位置</span></span><br><span class="line">    generateNodeLocalHash(hasher, hashes.size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出节点的个数</span></span><br><span class="line">    <span class="keyword">for</span> (StreamEdge outEdge : node.getOutEdges()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (isChainable(outEdge, isChainingEnabled, streamGraph)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Use the hash size again, because the nodes are chained to</span></span><br><span class="line">            <span class="comment">// this node. This does not add a hash for the chained nodes.</span></span><br><span class="line">            generateNodeLocalHash(hasher, hashes.size());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">byte</span>[] hash = hasher.hash().asBytes();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输入节点的id</span></span><br><span class="line">    <span class="keyword">for</span> (StreamEdge inEdge : node.getInEdges()) &#123;</span><br><span class="line">        <span class="keyword">byte</span>[] otherHash = hashes.get(inEdge.getSourceId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Sanity check</span></span><br><span class="line">        <span class="keyword">if</span> (otherHash == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line">                    <span class="string">"Missing hash for input node "</span></span><br><span class="line">                            + streamGraph.getSourceVertex(inEdge)</span><br><span class="line">                            + <span class="string">". Cannot generate hash for "</span></span><br><span class="line">                            + node</span><br><span class="line">                            + <span class="string">"."</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; hash.length; j++) &#123;</span><br><span class="line">            hash[j] = (<span class="keyword">byte</span>) (hash[j] * <span class="number">37</span> ^ otherHash[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> hash;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接下来看<code>setChaining</code>方法的实现：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setChaining</span><span class="params">(Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes, List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 找到chain的入口（source operator）</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;Integer, OperatorChainInfo&gt; chainEntryPoints =</span><br><span class="line">            buildChainedInputsAndGetHeadInputs(hashes, legacyHashes);</span><br><span class="line">    <span class="keyword">final</span> Collection&lt;OperatorChainInfo&gt; initialEntryPoints =</span><br><span class="line">            chainEntryPoints.entrySet().stream()</span><br><span class="line">                    .sorted(Comparator.comparing(Entry::getKey))</span><br><span class="line">                    .map(Entry::getValue)</span><br><span class="line">                    .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对每个入口调用createChain方法</span></span><br><span class="line">    <span class="keyword">for</span> (OperatorChainInfo info : initialEntryPoints) &#123;</span><br><span class="line">        createChain(</span><br><span class="line">                info.getStartNodeId(),</span><br><span class="line">                <span class="number">1</span>, <span class="comment">// operators start at position 1 because 0 is for chained source inputs</span></span><br><span class="line">                info,</span><br><span class="line">                chainEntryPoints);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>createChain</code>使用贪心算法，从首节点开始尽可能的尝试合并其子节点，当遇到不能合并的节点时，以下一个节点为首节点，继续该过程，直到遍历完所有节点。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;StreamEdge&gt; <span class="title">createChain</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Integer currentNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">int</span> chainIndex,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> OperatorChainInfo chainInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Map&lt;Integer, OperatorChainInfo&gt; chainEntryPoints)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    Integer startNodeId = chainInfo.getStartNodeId();</span><br><span class="line">    <span class="comment">// 判断该节点没有被处理过</span></span><br><span class="line">    <span class="keyword">if</span> (!builtVertices.contains(startNodeId)) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 记录用来建立JobGraph的边（即所有不能合并的边）</span></span><br><span class="line">        List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">        <span class="comment">// 记录当前节点可以合并的边</span></span><br><span class="line">        List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">        <span class="comment">// 记录当前节点不能合并的边</span></span><br><span class="line">        List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">        StreamNode currentNode = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历当前节点的出边，使用isChainable方法判断是否可以chain，分别将可chain与不可chain的边放到对应的list中</span></span><br><span class="line">        <span class="comment">// isChainable方法的实现后面会介绍</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge outEdge : currentNode.getOutEdges()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isChainable(outEdge, streamGraph)) &#123;</span><br><span class="line">                chainableOutputs.add(outEdge);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                nonChainableOutputs.add(outEdge);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历所有可chain的边，递归调用createChain，尝试将子节点的子节点chain在当前链条上</span></span><br><span class="line">        <span class="comment">// chainInfo用于保存当前链条chain了哪些节点，这里仍使用原来的chainInfo表名处在同一条chain上</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.addAll(</span><br><span class="line">                    createChain(</span><br><span class="line">                            chainable.getTargetId(),</span><br><span class="line">                            chainIndex + <span class="number">1</span>,</span><br><span class="line">                            chainInfo,</span><br><span class="line">                            chainEntryPoints));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 遍历所有可不chain的边</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">            <span class="comment">// 将不可chain的边加入到transitiveOutEdges，用于构建JobGraph的边</span></span><br><span class="line">            transitiveOutEdges.add(nonChainable);</span><br><span class="line">            <span class="comment">// 递归调用createChain方法，这里使用不可chain的边所连接的节点作为起始点，新建chainInfo用于保存新chain</span></span><br><span class="line">            createChain(</span><br><span class="line">                    nonChainable.getTargetId(),</span><br><span class="line">                    <span class="number">1</span>, <span class="comment">// operators start at position 1 because 0 is for chained source inputs</span></span><br><span class="line">                    chainEntryPoints.computeIfAbsent(</span><br><span class="line">                            nonChainable.getTargetId(),</span><br><span class="line">                            (k) -&gt; chainInfo.newChain(nonChainable.getTargetId())),</span><br><span class="line">                    chainEntryPoints);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 走到这里表示递归完成，已经递归到叶子节点，一条chain形成</span></span><br><span class="line">        chainedNames.put(</span><br><span class="line">                currentNodeId,</span><br><span class="line">                createChainedName(</span><br><span class="line">                        currentNodeId,</span><br><span class="line">                        chainableOutputs,</span><br><span class="line">                        Optional.ofNullable(chainEntryPoints.get(currentNodeId))));</span><br><span class="line">        chainedMinResources.put(</span><br><span class="line">                currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs));</span><br><span class="line">        chainedPreferredResources.put(</span><br><span class="line">                currentNodeId,</span><br><span class="line">                createChainedPreferredResources(currentNodeId, chainableOutputs));</span><br><span class="line">        <span class="comment">// 将当前节点加入到当前chain中</span></span><br><span class="line">        OperatorID currentOperatorId =</span><br><span class="line">                chainInfo.addNodeToChain(currentNodeId, chainedNames.get(currentNodeId));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置chain的输入输出类型</span></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getInputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId)</span><br><span class="line">                    .addInputFormat(currentOperatorId, currentNode.getInputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getOutputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId)</span><br><span class="line">                    .addOutputFormat(currentOperatorId, currentNode.getOutputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果当前节点是chain的起始节点，创建一个JobVertex，并返回StreamConfig</span></span><br><span class="line">        <span class="comment">// 如果不是起始节点，先创建一个空的StreamConfig</span></span><br><span class="line">        <span class="comment">// StreamConfig保存每个算子（StreamOperator）的配置</span></span><br><span class="line">        StreamConfig config =</span><br><span class="line">                currentNodeId.equals(startNodeId)</span><br><span class="line">                        ? createJobVertex(startNodeId, chainInfo)</span><br><span class="line">                        : <span class="keyword">new</span> StreamConfig(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置StreamConfig，例如具体执行的operator</span></span><br><span class="line">        setVertexConfig(</span><br><span class="line">                currentNodeId,</span><br><span class="line">                config,</span><br><span class="line">                chainableOutputs,</span><br><span class="line">                nonChainableOutputs,</span><br><span class="line">                chainInfo.getChainedSources());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">            <span class="comment">// 如果为chain的首节点，设置相应标记</span></span><br><span class="line">            config.setChainStart();</span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 遍历transitiveOutEdges，构建JobEdge</span></span><br><span class="line">            <span class="keyword">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</span><br><span class="line">                connect(startNodeId, edge);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            config.setOutEdgesInOrder(transitiveOutEdges);</span><br><span class="line">            config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId));</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            chainedConfigs.computeIfAbsent(</span><br><span class="line">                    startNodeId, k -&gt; <span class="keyword">new</span> HashMap&lt;Integer, StreamConfig&gt;());</span><br><span class="line"></span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            StreamNode node = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line">            config.setOperatorName(node.getOperatorName());</span><br><span class="line">            chainedConfigs.get(startNodeId).put(currentNodeId, config);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        config.setOperatorID(currentOperatorId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (chainableOutputs.isEmpty()) &#123;</span><br><span class="line">            config.setChainEnd();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> transitiveOutEdges;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的<code>connect()</code>方法会将前文提到的<code>JobVertex</code>，<code>IntermediateDataSet</code>，<code>JobEdge</code>三个对象产生联系：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JobEdge <span class="title">connectNewDataSetAsInput</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        JobVertex input, DistributionPattern distPattern, ResultPartitionType partitionType)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    IntermediateDataSet dataSet = input.createAndAddResultDataSet(partitionType);</span><br><span class="line"></span><br><span class="line">    JobEdge edge = <span class="keyword">new</span> JobEdge(dataSet, <span class="keyword">this</span>, distPattern);</span><br><span class="line">    <span class="keyword">this</span>.inputs.add(edge);</span><br><span class="line">    dataSet.addConsumer(edge);</span><br><span class="line">    <span class="keyword">return</span> edge;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>isChainable</code>方法是如何判断是否可chain的：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isChainable</span><span class="params">(StreamEdge edge, StreamGraph streamGraph)</span> </span>&#123;</span><br><span class="line">    StreamNode downStreamVertex = streamGraph.getTargetVertex(edge);</span><br><span class="line">    <span class="comment">// 首先下游入度必须是1</span></span><br><span class="line">    <span class="keyword">return</span> downStreamVertex.getInEdges().size() == <span class="number">1</span> &amp;&amp; isChainableInput(edge, streamGraph);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isChainableInput</span><span class="params">(StreamEdge edge, StreamGraph streamGraph)</span> </span>&#123;</span><br><span class="line">    StreamNode upStreamVertex = streamGraph.getSourceVertex(edge);</span><br><span class="line">    StreamNode downStreamVertex = streamGraph.getTargetVertex(edge);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里会判断operator是否配置了允许chain，partitioner必须是forward，并行度必须相等等条件</span></span><br><span class="line">    <span class="keyword">if</span> (!(upStreamVertex.isSameSlotSharingGroup(downStreamVertex)</span><br><span class="line">            &amp;&amp; areOperatorsChainable(upStreamVertex, downStreamVertex, streamGraph)</span><br><span class="line">            &amp;&amp; (edge.getPartitioner() <span class="keyword">instanceof</span> ForwardPartitioner)</span><br><span class="line">            &amp;&amp; edge.getShuffleMode() != ShuffleMode.BATCH</span><br><span class="line">            &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism()</span><br><span class="line">            &amp;&amp; streamGraph.isChainingEnabled())) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check that we do not have a union operation, because unions currently only work</span></span><br><span class="line">    <span class="comment">// through the network/byte-channel stack.</span></span><br><span class="line">    <span class="comment">// we check that by testing that each "type" (which means input position) is used only once</span></span><br><span class="line">    <span class="keyword">for</span> (StreamEdge inEdge : downStreamVertex.getInEdges()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (inEdge != edge &amp;&amp; inEdge.getTypeNumber() == edge.getTypeNumber()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>以上就是<code>setChaining</code>方法，该方法执行完后，JobGraph基本就构建完成了，文章开头代码样例构建出的jobGraph对象如下，可以看到source和map被chain在了一起：<br><img src="/uploads/Xnip2021-08-04_11-40-25.jpg" alt></p>
<p>FlinkUI上显示的拓扑图就是作业的JobGraph。</p>
<p>参考文章：</p>
<ol>
<li><a href="https://izualzhy.cn/flink-source-job-graph" target="_blank" rel="noopener">https://izualzhy.cn/flink-source-job-graph</a></li>
<li><a href="https://matt33.com/2019/12/09/flink-job-graph-3/" target="_blank" rel="noopener">https://matt33.com/2019/12/09/flink-job-graph-3/</a></li>
<li><a href="https://wuchong.me/blog/2016/05/10/flink-internals-how-to-build-jobgraph/" target="_blank" rel="noopener">https://wuchong.me/blog/2016/05/10/flink-internals-how-to-build-jobgraph/</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Profile</title>
    <url>/2018/04/24/Profile/</url>
    <content><![CDATA[<h1 id="戴恒宇"><a href="#戴恒宇" class="headerlink" title="戴恒宇"></a>戴恒宇</h1><p>大数据开发工程师，数据仓库工程师</p>
<h2 id="联系方式"><a href="#联系方式" class="headerlink" title="联系方式"></a>联系方式</h2><p>| Tel: 18512547673 |  Email: <a href="mailto:dhytheone@163.com" target="_blank" rel="noopener">dhytheone@163.com</a>|  <a href="http://dhy.party" target="_blank" rel="noopener">My WebSite</a> : <a href="http://dhy.party" target="_blank" rel="noopener">http://dhy.party</a></p>
<h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><ul>
<li>编程语言<ul>
<li>Java 熟练，掌握常用JVM基础知识</li>
<li>Shell/Python 日常运维脚本</li>
</ul>
</li>
<li>后端开发<ul>
<li>熟悉Spring/Spring Boot/MyBatis</li>
<li>熟悉并发编程</li>
</ul>
</li>
<li>离线数据开发<ul>
<li>熟悉离线数据仓库建模</li>
<li>Hive熟练，向社区反馈过多个bug，提交过patch被采纳</li>
<li>熟悉hadoop mapreduce/hdfs</li>
</ul>
</li>
<li>实时数据开发<ul>
<li>熟悉Flink、kafka</li>
<li>建设实时数仓经验及实时数据应用案例</li>
</ul>
</li>
<li>其他<ul>
<li>有hbase/es/druid使用经验</li>
<li>理解微服务架构。有使用dubbo经验</li>
<li>熟练阅读英文文档，奉行clean code，善于解决问题</li>
</ul>
</li>
</ul>
<h2 id="项目经验"><a href="#项目经验" class="headerlink" title="项目经验"></a>项目经验</h2><ol>
<li>离线数据开发、Hive二次开发与维护<ul>
<li>1.1. 离线数仓建设<ul>
<li>负责BU“用户服务”主题域离线数仓开发，完成自底向上从ods层、dw/mdw层、rpt层报表开发</li>
<li>数据工具开发：数据同步工具（DB、数仓、Redis之间）、</li>
</ul>
</li>
<li>1.2. Hive二次开发与维护<ul>
<li>负责部门Hive集群的部署、运维</li>
<li>二次开发Hive支持多metastore功能，修复日常hive bug，部分反馈社区，<a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hengyu.dai" target="_blank" rel="noopener">Apache Profile: hengyu.dai</a></li>
</ul>
</li>
<li>1.3. Hive服务系统<ul>
<li>接收Hive Hook传递的SQL，进行SQL解析、鉴权、查询记录归档</li>
<li>定时检测HiveServer/Metastore server的服务健康校验、发现失败服务自动重启</li>
</ul>
</li>
</ul>
</li>
<li>数据系统开发<ul>
<li>2.1. 元数据系统：全方位管理数据仓库<ul>
<li>离线数据仓库元数据系统开发，管理离线数仓中的表资源，主要功能包括：表的多维度查询，血缘关系，表的各方面监控。实现的关键词：SQL解析、Hive Hook。</li>
</ul>
</li>
<li>2.2. 即席查询：数据自助查询平台<ul>
<li>基于Hive Client开发，相比于底层使用hiveserver的开源HUE等产品，大幅提高稳定性。</li>
<li>开放给产品和运营同学使用，包含表的权限校验、常用数据口径管理</li>
<li>支持Hive/Presto/Hive on spark多种查询引擎，支持多执行节点</li>
</ul>
</li>
</ul>
</li>
<li>实时数仓开发<ul>
<li>3.1. 从零搭建基于Flink的实时数仓体系<ul>
<li>负责订单与资金模块的数据源接入，实时数据清洗，实时数据落地与应用（HDFS/HBase）</li>
<li>清洗用户实时流量数据（用户在不同页面的访问日志）作为算法模型中用户特征数据</li>
</ul>
</li>
<li>3.2. 实时数仓周边服务<ul>
<li>Redis实时维表：设计维表关联方案，开发Hive与Redis导数工具，开发通用关联维表的工具jar包。</li>
</ul>
</li>
</ul>
</li>
<li>数据应用<ul>
<li>4.1. 用户画像系统：提供统一的用户画像、身份服务<ul>
<li>数据赋能业务，通过离线数仓+实时计算清洗出的用户特征信息，定义用户画像，提供在线接口输出用户画像，用于各种基于用户身份的营销策略，例如发券、促销、新客优惠等。</li>
</ul>
</li>
<li>4.2. 故障自动定位系统：利用数据辅助业务线发现故障<ul>
<li>异常监控探测：利用业务系统监控看板，及时发现异常监控指标。</li>
<li>异常app发现：利用系统的异常trace数据定位app异常。</li>
<li>故障根源发现：综合多个故障appcode，通过绘制系统间调用拓扑图，从前向后追溯故障根源，给出故障报告。</li>
</ul>
</li>
<li>4.3. 通用数据接口：离线数仓与业务系统之间的数据桥梁<ul>
<li>将数据仓库中离线计算的数据通过HTTP接口传输给业务系统使用。</li>
<li>配置化、新增数据时无需开发代码。</li>
</ul>
</li>
<li>4.4. 实时大屏：直播带货效果的实时监控<ul>
<li>重要活动时，实时计算活动期间的各项运营指标，在BI系统上实时更新，实时数据大屏。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h2><ul>
<li>2016-至今 去哪儿网 目的地事业部 数据开发</li>
</ul>
<h2 id="教育经历"><a href="#教育经历" class="headerlink" title="教育经历"></a>教育经历</h2><ul>
<li>2009-2013 中山大学 软件学院 学士学位</li>
<li>2013-2016 南京大学 计算机科学与技术 硕士学位</li>
</ul>
]]></content>
      <tags>
        <tag>我, 其他</tag>
      </tags>
  </entry>
  <entry>
    <title>序列化与反序列化一[简介]</title>
    <url>/2018/04/26/%E7%A8%8B%E5%BA%8F%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    <content><![CDATA[<h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>什么是序列化？</p>
<blockquote>
<p>序列化是将程序中的对象或是数据结构转换成持久化的存储（例如文件或缓存）或是可以通过网络传输的字节流，并且转换的结果可以在后续需要的时候，依据预先定义的序列化格式，能够重新构造出原始的对象，这个过程在语义上等同于对于原始对象的clone。</p>
</blockquote>
<p>序列化就是把内存中的对象保存起来，反序列化就是把保存的东西给恢复成内存中的对象，本质上跟数据库的作用类似，只不过跟数据库相比，一方面序列化技术能存储更为复杂的数据类型，而数据库只能存储基本数据类型和数据结构，另一方面序列化对程序语言的依赖性更强，需要程序语言定义好序列化的东西是什么（对象的结构），在反序列的时候要知道这个元信息，才能将网络或是硬盘中的字节流转换为相应的对象。</p>
<p>需要注意的是，在面向对象编程语言中，序列化对象的结果中不包含这个对象所拥有的方法，只能描述对象是什么（field），不能描述对象能干什么（method）。</p>
<h3 id="常见的序列化技术"><a href="#常见的序列化技术" class="headerlink" title="常见的序列化技术"></a>常见的序列化技术</h3><p>以一个java的Cat类为例，介绍常见序列化技术的基本用法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>一些序列化的技术得到大部分编程语言的支持，我称之为编程语言无关的序列化技术，常见的有：</p>
<h4 id="XML"><a href="#XML" class="headerlink" title="XML"></a>XML</h4><h4 id="Json"><a href="#Json" class="headerlink" title="Json"></a>Json</h4><h4 id="Bson"><a href="#Bson" class="headerlink" title="Bson"></a>Bson</h4><h4 id="MessagePack"><a href="#MessagePack" class="headerlink" title="MessagePack"></a>MessagePack</h4><h4 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h4><p>其他一些序列化的技术只用在特定的编程语言中，以java为例，java中的序列化技术有：</p>
<h4 id="Java原生序列化"><a href="#Java原生序列化" class="headerlink" title="Java原生序列化"></a>Java原生序列化</h4><p>jdk原生的序列化方案要求被序列化的类实现<code>Serializable</code>接口，该接口是标记接口，并无任何方法，仅用于提示JVM，先挖个坑，以后再学习java原生序列化的原理，先介绍如何使用。</p>
<p>实现了<code>Serializable</code>接口的<code>Cat</code>类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * serialVersionUID 用于反序列化时验证版本是否一致</span></span><br><span class="line"><span class="comment">     * 如果没有显示指定，jvm执行序列化时根据当前类hash值自动生成一个uid</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span> ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Cat&#123;"</span> + <span class="string">"name='"</span> + name + <span class="string">'\''</span> + <span class="string">", age="</span> + age + <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>序列化的核心类是<code>ObjectOutputStream</code>，序列化代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JdkSerialization</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Cat tom = <span class="keyword">new</span> Cat(<span class="string">"tom"</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="string">"cat.out"</span>);</span><br><span class="line">        ObjectOutputStream oos = <span class="keyword">new</span> ObjectOutputStream(fos);</span><br><span class="line">        oos.writeObject(tom);</span><br><span class="line">        oos.flush();</span><br><span class="line">        oos.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看cat.out: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aced 0005 7372 0011 7365 7269 616c 697a</span><br><span class="line">6174 696f 6e2e 4361 7400 0000 0000 0000</span><br><span class="line">0102 0002 4900 0361 6765 4c00 046e 616d</span><br><span class="line">6574 0012 4c6a 6176 612f 6c61 6e67 2f53</span><br><span class="line">7472 696e 673b 7870 0000 0003 7400 0374</span><br><span class="line">6f6d</span><br></pre></td></tr></table></figure>
<p>反序列化cat对象: </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectInputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JdkDeSerialization</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="string">"cat.out"</span>);</span><br><span class="line">        ObjectInputStream ois = <span class="keyword">new</span> ObjectInputStream(fis);</span><br><span class="line">        Cat tom = (Cat) ois.readObject();</span><br><span class="line">        System.out.println(tom);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果为正确的cat对象: <code>Cat{name=&#39;tom&#39;, age=3}</code></p>
<h4 id="Kryo"><a href="#Kryo" class="headerlink" title="Kryo"></a>Kryo</h4><p>maven依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kryo&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.0.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>使用:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.Kryo;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.io.Input;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.io.Output;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KryoTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Kryo kryo;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KryoTest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.kryo = <span class="keyword">new</span> Kryo();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Cat cat)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Output output = <span class="keyword">new</span> Output(<span class="keyword">new</span> FileOutputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        kryo.writeObject(output, cat);</span><br><span class="line">        output.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Input input = <span class="keyword">new</span> Input(<span class="keyword">new</span> FileInputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        Cat cat = kryo.readObject(input, Cat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        input.close();</span><br><span class="line">        System.out.println(cat);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Cat tom = <span class="keyword">new</span> Cat(<span class="string">"tom"</span>, <span class="number">3</span>);</span><br><span class="line">        KryoTest kryoTest = <span class="keyword">new</span> KryoTest();</span><br><span class="line">        kryoTest.serialize(tom);</span><br><span class="line">        kryoTest.deserialize();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Hessian2"><a href="#Hessian2" class="headerlink" title="Hessian2"></a>Hessian2</h4><p>maven依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.caucho&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hessian&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.0.7&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.caucho.hessian.io.Hessian2Input;</span><br><span class="line"><span class="keyword">import</span> com.caucho.hessian.io.Hessian2Output;</span><br><span class="line"><span class="keyword">import</span> com.caucho.hessian.io.SerializerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hessian2Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SerializerFactory factory = <span class="keyword">new</span> SerializerFactory();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Cat cat)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Hessian2Output output = <span class="keyword">new</span> Hessian2Output(<span class="keyword">new</span> FileOutputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        output.setSerializerFactory(factory);</span><br><span class="line"></span><br><span class="line">        output.writeObject(cat);</span><br><span class="line">        output.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Hessian2Input input = <span class="keyword">new</span> Hessian2Input(<span class="keyword">new</span> FileInputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        input.setSerializerFactory(factory);</span><br><span class="line">        Cat cat = (Cat) input.readObject();</span><br><span class="line">        System.out.println(cat);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Cat tom = <span class="keyword">new</span> Cat(<span class="string">"tom"</span>, <span class="number">3</span>);</span><br><span class="line">        Hessian2Test test = <span class="keyword">new</span> Hessian2Test();</span><br><span class="line">        test.serialize(tom);</span><br><span class="line">        test.deserialize();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Protobuf"><a href="#Protobuf" class="headerlink" title="Protobuf"></a>Protobuf</h4><h4 id="Thrift"><a href="#Thrift" class="headerlink" title="Thrift"></a>Thrift</h4><h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>序列化</tag>
      </tags>
  </entry>
</search>
