<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>About Blank</title>
  
  <subtitle>随遇而安</subtitle>
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-07-29T09:38:05.538Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Hengyu Dai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Calcite入门介绍</title>
    <link href="http://yoursite.com/2021/07/29/Calcite%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2021/07/29/Calcite%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-07-29T09:38:05.000Z</published>
    <updated>2021-07-29T09:38:05.538Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>20190731 Kryo反序列化失败</title>
    <link href="http://yoursite.com/2019/07/31/20190731-Kryo%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%A4%B1%E8%B4%A5/"/>
    <id>http://yoursite.com/2019/07/31/20190731-Kryo%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%A4%B1%E8%B4%A5/</id>
    <published>2019-07-31T05:07:44.000Z</published>
    <updated>2020-04-11T05:19:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>SQl报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Error: java.lang.RuntimeException: Failed to load plan: viewfs://xxx/-mr-10005/49f2f715-02e7-4f66-802b-0b080a9c6129/map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 124</span><br><span class="line">Serialization trace:</span><br><span class="line">sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:456)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:303)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:315)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:607)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:582)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:676)</span><br><span class="line">    at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:169)</span><br><span class="line">    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)</span><br><span class="line">    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 124</span><br><span class="line">Serialization trace:</span><br><span class="line">sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readClass(SerializationUtilities.java:175)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)</span><br><span class="line">    at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:686)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readObject(SerializationUtilities.java:200)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializeObjectByKryo(SerializationUtilities.java:615)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializePlan(SerializationUtilities.java:524)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.SerializationUtilities.deserializePlan(SerializationUtilities.java:477)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:416)</span><br><span class="line">    ... 13 more</span><br></pre></td></tr></table></figure></p><h1 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h1><h2 id="问题产生的背景"><a href="#问题产生的背景" class="headerlink" title="问题产生的背景"></a>问题产生的背景</h2><p>首先描述下上述错误产生的上下文：</p><ol><li><p>Hive是一个将SQL编译成MR程序的sql编译器，并不负责执行MR任务</p></li><li><p>一个典型的MR任务，需要一个实现org.apache.hadoop.mapred.Mapper接口的mapper类，一个实现org.apache.hadoop.mapred.Reducer的reducer类，还要一个控制类设置任务相关参数，并提交给集群执行</p></li><li><p>Hive也不例外，Hive提交任务的控制器是org.apache.hadoop.hive.ql.exec.mr.ExecDriver，Mapper类是统一的，都是org.apache.hadoop.hive.ql.exec.mr.ExecMapper，reducer类也是统一的，org.apache.hadoop.hive.ql.exec.mr.ExecReducer</p></li></ol><p>下面都以mapper为例，reducer一样</p><ol start="4"><li><p>ExecMapper通过内部的operator来区分具体每个sql的每个mapper干什么事情，内部持有一个AbstractMapOperator属性，执行时是调用AbstractMapOperator#process来干活</p></li><li><p>Hive内部使用MapWork来描述一个mapper的工作，（4）中的AbstractMapOperator具体干什么事情，是由MapWork决定的，hadoop集群上的节点是通过Utilities.getMapWork(job)获取一个MapWork对象</p></li><li><p>终于进入正题了，Utilities.getMapWork(job)是读取HDFS的map.xml文件，将文件内容反序列化为java对象，而map.xml是由Hive客户端写入HDFS上的，也就是说，HDFS作为媒介，保存Hive端编译好的mapper/reducer内部的执行细节，然后在yarn集群上每个container从hdfs上读取xml文件，将其反序列化为对象，然后执行。</p></li><li><p>map.xml和reducer.xml虽然以xml结尾，但这是历史原因，早期的hive用xml保存序列化的结果，但是（记得是从2.0开始）目前hive只有一种序列化方式，就是kryo，xml其实是二进制文件。这个xml文件比较难捕捉，任务执行完成后不管是否正常结束都会自动删除，只能用debug的方式在写完文件后，下载下来。下面放一个上面SQL的map.xml文件：</p></li></ol><p>map.xml</p><ol start="8"><li>回头看下上面的错误就能看明白了，Hive客户端将map.xml文件写到HDFS上的一个路径，在hadoop端反序列化map.xml时，kryo反序列化失败了，导致hadoop端获取不到mapper任务</li></ol><h2 id="解决问题的尝试"><a href="#解决问题的尝试" class="headerlink" title="解决问题的尝试"></a>解决问题的尝试</h2><p>解决问题首先google，上述错误在google上信息很少，但也有几条。</p><ol><li><a href="https://github.com/EsotericSoftware/kryo/issues/307" target="_blank" rel="noopener">https://github.com/EsotericSoftware/kryo/issues/307</a></li></ol><p>这个issue跟我面临的问题基本一致，提到了kryo的仓库里，下面kyro的作者之一回复说可能是上一个序列化属性占了大量的空间导致取下一个kryo class id的时候取错了。但是在我的例子里，本身序列化的文件就不大，这条pass。</p><ol start="2"><li><a href="https://issues.apache.org/jira/browse/HIVE-11519" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-11519</a></li></ol><p>这是一个Hive的issue，里面提的问题也跟我遇到的一致，但是处于未解决状态，下面的回复引导了另一个issue：<a href="https://issues.apache.org/jira/browse/HIVE-12175" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-12175</a> 这个issue号称解决了Hive中大量的kryo序列化bug，但是fix version是2.0.0，我们用的hive版本是2.1.1，高于它，可以预见这个issue也没什么用</p><ol start="3"><li>翻遍了google，有说hive on spark问题的，有说hive并行化执行bug的，有说集群中存在老的hive相关类的class的，能通过改配置调整的都试过全部无效</li></ol><p>只能磕代码了。</p><p>第一个尝试是在本地复现反序列化的问题。将Hive编译的map.xml下载到本地，然后在Hive中新建一个测试类，尝试去反序列化该文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class KryoDebug &#123;</span><br><span class="line"> </span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        abnormal();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    public static void abnormal() throws Exception &#123;</span><br><span class="line">        Kryo kryo = SerializationUtilities.borrowKryo();</span><br><span class="line">        String path = KryoDebug.class.getClassLoader().getResource(&quot;map.xml&quot;).getPath();</span><br><span class="line">        File mapFile = new File(path);</span><br><span class="line">        FileInputStream inputStream = new FileInputStream(mapFile);</span><br><span class="line">        MapWork mapWork = SerializationUtilities.deserializePlan(kryo, inputStream, MapWork.class);</span><br><span class="line">        System.out.println(mapWork);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>很遗憾，上面的代码执行是没问题的，在本地无法复现反序列化的问题，为了进一步确定本地没问题，将出错的SQL使用本地模式执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SET mapreduce.framework.name=local;</span><br><span class="line">drop table if exists temp.xxx_debug2;</span><br><span class="line">create table temp.xxx_debug2 stored as orc as</span><br><span class="line">select</span><br><span class="line">to_json(price_promotion)</span><br><span class="line">from ods_xxx</span><br><span class="line">where dt =&apos;20190722&apos; and hour=20</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>使用本地模式执行上述SQL也是可以正常执行，因此排除了SQL bug、Hive bug、Hadoop bug等问题，怀疑是集群环境和本地环境不一致。</p><p>第二个尝试是去测试集群中各类的版本是否和Hive端一致.</p><p>首先是验证反序列的MapWork客户端和集群是否一致：在集群上跑输出来源和字段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if (planClass.equals(MapWork.class)) &#123;</span><br><span class="line">  String path = &quot;org.apache.hadoop.hive.ql.plan.MapWork&quot;;</span><br><span class="line">  ClassLoader classLoader = Utilities.getSessionSpecifiedClassLoader();</span><br><span class="line">  LOG.info(&quot;MapWork反序列化:&quot; + classLoader.getResource(path.replace(&apos;.&apos;, &apos;/&apos;) + &quot;.class&quot;).toString());</span><br><span class="line">  Class&lt;MapWork&gt; clazz = MapWork.class;</span><br><span class="line">  Field[] fields = clazz.getDeclaredFields();</span><br><span class="line">  StringBuilder sb = new StringBuilder();</span><br><span class="line">  for (Field field : fields) &#123;</span><br><span class="line">    sb.append(field.getName()).append(&quot;###&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  LOG.info(&quot;MapWork反序列化属性:&quot; + sb.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-07-31 17:42:25,266 INFO [main] org.apache.hadoop.hive.ql.exec.SerializationUtilities: MapWork反序列化:jar:file:/data5/hadoop2x/nm-local-dir/usercache/qhstats/appcache/application_1564438364333_147100/filecache/10/job.jar/job.jar!/org/apache/hadoop/hive/ql/plan/MapWork.class</span><br><span class="line">2019-07-31 17:42:25,266 INFO [main] org.apache.hadoop.hive.ql.exec.SerializationUtilities: MapWork反序列化属性:pathToAliases###pathToPartitionInfo###aliasToWork###aliasToPartnInfo###nameToSplitSample###bucketedColsByDirectory###sortedColsByDirectory###tmpHDFSPath###tmpPathForPartitionPruning###inputformat###indexIntermediateFile###numMapTasks###maxSplitSize###minSplitSize###minSplitSizePerNode###minSplitSizePerRack###samplingType###SAMPLING_ON_PREV_MR###SAMPLING_ON_START###leftInputJoin###baseSrc###mapAliases###mapperCannotSpanPartns###inputFormatSorted###useBucketizedHiveInputFormat###dummyTableScan###eventSourceTableDescMap###eventSourceColumnNameMap###eventSourceColumnTypeMap###eventSourcePartKeyExprMap###doSplitsGrouping###vectorizedRowBatch###includedBuckets###llapIoDesc###$assertionsDisabled###</span><br></pre></td></tr></table></figure></p><p>可以看到class来自hive提交上去的jar包，比较了属性和本地class属性比较，也完全一致。排除了集群classpath下MapWork类冲突的问题</p><p>其次验证kryo版本是否一致，验证的结果是集群上使用的kryo也是来自hive提上去的。排除kryo class类冲突问题。</p><p>第三个尝试是针对反序列化错误的字段：</p><p>sortedColsByDirectory (org.apache.hadoop.hive.ql.plan.MapWork)<br>在反序列化sortedColsByDirectory字段发生错误，在com.esotericsoftware.kryo.pool.KryoFactory#create中让kryo不去序列化该字段：</p><p><img src="/uploads/Jietu20200411-131223.jpg" alt></p><p>重新编译hive后执行还是不行，包另一个字段出错了，而且这种方式是不可取的，不能说不序列化一个字段就不序列化了，sortedColsByDirectory在一些SQL中有用。</p><p>第四个尝试是怀疑SQL中用的to_json UDF有问题，UDF包中有hive相关类，在这里的尝试是将该UDF中所有用到的class，依赖全部改为provided（除了hive和hadoop中不存在的class），结果居然还是报同样的错。</p><p>第五个尝试是怀疑jdk版本的问题，hiveclient的jdk是8u60，hadoop集群jdk是8u131，于是将客户端jdk版本升级到8u131，仍然报错。</p><p>上述尝试全部失败后，实在没思路了，有一周时间没继续研究这个问题。</p><h2 id="找到新的切入点"><a href="#找到新的切入点" class="headerlink" title="找到新的切入点"></a>找到新的切入点</h2><p>既然是kryo反序列化的问题，还是得从kryo入手。</p><p>kyro本身在序列化的时候，可以通过执行下面指令将序列化的详细过程输出到stdout中</p><blockquote><p>Log.TRACE();</p></blockquote><p>在Hive中加入上述代码，重新编译Hive并执行SQL，输出的结果大概是这样：</p><p><img src="/uploads/Jietu20200411-131351.jpg" alt></p><p>从错误日志：</p><p>Encountered unregistered class ID: 124<br>可以知道是124号class反序列化异常，看下124号序列话的是什么：</p><p><img src="/uploads/Jietu20200411-131438.jpg" alt></p><p>是org.apache.hadoop.hive.ql.plan.TableScanDesc，于是猜测是该类未在序列化时注册，于是在hive中注册kyro时，手动注册该类：</p><p>org.apache.hadoop.hive.ql.exec.SerializationUtilities</p><p><img src="/uploads/Jietu20200411-131517.jpg" alt></p><p>重新编译并执行，问题依然存在，此时已经处于崩溃边缘。</p><p>看来从序列化日志是看不出什么东西了，那就从反序列日志入手：</p><p>本地的反序列日志容易获得，只需在本地运行上面的KryoDebug时，加上Log.TRACE();但是在集群上跑的时候，加上Log.TRACE();并不能在集群日志上打出反序列化日志，这是为什么？</p><p>研究了一阵发现集群日志只会显示LOG输出的日志，kyro的序列化日志是直接输出到标准输出的，而stdout输出的东西不会显示在集群日志上，我又没办法登上集群机器上看标准输出的东西，最后想了个取巧的方案，将标准输出“移植”到LOG中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// 这个类用于将标准输出转移到LOG里</span><br><span class="line">public class CustomOutputStream extends OutputStream &#123;</span><br><span class="line">    Logger logger;</span><br><span class="line">    StringBuilder stringBuilder;</span><br><span class="line"> </span><br><span class="line">    public CustomOutputStream(Logger logger)</span><br><span class="line">    &#123;</span><br><span class="line">        this.logger = logger;</span><br><span class="line">        stringBuilder = new StringBuilder();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    @Override</span><br><span class="line">    public final void write(int i) throws IOException &#123;</span><br><span class="line">        char c = (char) i;</span><br><span class="line">        if(c == &apos;\r&apos; || c == &apos;\n&apos;) &#123;</span><br><span class="line">            if(stringBuilder.length() &gt; 0) &#123;</span><br><span class="line">                logger.info(stringBuilder.toString());</span><br><span class="line">                stringBuilder = new StringBuilder();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else</span><br><span class="line">            stringBuilder.append(c);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">// 全局设置一次重定向stdout</span><br><span class="line">System.setOut(new PrintStream(new CustomOutputStream(LOG)));</span><br></pre></td></tr></table></figure><p>这时候集群日志上终于可以显示Kryo日志了：</p><p><img src="/uploads/Jietu20200411-131613.jpg" alt></p><p>将集群上的kryo反序列化日志（失败的）和我本机的反序列化日志（成功的）进行对比，果然发现不同：</p><p>本地日志<br><img src="/uploads/Jietu20200411-131645.jpg" alt></p><p>集群日志<br><img src="/uploads/Jietu20200411-131716.jpg" alt><br>对比两个日志，发现从Field _parent: class org.codehaus.jackson.sym.BytesToNameCanonicalizer这一行为分割，前面的日志两者一样，后面的日志开始不一样。</p><p>于是怀疑是这个类的问题，这是jackson里的类，udf中用到的。但是之前已经尝试过将udf中的类改为provided，并没有用。突然想起来会不会hive中的jackson和hadoop中的jackson冲突，让hadoop团队查了下hadoop中的jackson版本是1.9.13，然后我查了下hive的jackson版本，是1.9.2.</p><p>最后一次尝试！</p><p>将hive中的jackson版本改为1.9.13，重新编译hive，执行sql。amazing happens！成功了。</p><p>所以最终定位到的问题是hive和hadoop中存在jackson类冲突，而udf用到了该类，导致在序列化时，用的hive端的jackson（1.9.2），而在反序列化时，用的hadoop端的jackson（1.9.13），至于为什么反序列时不用1.9.2的jackson，这谁tm知道呢，这就是同名类加载顺序不确定性恶心的地方了。</p><h1 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h1><p>这是我处理过的最为棘手的一个Hive问题。难点在于：</p><ul><li>定位问题难，kryo反序列报错，这个错可能导致的原因太多了，而且更为关键的是我本地无法复现反序列化错误。</li><li>集群上调试困难，代码是跑在hadoop集群上报错的，而我无法debug hadoop集群代码（毕竟你不能在公司生产集群上打个断点调试）<br>收获是学习了kryo相关知识。</li></ul><p>教训是class冲突真特么难分析，终于知道为什么公司强制每次push代码都检测工程依赖有没有重复的class了，万一因为class冲突导致错误是真的难发现。</p><p>而在我们的HIVE SQL运行环境里，似乎类冲突很难避免，因为hadoop/hive/udf jar都有各自的依赖包，在执行sql时，这些依赖搅成一团，难保不冲突。</p><p>该怎么做尽量减少这类问题？</p><p>UDF pom里绝对绝对不能随意添加依赖，在添加之前必须检测hive和hadoop中是否已经存在该依赖！如果都不存在，可以添加，如果有一个存在，尽量与已经存在的版本保持一致</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;SQl报错&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20190523 关于HiveCli启动慢原因的调查</title>
    <link href="http://yoursite.com/2019/05/23/20190523-%E5%85%B3%E4%BA%8EHiveCli%E5%90%AF%E5%8A%A8%E6%85%A2%E5%8E%9F%E5%9B%A0%E7%9A%84%E8%B0%83%E6%9F%A5/"/>
    <id>http://yoursite.com/2019/05/23/20190523-%E5%85%B3%E4%BA%8EHiveCli%E5%90%AF%E5%8A%A8%E6%85%A2%E5%8E%9F%E5%9B%A0%E7%9A%84%E8%B0%83%E6%9F%A5/</id>
    <published>2019-05-23T05:02:30.000Z</published>
    <updated>2020-04-11T05:19:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h1><p>HiveClient 有时候会启动非常慢，具体表现是进入hive时当终端输出</p><blockquote><p>Logging initialized using configuration in file:/home/q/hive/apache-hive-2.1.1-qhstats-bin/conf/hive-log4j2.properties Async: false</p></blockquote><p>会隔上几秒甚至几十秒甚至几分钟才出现</p><blockquote><p>hive (default)&gt;<br>然后才能正常使用Hive</p></blockquote><h1 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h1><h2 id="1-打开debug日志查看启动Hive时卡在哪个步骤了"><a href="#1-打开debug日志查看启动Hive时卡在哪个步骤了" class="headerlink" title="1. 打开debug日志查看启动Hive时卡在哪个步骤了"></a>1. 打开debug日志查看启动Hive时卡在哪个步骤了</h2><blockquote><p>sudo -uqhstats hive_211 –hiveconf hive.root.logger=debug,console</p></blockquote><p>观察启动后的日志</p><p><img src="/uploads/Jietu20200411-130501.jpg" alt></p><p>红框中两行日志相隔了一分多钟。</p><p>下方的日志很明显，是在创建HDFS目录。</p><p>所以元凶就是HDFS的问题，集群namenode响应慢。</p><h2 id="2-深入了解下Hive启动时需要创建哪些临时目录"><a href="#2-深入了解下Hive启动时需要创建哪些临时目录" class="headerlink" title="2. 深入了解下Hive启动时需要创建哪些临时目录"></a>2. 深入了解下Hive启动时需要创建哪些临时目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Create dirs &amp; session paths for this session:</span><br><span class="line"> * 1. HDFS scratch dir</span><br><span class="line"> * 2. Local scratch dir</span><br><span class="line"> * 3. Local downloaded resource dir</span><br><span class="line"> * 4. HDFS session path</span><br><span class="line"> * 5. hold a lock file in HDFS session dir to indicate the it is in use</span><br><span class="line"> * 6. Local session path</span><br><span class="line"> * 7. HDFS temp table space</span><br><span class="line"> * @param userName</span><br><span class="line"> * @throws IOException</span><br><span class="line"> */</span><br><span class="line">private void createSessionDirs(String userName) throws IOException &#123;</span><br></pre></td></tr></table></figure><ul><li>HDFS 临时写目录（存放执行过程中hdfs临时文件）</li><li>本地临时写目录（local task需要本地写）</li><li>本地下载资源目录（如udf jar包）</li><li>hdfs会话目录</li><li>hdfs锁文件</li><li>本地会话目录</li><li>hdfs临时表空间<br>可以看到当client启动时，需要创建很多hdfs/本地临时目录供当前会话使用</li></ul><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>hive client启动慢的原因是hadoop集群namenode响应慢。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;场景&quot;&gt;&lt;a href=&quot;#场景&quot; class=&quot;headerlink&quot; title=&quot;场景&quot;&gt;&lt;/a&gt;场景&lt;/h1&gt;&lt;p&gt;HiveClient 有时候会启动非常慢，具体表现是进入hive时当终端输出&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Logging ini</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20190429 ORC表一个有趣的问题</title>
    <link href="http://yoursite.com/2019/04/29/20190429-ORC%E8%A1%A8%E4%B8%80%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2019/04/29/20190429-ORC%E8%A1%A8%E4%B8%80%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2019-04-29T04:57:43.000Z</published>
    <updated>2020-04-11T05:00:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>（其实这并不是bug，只是一个现象，或者说好久没更新这个系列wiki，随便写写）</p><p>现象：</p><p>两张Hive表关联的时候，mapper阶段卡在某个进度很久不动。</p><p>为什么？</p><p>orc是一种压缩率特别高的文件格式。因为其列存储的特点，当某些列存在很多一样的值时，比如NULL，压缩率特别极其高。</p><p>上面关联的一张表，其条数有1亿多，但是文件只有不到30M大小。</p><p>造成什么后果？</p><p>hive根据输入文件大小决定分配多少mapper，上面那个不到30M的文件，很有可能被分给一个mapper，那这个mapper就要处理1亿多条数据。。。</p><p>你也许觉得文件很小，处理很快，实际上，读取文件确实很快，可是别忘了mapper阶段需要对输入文件按照key排序，造成一个mapper要排序1亿多条数据（惊不惊喜？）</p><p>于是，这个mapper stuck了</p><p>我该怎么办呢？</p><p>备选方案1：你可以设置一个mapper处理数据量小小小，比如设置成1M？这样那个30M的文件，会被分给30个mapper处理，可是~如果需要join的另一个表文件很大很大，比如1T，那你算算另一个表要用多少mapper吧（更加惊喜了）</p><p>备选方案2：把那个30M的表换个格式，简单点就用文本格式吧~体积一下子就膨胀了，单个mapper处理的数据量就变小啦。</p><p>备选方案3:  从数据源想想为啥压缩率这么高，是不是数据有问题。</p><p>当你mapper卡住的时候，怎么确定是不是这篇wiki描述的问题？</p><p>step1~查看那个卡住的mapper，看看它在干啥.</p><p>下面这个spilling map output表示mapper在排序（而且，这时候mapper在用硬盘做排序~很慢）</p><p><img src="/uploads/Jietu20200411-125841.jpg" alt></p><p>step2~往上翻翻日志，看看这个mapper读取的文件，然后看看这个文件是不是很小？它对应的表的条数是不是很多？</p><p>如果是的话那就没错了~</p><p><img src="/uploads/Jietu20200411-125934.jpg" alt></p><p>顺便提下，当mapper日志中出现“spilling map output”时，你可以增加mapreduce.task.io.sort.mb这个参数的值，调整排序内存的大小，减少spill disk的次数加快速度</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;（其实这并不是bug，只是一个现象，或者说好久没更新这个系列wiki，随便写写）&lt;/p&gt;
&lt;p&gt;现象：&lt;/p&gt;
&lt;p&gt;两张Hive表关联的时候，mapper阶段卡在某个进度很久不动。&lt;/p&gt;
&lt;p&gt;为什么？&lt;/p&gt;
&lt;p&gt;orc是一种压缩率特别高的文件格式。因为其列存储的特</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20180803 修复metastore挂了重启之后，hiveserver无法重连的问题</title>
    <link href="http://yoursite.com/2018/08/03/20180803-%E4%BF%AE%E5%A4%8Dmetastore%E6%8C%82%E4%BA%86%E9%87%8D%E5%90%AF%E4%B9%8B%E5%90%8E%EF%BC%8Chiveserver%E6%97%A0%E6%B3%95%E9%87%8D%E8%BF%9E%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/08/03/20180803-%E4%BF%AE%E5%A4%8Dmetastore%E6%8C%82%E4%BA%86%E9%87%8D%E5%90%AF%E4%B9%8B%E5%90%8E%EF%BC%8Chiveserver%E6%97%A0%E6%B3%95%E9%87%8D%E8%BF%9E%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2018-08-03T04:56:07.000Z</published>
    <updated>2020-04-11T04:57:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>当外库的metastore服务挂了，手动重启服务，hiveserver无法探测到服务变得可用而自动重连，必须重启hiveserver才能重连到metastore</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><ol><li>为了防止每次连接都要新建metastore client对象（一次tcp连接过程），我们将首次连接好后的metastore client对象缓存起来，以后使用直接拿缓存即可，但是当底层网络连接断开后，缓存中的连接对象变得不可用，这是导致故障的原因。</li><li>Hive开发者为了解决这个问题，实际上使用的metastore client是RetryingMetaStoreClient，该对象是真正的metastore client的动态代理，带有尝试功能，当首次连接失败后，调用metastore client的reconnect方法，重建底层tcp连接。</li><li>外库的metastore client为了限制哪些方法可用，本身已经做了一层jdk动态代理，没法再使用RetryingMetaStoreClient去动态代理一个被代理过的对象，但是可以换一个思路，模仿RetryingMetaStoreClient，增加重试功能。</li></ol><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><blockquote><p>org.apache.hadoop.hive.metastore.HiveForeignMetaStoreProxy</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;</span><br><span class="line">    String methodName = method.getName();</span><br><span class="line"> </span><br><span class="line">    if (NOT_IMPLEMENTED_METHODS.contains(methodName))</span><br><span class="line">        throw new MetaException(methodName + &quot; on a foreign database is not implemented&quot;);</span><br><span class="line"> </span><br><span class="line">    if (!ALLOWED_METHODS.contains(methodName))</span><br><span class="line">        throw new MetaException(methodName + &quot; on a foreign database is not allowed&quot;);</span><br><span class="line"> </span><br><span class="line">    int retry = 0;</span><br><span class="line">    String virtualDBName = (String) args[0];</span><br><span class="line"> </span><br><span class="line">    try &#123;</span><br><span class="line">        while (retry++ &lt; RETRY_COUNT) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                if (MAPTABLE_METHODS.contains(methodName)) &#123;</span><br><span class="line">                    assert args.length != 0;</span><br><span class="line">                    String mappedDBName = foreignMapping.get(virtualDBName);</span><br><span class="line">                    assert mappedDBName != null;</span><br><span class="line">                    args[0] = mappedDBName;</span><br><span class="line"> </span><br><span class="line">                    Object ret = method.invoke(foreignMSC, args);</span><br><span class="line">                    if (ret instanceof Table) &#123;</span><br><span class="line">                        ((Table) ret).setDbName(virtualDBName);</span><br><span class="line">                    &#125;</span><br><span class="line">                    return ret;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    return method.invoke(foreignMSC, args);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                args[0] = virtualDBName;</span><br><span class="line">                if (retry &gt;= RETRY_COUNT) &#123;</span><br><span class="line">                    throw e;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    foreignMSC.reconnect(); // client重连</span><br><span class="line">                    Thread.sleep(1500);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; catch (UndeclaredThrowableException e) &#123;</span><br><span class="line">        throw e.getCause().getCause();</span><br><span class="line">    &#125; catch (InvocationTargetException e) &#123;</span><br><span class="line">        Throwable underlying = e.getCause();</span><br><span class="line">        if ((underlying instanceof TApplicationException) ||</span><br><span class="line">                (underlying instanceof TProtocolException) ||</span><br><span class="line">                (underlying instanceof TTransportException)) &#123;</span><br><span class="line">            throw underlying;</span><br><span class="line">        &#125; else if ((underlying instanceof MetaException) &amp;&amp;</span><br><span class="line">                underlying.getMessage().matches(&quot;JDO[a-zA-Z]*Exception&quot;)) &#123;</span><br><span class="line">            throw underlying;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            throw underlying;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    return null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h1&gt;&lt;p&gt;当外库的metastore服务挂了，手动重启服务，hiveserver无法探测到服务变得可用而自动重连，必须重启hivese</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20180628 Hive On Spark任务初始化失败</title>
    <link href="http://yoursite.com/2018/06/28/20180628-Hive-On-Spark%E4%BB%BB%E5%8A%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E5%A4%B1%E8%B4%A5/"/>
    <id>http://yoursite.com/2018/06/28/20180628-Hive-On-Spark%E4%BB%BB%E5%8A%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E5%A4%B1%E8%B4%A5/</id>
    <published>2018-06-28T04:50:36.000Z</published>
    <updated>2020-04-11T04:53:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>在Hive中指定spark引擎，提交SQL后出现错误：</p><blockquote><p>FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.</p></blockquote><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="1-Hive-on-spark部署方式"><a href="#1-Hive-on-spark部署方式" class="headerlink" title="1. Hive on spark部署方式"></a>1. Hive on spark部署方式</h2><p>根据 Hive on Spark: Getting Started ，Hive on spark在Hive 2.2.0之前和之后的版本部署方式不同，当前生产环境的hive是2.1.1，部署spark环境需要编译spark得到 spark-assembly*.jar，然后将jar包放在hive的lib目录下，就完成了部署。（注意，hive on spark并不需要在hive服务器上安装spark客户端）</p><h2 id="2-当服务器上部署了Spark客户端，并设置了SPARK-HOME环境变量"><a href="#2-当服务器上部署了Spark客户端，并设置了SPARK-HOME环境变量" class="headerlink" title="2. 当服务器上部署了Spark客户端，并设置了SPARK_HOME环境变量"></a>2. 当服务器上部署了Spark客户端，并设置了SPARK_HOME环境变量</h2><p>算法同学需要使用spark客户端提交spark任务，在服务器上部署了spark客户端，版本是2.1.2，并设置了SPARK_HOME环境变量，而2.1.2版本的spark和2.1.1版本的hive是不兼容的（2.1.1版本pom文件中指定spark版本是1.6）</p><p>Hive中使用spark提交任务部分代码片段如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">if (sparkHome != null) &#123; // 如果sparkHome不为空，将sparkHome目录下的bin/spark-submit加入执行参数</span><br><span class="line">  argv.add(new File(sparkHome, &quot;bin/spark-submit&quot;).getAbsolutePath());</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  LOG.info(&quot;No spark.home provided, calling SparkSubmit directly.&quot;);</span><br><span class="line">  argv.add(new File(System.getProperty(&quot;java.home&quot;), &quot;bin/java&quot;).getAbsolutePath());</span><br><span class="line"> </span><br><span class="line">  if (master.startsWith(&quot;local&quot;) || master.startsWith(&quot;mesos&quot;) || master.endsWith(&quot;-client&quot;) || master.startsWith(&quot;spark&quot;)) &#123;</span><br><span class="line">    String mem = conf.get(&quot;spark.driver.memory&quot;);</span><br><span class="line">    if (mem != null) &#123;</span><br><span class="line">      argv.add(&quot;-Xms&quot; + mem);</span><br><span class="line">      argv.add(&quot;-Xmx&quot; + mem);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    String cp = conf.get(&quot;spark.driver.extraClassPath&quot;);</span><br><span class="line">    if (cp != null) &#123;</span><br><span class="line">      argv.add(&quot;-classpath&quot;);</span><br><span class="line">      argv.add(cp);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    String libPath = conf.get(&quot;spark.driver.extraLibPath&quot;);</span><br><span class="line">    if (libPath != null) &#123;</span><br><span class="line">      argv.add(&quot;-Djava.library.path=&quot; + libPath);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    String extra = conf.get(DRIVER_OPTS_KEY);</span><br><span class="line">    if (extra != null) &#123;</span><br><span class="line">      for (String opt : extra.split(&quot;[ ]&quot;)) &#123;</span><br><span class="line">        if (!opt.trim().isEmpty()) &#123;</span><br><span class="line">          argv.add(opt.trim());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 如果sparkHome为空，直接将org.apache.spark.deploy.SparkSubmit加入执行变量，此时，将会使用classpath下的该类提交任务，也就是lib目录下的spark jar</span><br><span class="line">  argv.add(&quot;org.apache.spark.deploy.SparkSubmit&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面的注释清楚的说明了提交spark任务途径的优先级：</p><ul><li>如sparkHome设置，使用sparkHome/bin/spark-submit提交任务。</li><li>如果sparkHome没设置，使用org.apache.spark.deploy.SparkSubmit提交任务，该类在spark-assembly.jar中。</li></ul><p>sparkHome的设置方式是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String sparkHome = conf.get(SPARK_HOME_KEY);</span><br><span class="line">if (sparkHome == null) &#123;</span><br><span class="line">  sparkHome = System.getenv(SPARK_HOME_ENV);</span><br><span class="line">&#125;</span><br><span class="line">if (sparkHome == null) &#123;</span><br><span class="line">  sparkHome = System.getProperty(SPARK_HOME_KEY);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>优先级如下：</p><ol><li>先取conf对象中的 spark.home 设置，可以在hive-site.xml中设置（当前未设置）</li><li>取 SPARK_HOME 环境变量</li><li>取 spark.home系统属性</li></ol><p>所以，当设置了SPARK_HOME环境变量时，hive将使用服务器上部署的spark客户端提交任务，而spark客户端版本2.1.2和hive版本不兼容，导致了错误</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>为了保证hive on spark不受到其他spark版本影响，做了以下双重保证：</p><ol><li>不设置SPARK_HOME环境变量</li><li>假如不小心设置了，在Hive中抹去SPARK_HOME环境变量</li></ol><p>对于1， 将 job工程和机器上/etc/profile.d/z-hadoop.sh中的</p><p>export SPARK_HOME=/home/q/spark/spark-default</p><p>设置删除，取而代之的是，在job工程中封装函数，需要的时候再指定spark_home<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># hive on spark 目前只支持spark1.6 spark-default版本是2.1.2</span><br><span class="line"># 为同时支持hive on spark和直接提交spark任务，封装此函数而不直接export SPARK_HOME</span><br><span class="line">function SPARK_SUBMIT &#123;</span><br><span class="line">    export SPARK_HOME=/home/q/spark/spark-default</span><br><span class="line">    $&#123;SPARK_HOME&#125;/bin/spark-submit $@</span><br><span class="line">    unset SPARK_HOME</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>对于2，在hive-config.sh中，抹去SPARK_HOME</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;在Hive中指定spark引擎，提交SQL后出现错误：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FAILED: Semant</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>序列化与反序列化一[简介]</title>
    <link href="http://yoursite.com/2018/04/26/%E7%A8%8B%E5%BA%8F%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>http://yoursite.com/2018/04/26/%E7%A8%8B%E5%BA%8F%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</id>
    <published>2018-04-26T10:51:43.000Z</published>
    <updated>2018-04-27T11:43:07.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>什么是序列化？</p><blockquote><p>序列化是将程序中的对象或是数据结构转换成持久化的存储（例如文件或缓存）或是可以通过网络传输的字节流，并且转换的结果可以在后续需要的时候，依据预先定义的序列化格式，能够重新构造出原始的对象，这个过程在语义上等同于对于原始对象的clone。</p></blockquote><p>序列化就是把内存中的对象保存起来，反序列化就是把保存的东西给恢复成内存中的对象，本质上跟数据库的作用类似，只不过跟数据库相比，一方面序列化技术能存储更为复杂的数据类型，而数据库只能存储基本数据类型和数据结构，另一方面序列化对程序语言的依赖性更强，需要程序语言定义好序列化的东西是什么（对象的结构），在反序列的时候要知道这个元信息，才能将网络或是硬盘中的字节流转换为相应的对象。</p><p>需要注意的是，在面向对象编程语言中，序列化对象的结果中不包含这个对象所拥有的方法，只能描述对象是什么（field），不能描述对象能干什么（method）。</p><h3 id="常见的序列化技术"><a href="#常见的序列化技术" class="headerlink" title="常见的序列化技术"></a>常见的序列化技术</h3><p>以一个java的Cat类为例，介绍常见序列化技术的基本用法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一些序列化的技术得到大部分编程语言的支持，我称之为编程语言无关的序列化技术，常见的有：</p><h4 id="XML"><a href="#XML" class="headerlink" title="XML"></a>XML</h4><h4 id="Json"><a href="#Json" class="headerlink" title="Json"></a>Json</h4><h4 id="Bson"><a href="#Bson" class="headerlink" title="Bson"></a>Bson</h4><h4 id="MessagePack"><a href="#MessagePack" class="headerlink" title="MessagePack"></a>MessagePack</h4><h4 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h4><p>其他一些序列化的技术只用在特定的编程语言中，以java为例，java中的序列化技术有：</p><h4 id="Java原生序列化"><a href="#Java原生序列化" class="headerlink" title="Java原生序列化"></a>Java原生序列化</h4><p>jdk原生的序列化方案要求被序列化的类实现<code>Serializable</code>接口，该接口是标记接口，并无任何方法，仅用于提示JVM，先挖个坑，以后再学习java原生序列化的原理，先介绍如何使用。</p><p>实现了<code>Serializable</code>接口的<code>Cat</code>类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * serialVersionUID 用于反序列化时验证版本是否一致</span></span><br><span class="line"><span class="comment">     * 如果没有显示指定，jvm执行序列化时根据当前类hash值自动生成一个uid</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span> ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Cat&#123;"</span> + <span class="string">"name='"</span> + name + <span class="string">'\''</span> + <span class="string">", age="</span> + age + <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>序列化的核心类是<code>ObjectOutputStream</code>，序列化代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JdkSerialization</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Cat tom = <span class="keyword">new</span> Cat(<span class="string">"tom"</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="string">"cat.out"</span>);</span><br><span class="line">        ObjectOutputStream oos = <span class="keyword">new</span> ObjectOutputStream(fos);</span><br><span class="line">        oos.writeObject(tom);</span><br><span class="line">        oos.flush();</span><br><span class="line">        oos.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看cat.out: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aced 0005 7372 0011 7365 7269 616c 697a</span><br><span class="line">6174 696f 6e2e 4361 7400 0000 0000 0000</span><br><span class="line">0102 0002 4900 0361 6765 4c00 046e 616d</span><br><span class="line">6574 0012 4c6a 6176 612f 6c61 6e67 2f53</span><br><span class="line">7472 696e 673b 7870 0000 0003 7400 0374</span><br><span class="line">6f6d</span><br></pre></td></tr></table></figure><p>反序列化cat对象: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectInputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JdkDeSerialization</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="string">"cat.out"</span>);</span><br><span class="line">        ObjectInputStream ois = <span class="keyword">new</span> ObjectInputStream(fis);</span><br><span class="line">        Cat tom = (Cat) ois.readObject();</span><br><span class="line">        System.out.println(tom);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果为正确的cat对象: <code>Cat{name=&#39;tom&#39;, age=3}</code></p><h4 id="Kryo"><a href="#Kryo" class="headerlink" title="Kryo"></a>Kryo</h4><p>maven依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kryo&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.0.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>使用:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.Kryo;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.io.Input;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.io.Output;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KryoTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Kryo kryo;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KryoTest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.kryo = <span class="keyword">new</span> Kryo();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Cat cat)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Output output = <span class="keyword">new</span> Output(<span class="keyword">new</span> FileOutputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        kryo.writeObject(output, cat);</span><br><span class="line">        output.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Input input = <span class="keyword">new</span> Input(<span class="keyword">new</span> FileInputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        Cat cat = kryo.readObject(input, Cat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        input.close();</span><br><span class="line">        System.out.println(cat);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Cat tom = <span class="keyword">new</span> Cat(<span class="string">"tom"</span>, <span class="number">3</span>);</span><br><span class="line">        KryoTest kryoTest = <span class="keyword">new</span> KryoTest();</span><br><span class="line">        kryoTest.serialize(tom);</span><br><span class="line">        kryoTest.deserialize();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Hessian2"><a href="#Hessian2" class="headerlink" title="Hessian2"></a>Hessian2</h4><p>maven依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.caucho&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hessian&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.0.7&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>使用</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> serialization;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.caucho.hessian.io.Hessian2Input;</span><br><span class="line"><span class="keyword">import</span> com.caucho.hessian.io.Hessian2Output;</span><br><span class="line"><span class="keyword">import</span> com.caucho.hessian.io.SerializerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hessian2Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SerializerFactory factory = <span class="keyword">new</span> SerializerFactory();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Cat cat)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Hessian2Output output = <span class="keyword">new</span> Hessian2Output(<span class="keyword">new</span> FileOutputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        output.setSerializerFactory(factory);</span><br><span class="line"></span><br><span class="line">        output.writeObject(cat);</span><br><span class="line">        output.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Hessian2Input input = <span class="keyword">new</span> Hessian2Input(<span class="keyword">new</span> FileInputStream(<span class="string">"cat.out"</span>));</span><br><span class="line">        input.setSerializerFactory(factory);</span><br><span class="line">        Cat cat = (Cat) input.readObject();</span><br><span class="line">        System.out.println(cat);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Cat tom = <span class="keyword">new</span> Cat(<span class="string">"tom"</span>, <span class="number">3</span>);</span><br><span class="line">        Hessian2Test test = <span class="keyword">new</span> Hessian2Test();</span><br><span class="line">        test.serialize(tom);</span><br><span class="line">        test.deserialize();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Protobuf"><a href="#Protobuf" class="headerlink" title="Protobuf"></a>Protobuf</h4><h4 id="Thrift"><a href="#Thrift" class="headerlink" title="Thrift"></a>Thrift</h4><h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h3&gt;&lt;p&gt;什么是序列化？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;序列化是将程序中的对象或是数据结构转换成持久化的存储（例如文件或缓存）或是可以通过网络传输的</summary>
      
    
    
    
    <category term="java" scheme="http://yoursite.com/categories/java/"/>
    
    
    <category term="序列化" scheme="http://yoursite.com/tags/序列化/"/>
    
  </entry>
  
  <entry>
    <title>Profile</title>
    <link href="http://yoursite.com/2018/04/24/Profile/"/>
    <id>http://yoursite.com/2018/04/24/Profile/</id>
    <published>2018-04-24T13:00:38.000Z</published>
    <updated>2020-04-11T10:23:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="戴恒宇"><a href="#戴恒宇" class="headerlink" title="戴恒宇"></a>戴恒宇</h1><p>大数据开发工程师，数据仓库工程师</p><h2 id="联系方式"><a href="#联系方式" class="headerlink" title="联系方式"></a>联系方式</h2><p>| Tel: 18512547673 |  Email: <a href="mailto:dhytheone@163.com" target="_blank" rel="noopener">dhytheone@163.com</a>|  <a href="http://dhy.party" target="_blank" rel="noopener">My WebSite</a> : <a href="http://dhy.party" target="_blank" rel="noopener">http://dhy.party</a></p><h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><ul><li>编程语言<ul><li>Java 熟练，掌握常用JVM基础知识</li><li>Shell/Python 日常运维脚本</li></ul></li><li>后端开发<ul><li>熟悉Spring/Spring Boot/MyBatis</li><li>熟悉并发编程</li></ul></li><li>离线数据开发<ul><li>熟悉离线数据仓库建模</li><li>Hive熟练，向社区反馈过多个bug，提交过patch被采纳</li><li>熟悉hadoop mapreduce/hdfs</li></ul></li><li>实时数据开发<ul><li>熟悉Flink、kafka</li><li>建设实时数仓经验及实时数据应用案例</li></ul></li><li>其他<ul><li>有hbase/es/druid使用经验</li><li>理解微服务架构。有使用dubbo经验</li><li>熟练阅读英文文档，奉行clean code，善于解决问题</li></ul></li></ul><h2 id="项目经验"><a href="#项目经验" class="headerlink" title="项目经验"></a>项目经验</h2><ol><li>离线数据开发、Hive二次开发与维护<ul><li>1.1. 离线数仓建设<ul><li>负责BU“用户服务”主题域离线数仓开发，完成自底向上从ods层、dw/mdw层、rpt层报表开发</li><li>数据工具开发：数据同步工具（DB、数仓、Redis之间）、</li></ul></li><li>1.2. Hive二次开发与维护<ul><li>负责部门Hive集群的部署、运维</li><li>二次开发Hive支持多metastore功能，修复日常hive bug，部分反馈社区，<a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hengyu.dai" target="_blank" rel="noopener">Apache Profile: hengyu.dai</a></li></ul></li><li>1.3. Hive服务系统<ul><li>接收Hive Hook传递的SQL，进行SQL解析、鉴权、查询记录归档</li><li>定时检测HiveServer/Metastore server的服务健康校验、发现失败服务自动重启</li></ul></li></ul></li><li>数据系统开发<ul><li>2.1. 元数据系统：全方位管理数据仓库<ul><li>离线数据仓库元数据系统开发，管理离线数仓中的表资源，主要功能包括：表的多维度查询，血缘关系，表的各方面监控。实现的关键词：SQL解析、Hive Hook。</li></ul></li><li>2.2. 即席查询：数据自助查询平台<ul><li>基于Hive Client开发，相比于底层使用hiveserver的开源HUE等产品，大幅提高稳定性。</li><li>开放给产品和运营同学使用，包含表的权限校验、常用数据口径管理</li><li>支持Hive/Presto/Hive on spark多种查询引擎，支持多执行节点</li></ul></li></ul></li><li>实时数仓开发<ul><li>3.1. 从零搭建基于Flink的实时数仓体系<ul><li>负责订单与资金模块的数据源接入，实时数据清洗，实时数据落地与应用（HDFS/HBase）</li><li>清洗用户实时流量数据（用户在不同页面的访问日志）作为算法模型中用户特征数据</li></ul></li><li>3.2. 实时数仓周边服务<ul><li>Redis实时维表：设计维表关联方案，开发Hive与Redis导数工具，开发通用关联维表的工具jar包。</li></ul></li></ul></li><li>数据应用<ul><li>4.1. 用户画像系统：提供统一的用户画像、身份服务<ul><li>数据赋能业务，通过离线数仓+实时计算清洗出的用户特征信息，定义用户画像，提供在线接口输出用户画像，用于各种基于用户身份的营销策略，例如发券、促销、新客优惠等。</li></ul></li><li>4.2. 故障自动定位系统：利用数据辅助业务线发现故障<ul><li>异常监控探测：利用业务系统监控看板，及时发现异常监控指标。</li><li>异常app发现：利用系统的异常trace数据定位app异常。</li><li>故障根源发现：综合多个故障appcode，通过绘制系统间调用拓扑图，从前向后追溯故障根源，给出故障报告。</li></ul></li><li>4.3. 通用数据接口：离线数仓与业务系统之间的数据桥梁<ul><li>将数据仓库中离线计算的数据通过HTTP接口传输给业务系统使用。</li><li>配置化、新增数据时无需开发代码。</li></ul></li><li>4.4. 实时大屏：直播带货效果的实时监控<ul><li>重要活动时，实时计算活动期间的各项运营指标，在BI系统上实时更新，实时数据大屏。</li></ul></li></ul></li></ol><h2 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h2><ul><li>2016-至今 去哪儿网 目的地事业部 数据开发</li></ul><h2 id="教育经历"><a href="#教育经历" class="headerlink" title="教育经历"></a>教育经历</h2><ul><li>2009-2013 中山大学 软件学院 学士学位</li><li>2013-2016 南京大学 计算机科学与技术 硕士学位</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;戴恒宇&quot;&gt;&lt;a href=&quot;#戴恒宇&quot; class=&quot;headerlink&quot; title=&quot;戴恒宇&quot;&gt;&lt;/a&gt;戴恒宇&lt;/h1&gt;&lt;p&gt;大数据开发工程师，数据仓库工程师&lt;/p&gt;
&lt;h2 id=&quot;联系方式&quot;&gt;&lt;a href=&quot;#联系方式&quot; class=&quot;headerli</summary>
      
    
    
    
    
    <category term="我, 其他" scheme="http://yoursite.com/tags/我-其他/"/>
    
  </entry>
  
  <entry>
    <title>20180201 查询LZO表出现额外的NULL和乱码</title>
    <link href="http://yoursite.com/2018/02/01/20180201-%E6%9F%A5%E8%AF%A2LZO%E8%A1%A8%E5%87%BA%E7%8E%B0%E9%A2%9D%E5%A4%96%E7%9A%84NULL%E5%92%8C%E4%B9%B1%E7%A0%81/"/>
    <id>http://yoursite.com/2018/02/01/20180201-%E6%9F%A5%E8%AF%A2LZO%E8%A1%A8%E5%87%BA%E7%8E%B0%E9%A2%9D%E5%A4%96%E7%9A%84NULL%E5%92%8C%E4%B9%B1%E7%A0%81/</id>
    <published>2018-02-01T04:38:12.000Z</published>
    <updated>2020-04-11T04:48:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>以下查询shotel_id字段为null的SQL（实际上表中不存在NULL值）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from dw_www</span><br><span class="line">where dt=&apos;20180115&apos; and shotel_id is null limit 10;</span><br></pre></td></tr></table></figure></p><p>结果是</p><p><img src="/uploads/Jietu20200411-123922.jpg" alt></p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>从以下三个可能原因入手：</p><h2 id="1-LZO压缩导致的乱码和NULL"><a href="#1-LZO压缩导致的乱码和NULL" class="headerlink" title="1. LZO压缩导致的乱码和NULL"></a>1. LZO压缩导致的乱码和NULL</h2><p>最初怀疑是lzo的压缩和解压过程中引入了脏数据，验证过程如下：</p><ol><li>跑一个分区的数据，数据不用lzo压缩，具体的SQL是：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table tmp_lzo_fix_bug</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &apos;,&apos;</span><br><span class="line">as</span><br><span class="line">select xxx -- select语句和生成乱码表的创建过程一样，使用20180115的数据</span><br></pre></td></tr></table></figure></li></ol><p>通过以上SQL，创建了一个用逗号分隔的文本格式的表tmp_lzo_fix_bug，其数据应该与dw_xxx的20180115分区一致。生成后执行SQL验证tmp_lzo_fix_bug是否存在NULL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from tmp_lzo_fix_bug where shotel_id is null limit 10;</span><br></pre></td></tr></table></figure><p>以上SQL查询结果为空，说明tmp_lzo_fix_bug表中不存在为NULL的数据，也即未压缩的文本文件中的数据是正常的。</p><ol start="2"><li>既然未压缩前数据正常，那么接下来验证是不是压缩导致了数据损坏。<br>将20180115分区的lzo文件 000000_0.lzo下载下来，并解压<br><img src="/uploads/Jietu20200411-124214.jpg" alt></li></ol><p>使用lzop对其解压，将结果命名为decompress：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lzop -dv 000000_0.lzo -o decompress</span><br></pre></td></tr></table></figure></p><p>接下来只需要验证decompress文件中是否存在NULL和乱码，就能证明是否是lzo带来的问题，由于解压后的文件非常大，不能直接用vim/cat打开。</p><p>由于在HIVE中，NULL在文本文件中的表示是 \N， 因此只需要看文件中的 \N 即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat decompress | grep -a -F &quot;\N&quot;</span><br></pre></td></tr></table></figure><p>执行以上命令没有任何数据匹配，说明压缩后的lzo文件被解压后没有带来异常数据。</p><p>因此，由LZO压缩导致的乱码和NULL被排除。</p><h2 id="2-由hadoop集群未正确安装lzo压缩-解压软件或者lzo软件版本不对导致"><a href="#2-由hadoop集群未正确安装lzo压缩-解压软件或者lzo软件版本不对导致" class="headerlink" title="2.由hadoop集群未正确安装lzo压缩/解压软件或者lzo软件版本不对导致"></a>2.由hadoop集群未正确安装lzo压缩/解压软件或者lzo软件版本不对导致</h2><p>这个jira反映了类似的问题：<a href="https://issues.apache.org/jira/browse/HIVE-1138" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HIVE-1138</a></p><p>作者最终发现是由于未正确配置 io.compression.codecs 导致了乱码和NULL，验证了这个设置，已经正确设置了lzo codec。</p><p>然而在本地(pg5)上，由上一步验证了pg5上的lzo安装是正确的，并且上面的出现NULL的测试SQL中必须要有 下面的设置，才会出现NULL和乱码的情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br></pre></td></tr></table></figure><p>上述设置是强制HIVE走hadoop集群而不是本地的fetch task直接从HDFS上取数据。因此怀疑是hadoop集群机器上的lzo软件问题。</p><p>如何验证以上猜测？ </p><p>强制SQL使用本地hadoop。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET mapreduce.framework.name=local;</span><br></pre></td></tr></table></figure><p>加入上述设置使用本地hadoop来重新生成一份lzo压缩的表分区数据。</p><p>最终的验证结果是，这样生成的数据仍然存在NULL和乱码。</p><p>因此这个hadoop集群的原因也被排除。</p><h2 id="3-Hive自身的原因，读取lzo文件过程引入脏数据"><a href="#3-Hive自身的原因，读取lzo文件过程引入脏数据" class="headerlink" title="3.Hive自身的原因，读取lzo文件过程引入脏数据"></a>3.Hive自身的原因，读取lzo文件过程引入脏数据</h2><p>受到这篇文章启发：<a href="https://github.com/twitter/hadoop-lzo/issues/49" target="_blank" rel="noopener">https://github.com/twitter/hadoop-lzo/issues/49</a></p><p>这篇文章反映，Hive表没有设置正确的inputformat/outputformat同样会导致这个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">STORED AS INPUTFORMAT &quot;com.hadoop.mapred.DeprecatedLzoTextInputFormat&quot;</span><br><span class="line">      OUTPUTFORMAT &quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&quot;</span><br></pre></td></tr></table></figure><p>如果未正确设置outputformat为org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat，那么HIVE会将lzo的index文件当成数据文件来读取，所以引入了脏数据（即index文件）</p><p>然而查看这个表的格式，发现inputformat和outputformat都是正确设置了的。</p><p>接下来对Hive读取数据的过程调试，发现在实际读取文件时，确实将index文件当做数据来读了：</p><p><img src="/uploads/Jietu20200411-124516.jpg" alt></p><p>尝试设置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from dw_xxx</span><br><span class="line">where dt=&apos;20180115&apos; and shotel_id is null limit 10;</span><br></pre></td></tr></table></figure></p><p>执行上述SQL果然没了NULL和乱码，跟踪读取文件：</p><p><img src="/uploads/Jietu20200411-124632.jpg" alt></p><p>确实没有将index文件加进来。</p><p>那么最终的原因找到了，CombineHiveInputFormat读取文件时错误的将索引文件当成数据文件读入了。</p><h1 id="影响范围与解决方案"><a href="#影响范围与解决方案" class="headerlink" title="影响范围与解决方案"></a>影响范围与解决方案</h1><p>影响范围必须满足一下两个条件：</p><ul><li>表格式是lzo</li><li>表中数据存在索引文件，即在job中使用了lzo_index对数据建了索引</li></ul><p>解决方案是，凡是在读取lzo的时候，都加上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure></p><p>结合以前的lzo无法分割的issue，所有使用lzo表的地方都要加上上述设置</p><h1 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h1><ol><li>既然lzo文件与CombineHiveInputFormat之间有这么多兼容问题，能不能将org.apache.hadoop.hive.ql.io.HiveInputFormat设置为Hive默认的hive.input.format？</li></ol><p>答：不能，CombineHiveInputFormat在绝大部分情况下是有利的，能够合并小文件输入交给一个mapper处理，如果设置为HiveInputFormat，部分job由于有太多小文件，小文件列表会占用过多内存，导致OOM.</p><p>2 . 那遇到LZO的表该怎么办？</p><p>两个办法，要么加上set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;设置，要么将表改为orc格式，orc格式在空间效率和时间效率上都远优于lzo压缩的表。</p><p>因此，强烈建议表的格式采用orc！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;以下查询shotel_id字段为null的SQL（实际上表中不存在NULL值）&lt;br&gt;&lt;figure class=&quot;high</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20180109 mapper超物理内存限制</title>
    <link href="http://yoursite.com/2018/01/09/20180109-mapper%E8%B6%85%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E9%99%90%E5%88%B6/"/>
    <id>http://yoursite.com/2018/01/09/20180109-mapper%E8%B6%85%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E9%99%90%E5%88%B6/</id>
    <published>2018-01-09T04:26:54.000Z</published>
    <updated>2020-04-11T04:33:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p><img src="/uploads/Jietu20200411-122757.jpg" alt><br>显示容器使用的内存(2.1G)已经超过物理内存限制(2G)</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="hadoop中关于mapper-reducer几个内存参数的设置。"><a href="#hadoop中关于mapper-reducer几个内存参数的设置。" class="headerlink" title="hadoop中关于mapper/reducer几个内存参数的设置。"></a>hadoop中关于mapper/reducer几个内存参数的设置。</h2><p>注：以下每个设置都存在两个等效的名字，前者是hadoop0.x和1.x版本的名称，已废弃，后者是hadoop2.x版本的名称，虽然hadoop为了兼容性前者依然有效，但是建议全部使用后者。</p><h2 id="mapred-job-map-memory-mb-或-mapreduce-map-memory-mb"><a href="#mapred-job-map-memory-mb-或-mapreduce-map-memory-mb" class="headerlink" title="mapred.job.map.memory.mb 或 mapreduce.map.memory.mb"></a>mapred.job.map.memory.mb 或 mapreduce.map.memory.mb</h2><p>该参数是Hadoop允许为一个mapper分配的最大内存上限，单位是M, 如果内存不够，将会报类似下面的错误：</p><blockquote><p>Containerpid=container_1406552545451_0009_01_000002,containerID=container_234132_0001_01_000001 is running beyond physical memory limits. Current usage: 569.1 MB of 512 MB physical memory used; 970.1 MB of 1.0 GB virtual memory used. Killing container.</p></blockquote><h2 id="mapred-map-child-java-opts-或-mapreduce-map-java-opts"><a href="#mapred-map-child-java-opts-或-mapreduce-map-java-opts" class="headerlink" title="mapred.map.child.java.opts 或 mapreduce.map.java.opts"></a>mapred.map.child.java.opts 或 mapreduce.map.java.opts</h2><p>该参数是对java进程的内存参数设置，常用来设置一个mapper进程最大的堆内存上限(-Xmx)，使用 ‘-Xmx数字M’设置，如果堆内存不够，将报下面的错误：</p><blockquote><p>Error: java.lang.RuntimeException: java.lang.OutOfMemoryError</p></blockquote><h2 id="mapred-job-reduce-memory-mb-或-mapreduce-reduce-memory-mb"><a href="#mapred-job-reduce-memory-mb-或-mapreduce-reduce-memory-mb" class="headerlink" title="mapred.job.reduce.memory.mb 或  mapreduce.reduce.memory.mb"></a>mapred.job.reduce.memory.mb 或  mapreduce.reduce.memory.mb</h2><p>与mapper类似，该设置是针对reducer的最大内存上限设置</p><h2 id="mapred-reduce-child-java-opts-或-mapreduce-reduce-java-opts"><a href="#mapred-reduce-child-java-opts-或-mapreduce-reduce-java-opts" class="headerlink" title="mapred.reduce.child.java.opts 或 mapreduce.reduce.java.opts"></a>mapred.reduce.child.java.opts 或 mapreduce.reduce.java.opts</h2><p>跟mapper类似，该设置是针对reduce进程最大堆内存上限设置。</p><h1 id="这些设置之间有什么关系？"><a href="#这些设置之间有什么关系？" class="headerlink" title="这些设置之间有什么关系？"></a>这些设置之间有什么关系？</h1><p>上面说了两类设置，最大内存上限和最大堆内存上限。前者是针对container容器的设置，后者是针对java进程的JVM内存设置。java进程在container容器中运行，</p><p>所以理所应当的最大内存上限要大于最大堆内存上限，根据JVM内存模型，(最大内存 - 最大堆内存) 至少要大于 方法区 需要的内存大小。</p><p>建议  JVM最大堆内存 = Container最大内存 * 75% （当然，如果设置这两者相等，而运行中两者的值都超过了需要的内存大小，也不会报错）</p><h1 id="应该如何设置？"><a href="#应该如何设置？" class="headerlink" title="应该如何设置？"></a>应该如何设置？</h1><p>mapreduce.map.memory.mb=5120</p><p>mapreduce.reduce.memory.mb=8192</p><p>mapreduce.map.java.opts=’-Xmx8192M’</p><p>mapreduce.reduce.java.opts=’-Xmx8192M’</p><p>google上述设置发现这两项是CDH版本的Hadoop针对JVM内存的设置，在这里不知道有没有生效。（前面介绍的几项设置是hadoop标准版的设置）</p><p>BTW:<br>除了上面几项设置，还有一项设置能够在单个mapper处理的数据量较大时，加快mapper的速度。</p><p>mapreduce.task.io.sort.mb</p><p>该设置的默认值是100， 单位是M，作用是map阶段spill过程中，用于对mapper结果进行排序的内存大小（归并排序）</p><p>当内存不够时，会使用硬盘辅助排序，这就减慢了速度，必要时可以通过mapper的日志，如果发现在排序上耗时很多，可以将该设置增加到500. </p><p>不常用。</p><h1 id="shuffle报错"><a href="#shuffle报错" class="headerlink" title="shuffle报错"></a>shuffle报错</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:511)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:333)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)</span><br></pre></td></tr></table></figure><p>处理办法</p><blockquote><p>Cause 原因：reduce会在map执行到一定比例启动多个fetch线程去拉取map的输出结果，放到reduce的内存、磁盘中，然后进行merge。当数据量大时，拉取到内存的数据就会引起OOM，所以此时要减少fetch占内存的百分比，将fetch的数据直接放在磁盘上。<br>  有关参数：mapreduce.reduce.shuffle.memory.limit.percent</p></blockquote><p>将mapreduce.reduce.shuffle.memory.limit.percent设置为0.15，执行成功（默认0.25）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/uploads/Jietu20200411-122757.jpg&quot; alt&gt;&lt;br&gt;显示容器使用的内存</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20180109 HS2查询SQL报NPE以及Explain SQL的ClassCastException</title>
    <link href="http://yoursite.com/2018/01/09/20180109-HS2%E6%9F%A5%E8%AF%A2SQL%E6%8A%A5NPE%E4%BB%A5%E5%8F%8AExplain-SQL%E7%9A%84ClassCastException/"/>
    <id>http://yoursite.com/2018/01/09/20180109-HS2%E6%9F%A5%E8%AF%A2SQL%E6%8A%A5NPE%E4%BB%A5%E5%8F%8AExplain-SQL%E7%9A%84ClassCastException/</id>
    <published>2018-01-09T04:12:07.000Z</published>
    <updated>2020-04-11T04:27:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>一段SQL查询语句，在HiveCli上查询正常，在邮件服务器上通过HiveServer2查询报错.</p><p>报错信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">    at org.apache.hadoop.hive.shims.Hadoop23Shims$1.listStatus(Hadoop23Shims.java:134)</span><br><span class="line">    at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:217)</span><br><span class="line">    at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:75)</span><br><span class="line">    at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getSplits(HadoopShimsSecure.java:319)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getCombineSplits(CombineHiveInputFormat.java:425)</span><br><span class="line">    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:532)</span><br><span class="line">    at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:518)</span><br><span class="line">    at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)</span><br><span class="line">    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)</span><br><span class="line">    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268)</span><br><span class="line">    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)</span><br><span class="line">    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:416)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:138)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)</span><br><span class="line">    at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79)</span><br></pre></td></tr></table></figure></p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="1-从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat："><a href="#1-从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat：" class="headerlink" title="1. 从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat："></a>1. 从出错的堆栈上能看到是由于CombineHiveInputFormat引起的问题，尝试改为HiveInputFormat：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure><p>问题消失。</p><h2 id="2-错误是怎么触发的？"><a href="#2-错误是怎么触发的？" class="headerlink" title="2. 错误是怎么触发的？"></a>2. 错误是怎么触发的？</h2><p>下面是对这个bug的调试过程</p><h3 id="首先发现，出错的位置是第4个job，如下图"><a href="#首先发现，出错的位置是第4个job，如下图" class="headerlink" title="首先发现，出错的位置是第4个job，如下图"></a>首先发现，出错的位置是第4个job，如下图</h3><p><img src="/uploads/Jietu20200411-121510.jpg" alt></p><p>使用explain语句查看第4个job是干什么的（直接explain的时候报错，遇到了一个 Correlation Optimizer的bug，将在以后说明这个问题，关闭 Correlation Optimizer以后再执行explain），explain的结果如下：</p><p><img src="/uploads/Jietu20200411-121654.jpg" alt></p><p>发现第4个job是一个MR过程，并且是由group by造成的MR，group by的聚合是count, sum, count, sum。对比原SQL，发现这个MR是最后的一个阶段。因此考虑将出错SQL内部的查询建成一张临时表，在临时表的基础上执行相应的聚合，查看会不会报错.</p><p>遗憾的是，将中间过程拆分后，不管是在HIVECLI 还是HiveServer2上执行都不报错。</p><h3 id="上面的方法失败，只能远程调试HiveServer2了"><a href="#上面的方法失败，只能远程调试HiveServer2了" class="headerlink" title="上面的方法失败，只能远程调试HiveServer2了"></a>上面的方法失败，只能远程调试HiveServer2了</h3><p>首先直接跟踪到出错的位置：<br><img src="/uploads/Jietu20200411-121901.jpg" alt><br>上述while循环这段代码的意思是，扫描输入路径，将不是文件的东西，或者size为0的文件，或者schema以nullscan开头的文件，排除出输入文件。<br>问题就发生在 stat.getPath().toUri().getScheme().equals(“nullscan”) 这个判断中，从截图可以看到，循环的位置 stat.getPath().toUri().getScheme() 值是null，那么调用equals方法就会报NPE错误了。<br>作为对比，看一下使用HiveCli执行到这里时的情况：<br><img src="/uploads/Jietu20200411-122010.jpg" alt></p><p>从上图可以看到，使用HiveCli执行到这里时，stat.getPath().toUri().getScheme() 的值为hdfs。那么这时会判断 “hdfs”.equals(“nullscan”)，不会报错，并且下面的 it.remove()也不会执行到(判断这个文件是正常的文件，加入到当前MR job的)</p><h3 id="到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）"><a href="#到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）" class="headerlink" title="到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）"></a>到这步，就发现一个Hive的bug（虽然这个bug对这个问题没有根本性的影响）</h3><p>首先看一下导致上述bug的 “nullscan” 是什么， nullscan 应该是hadoop标识一个文件处于非正常状态，告诉job不要去扫描读取这个文件的标识。<br><img src="/uploads/Jietu20200411-122121.jpg" alt></p><p>上述发生错误的代码，Hive的本意是，调用listStats(job)方法获取需要扫描的文件列表（super.listStatus方法是Hadoop源码，不在Hive中），然后判断每个文件的schema是不是”nullscan”<br>调试super.listStatus代码，发现传入方法时，获取的文件列表，schema确实是nullscan:</p><p><img src="/uploads/Jietu20200411-122237.jpg" alt></p><p>可是当该方法返回时，hadoop会将nullscan处理掉，</p><p><img src="/uploads/Jietu20200411-122318.jpg" alt></p><p>因此，这里Hive误以为返回值是带nullscan的schema，但是其实经过hadoop处理后，已经不是一个完整的URI了，因此获取schema结果是null，再使用equals方法，就报错。</p><p>以上是导致这个错误的直接原因。</p><h3 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h3><p>这不是这个bug的最终形态，因为如果仅仅在调用equals方法之前，加一个非Null判断，虽然能避免NPE，但无法真正的解决问题，本质上这个问题是一个MR job的输入路径错了，不该是nullscan的文件。</p><p>应该是一个正常的HDFS文件。 </p><p>从上面的截图可以看到，通过HIVECLI执行时，输入文件是 LocatedFileStatus，其完整路径是 hdfs://xxx…. 对应一个HDFS的文件。</p><p>通过HiveServer2执行时，输入文件是nullscan://null/default…..，是一个错误的标识。</p><p>为什么HiveServer2执行时的输入路径不对？</p><p>输入路径是在JobConf对象的properties中的mapred.input.dir配置的，查看hiveserver2提交当前作业的job对象，发现在提交给Hadoop执行时，其输入路径就已经给错了，下面是job配置文件中mapred.input.dir的值</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;一段SQL查询语句，在HiveCli上查询正常，在邮件服务器上通过HiveServer2查询报错.&lt;/p&gt;
&lt;p&gt;报错信息&lt;</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20180105 Hive查询partitioned view报错</title>
    <link href="http://yoursite.com/2018/01/05/20180105-Hive%E6%9F%A5%E8%AF%A2partitioned-view%E6%8A%A5%E9%94%99/"/>
    <id>http://yoursite.com/2018/01/05/20180105-Hive%E6%9F%A5%E8%AF%A2partitioned-view%E6%8A%A5%E9%94%99/</id>
    <published>2018-01-05T04:05:30.000Z</published>
    <updated>2020-04-11T04:08:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from view_xxx where dt=20180101 limit 10;</span><br><span class="line">FAILED: IndexOutOfBoundsException Index: 19, Size: 19</span><br></pre></td></tr></table></figure><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>Hive在做列剪枝的时候，在生成的Operatore Tree中，SEL Operator收集当前Operator需要查询哪些字段，最终告诉底层的Table Scan Operator，只需要查询目标字段，而不是读取表的全部字段，从而完成列剪枝。</p><p>该bug最简单的复现场景是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table foo</span><br><span class="line">(</span><br><span class="line">`a` string</span><br><span class="line">) partitioned by (`b` string)</span><br><span class="line">;</span><br><span class="line"> </span><br><span class="line">create view bar partitioned on (b) as</span><br><span class="line">select a,b from foo;</span><br><span class="line"> </span><br><span class="line">select * from bar;</span><br></pre></td></tr></table></figure><p>select * from bar 生成的原始Operator Tree是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TS[0]</span><br><span class="line">   |</span><br><span class="line">SEL[1]</span><br><span class="line">   |</span><br><span class="line">SEL[2]</span><br><span class="line">   |</span><br><span class="line">FS[3]</span><br></pre></td></tr></table></figure><p>SEL[1]用于确定从bar中实际上选择了哪些字段，在ColumnPrunerProcFactory.java调用bar的Table.getCols().get(index).getName()方法获取第index个列的名称时，由于getCols()不返回分区列，bar的分区列是b，除了b的字段是a，因此在这里返回的list是{“a”}， SEL[1]轮询查询的a,b字段，a的index是0，b的index是1，所以循环到b时，从{“a”}中获取index=1的值，报IndexOutOfBoundsException</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><ol><li>避免踩坑：不建分区视图（建分区不要指定partitioned on，底层表是分区表的话，hive会自动判断分区视图）</li><li>修复bug，新建一个list，将普通字段getCols()和分区字段getPartCols()都添加进去，从这个list中获取字段的name</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20171207 数据校验失败</title>
    <link href="http://yoursite.com/2017/12/07/20171207-%E6%95%B0%E6%8D%AE%E6%A0%A1%E9%AA%8C%E5%A4%B1%E8%B4%A5/"/>
    <id>http://yoursite.com/2017/12/07/20171207-%E6%95%B0%E6%8D%AE%E6%A0%A1%E9%AA%8C%E5%A4%B1%E8%B4%A5/</id>
    <published>2017-12-07T04:00:34.000Z</published>
    <updated>2020-04-11T04:03:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>Hive开启向量化执行时，查询结果不准确<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-- 1. result : 20171205   199107</span><br><span class="line">set hive.vectorized.execution.enabled=true;</span><br><span class="line">select</span><br><span class="line">dt,</span><br><span class="line">sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as bar</span><br><span class="line">from foo</span><br><span class="line">where dt=20171205</span><br><span class="line">group by dt</span><br><span class="line">;</span><br><span class="line"> </span><br><span class="line">-- 2. result : 20171205    0</span><br><span class="line">set hive.vectorized.execution.enabled=false;</span><br><span class="line">select</span><br><span class="line">dt,</span><br><span class="line">sum(case when id =&apos;&apos; or id is null then 1 else 0 end) as bar</span><br><span class="line">from foo</span><br><span class="line">where dt=20171205</span><br><span class="line">group by dt</span><br><span class="line">;</span><br></pre></td></tr></table></figure></p><p>向量化执行打开时，id =’’ or id is null 的判断有bug</p><p>影响范围，同时满足一下条件时存在bug：</p><ol><li>ORC格式的表</li><li>向量化执行打开</li><li>判断条件存在 col = ‘’ or col is null</li></ol><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>关闭向量化执行，hive-site.xml中设置hive.vectorized.execution.enabled=false</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;Hive开启向量化执行时，查询结果不准确&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>20171205 查询LZO压缩的表，Map无法切割大文件</title>
    <link href="http://yoursite.com/2017/12/05/20171205-%E6%9F%A5%E8%AF%A2LZO%E5%8E%8B%E7%BC%A9%E7%9A%84%E8%A1%A8%EF%BC%8CMap%E6%97%A0%E6%B3%95%E5%88%87%E5%89%B2%E5%A4%A7%E6%96%87%E4%BB%B6/"/>
    <id>http://yoursite.com/2017/12/05/20171205-%E6%9F%A5%E8%AF%A2LZO%E5%8E%8B%E7%BC%A9%E7%9A%84%E8%A1%A8%EF%BC%8CMap%E6%97%A0%E6%B3%95%E5%88%87%E5%89%B2%E5%A4%A7%E6%96%87%E4%BB%B6/</id>
    <published>2017-12-05T03:21:53.000Z</published>
    <updated>2020-04-11T03:47:21.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题还原"><a href="#问题还原" class="headerlink" title="问题还原"></a>问题还原</h1><p>查询LZO格式的表，如果单个HDFS文件很大，在读取表阶段，Hive没有切割大文件交给多个mapper处理，导致单个mapper处理很大的文件，速度极慢</p><p>复现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line">select * from ods_test_table where dt=&apos;20171201&apos;;</span><br></pre></td></tr></table></figure></p><p>执行上述select语句，发现只有2个mapper处理:</p><p><img src="/uploads/Jietu20200411-112858.jpg" alt></p><p>查看HDFS文件</p><p><img src="/uploads/Jietu20200411-113505.jpg" alt></p><p>一个很大的数据文件(14.1G), 一个索引文件，一个空的标识文件。</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>先说如何解决该问题的结论：</p><ul><li>set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</li></ul><p>hive.input.format的默认值是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, 将其设置为HiveInputFormat即可。</p><h2 id="1-Hive-如何确定mapper数量？"><a href="#1-Hive-如何确定mapper数量？" class="headerlink" title="1.Hive 如何确定mapper数量？"></a>1.Hive 如何确定mapper数量？</h2><p>Hadoop体系下的MR作业使用InputFormat用来将HDFS文件分片(getSplit()方法)和对单标记录的读取为key-value对(getRecordReader())，所以InputFormat提供了数据切分的功能，inputformat是一个接口，对于每一种文件的输入格式都有一种对应的inputformat实现，如DeprecatedLzoTextInputFormat, OrcInputFormat, RCFileInputFormat,TextInputFormat. 可以认为，每种InputFormat实现的getSplit方法(该参数是JobConf对象)返回的InputSplit[] 数组，数组的元素个数就是读取该文件最终需要的mapper数.</p><h2 id="2-Hive中的两种InputFormat"><a href="#2-Hive中的两种InputFormat" class="headerlink" title="2.Hive中的两种InputFormat"></a>2.Hive中的两种InputFormat</h2><p>Hive默认的InputFormat是 CombineHiveInputFormat，这个inputformat用于当hdfs上存在小文件时，将多个小文件合并起来供一个mapper处理，从而节省mapper资源。一个备选是 HiveInputFormat ，相比前者没有合并小文件的功能。那这两个inputformat和前面说的各种文件类型所实现的各自的inputformat是什么关系呢？以HiveInputFormat为例，查看其getSplits()方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">for (Path dir : dirs) &#123;</span><br><span class="line">      PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);</span><br><span class="line">      // 获取hdfs路径下，文件实际的input format class</span><br><span class="line">      Class&lt;? extends InputFormat&gt; inputFormatClass = part.getInputFileFormatClass();</span><br><span class="line">      // 省略</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">// 省略</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">currentInputFormatClass = inputFormatClass;   // 将实际的input format 设置为当前的inputformat class</span><br><span class="line">// 省略</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">addSplitsForGroup(currentDirs, currentTableScan, newjob,</span><br><span class="line">    getInputFormatFromCache(currentInputFormatClass, job),</span><br><span class="line">    currentInputFormatClass, currentDirs.size()*(numSplits / dirs.length),</span><br><span class="line">    currentTable, result);    // 使用currentInputFormatClass获取splits，该方法最终调用文件类型实际的inputformat.getSplits()方法获取split数量</span><br></pre></td></tr></table></figure><p>通过上面的代码可知，hive中的CombineHiveInputFormat和HiveInputFormat是对各种文件格式的inputformat的包装。最终生效的仍是不同文件格式的inputformat实现。</p><h2 id="3-为什么HiveInputFormat能够切分大的LZO文件为小文件？"><a href="#3-为什么HiveInputFormat能够切分大的LZO文件为小文件？" class="headerlink" title="3.为什么HiveInputFormat能够切分大的LZO文件为小文件？"></a>3.为什么HiveInputFormat能够切分大的LZO文件为小文件？</h2><p>上面的代码显示了HiveInputFormat最终调用文件类型所实现的inputformat的getSplits()方法，lzo的inputformat实现是 DeprecatedLzoTextInputFormat, 查看其getSplits()方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">for (FileSplit fileSplit: splits) &#123;</span><br><span class="line">      Path file = fileSplit.getPath();</span><br><span class="line">      FileSystem fs = file.getFileSystem(conf);</span><br><span class="line"> </span><br><span class="line">      if (!LzoInputFormatCommon.isLzoFile(file.toString())) &#123;</span><br><span class="line">        // non-LZO file, keep the input split as is.</span><br><span class="line">        result.add(fileSplit);</span><br><span class="line">        continue;</span><br><span class="line">      &#125;</span><br><span class="line"> </span><br><span class="line">      // LZO file, try to split if the .index file was found</span><br><span class="line">      LzoIndex index = indexes.get(file);</span><br><span class="line">      if (index == null) &#123;</span><br><span class="line">        throw new IOException(&quot;Index not found for &quot; + file);</span><br><span class="line">      &#125;</span><br><span class="line"> </span><br><span class="line">      // 省略</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面代码的注释很清楚说明了，当.index文件存在，即lzo索引文件存在时，将split文件，因此使用DeprecatedLzoTextInputFormat原生的getSplits()方法，也就是设置hive的inputformat为HiveInputFormat能够拆分lzo文件。</p><h2 id="4-为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？"><a href="#4-为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？" class="headerlink" title="4.为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？"></a>4.为什么Hive默认的CombineHiveInputFormat无法拆分lzo文件？</h2><p>捋一下CombineHiveInputFormat类中的调用关系</p><p>CombineHiveInputFormat#getSplits()  →  CombineHiveInputFormat#getCombineSplits() →  CombineFileInputFormatShim#getSplits() →  HadoopShimsSecure#getSplits()</p><p>上面的CombineFileInputFormatShim是Hive为不同版本的Hadoop提供的称为HadoopShims的机制，用于兼容不同的Hadoop版本，相当于为不同的hadoop版本做一个一致的封装，上面的过程，还在Hive的代码范围内。</p><p>下面进入Hadoop代码：</p><p>HadoopShimsSecure#getSplits() →  org.apache.hadoop.mapred.lib.CombineFileInputFormat#getSplits() →  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits()</p><p>可以看到进入hadoop代码后，最终生效的是hadoop的CombineFileInputFormat的getSplits()方法，hive将拆分任务交给了hadoop。</p><p>继续上面的调用链：</p><p>org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getSplits() → org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#getMoreSplits()</p><p>在getMoreSplits()方法中，调用 org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat#isSplitable() 来判断当前路径的文件是否可切割，那么isSplitable()是如何判断文件是否可切割呢？</p><p><img src="/uploads/Jietu20200411-114112.jpg" alt></p><p>通过 判断当前文件的压缩方式是不是 SplittableCompressionCodec 实例来决定当前文件是否可分!</p><p>lzo的codec是LzopCodec，查看LzopCodec的声明:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public class LzoCodec implements Configurable, CompressionCodec &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并没有实现SplittableCompressionCodec，因此Hadoop判断该文件不可拆分！最终导致只有一个mapper处理.</p><p>一张《hadoop definitiion guide》的图：</p><p><img src="/uploads/Jietu20200411-114220.jpg" alt><br>显示lzo不支持split，但是lzo在实际的实现中，当存在index文件时，是可以拆分的，因此重写了isSplitable()方法，但是只有在调用了lzo实现的getSplits()方法才会发现该文件可以split，而通过hadoop的</p><p>CombineFileInputFormat实现，lzo的LzoCodec被判定为不可切分。</p><h2 id="5-为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？"><a href="#5-为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？" class="headerlink" title="5. 为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？"></a>5. 为什么同为默认的CombineFileInputFormat设置，ORC格式的文件可以被split？</h2><p>跟踪orc文件的读取过程，最终在isSplitable()方法中获取codec时，orc返回结果是null，而对于null，isSplitable()方法直接返回true。。。。。。</p><p><img src="/uploads/Jietu20200411-114315.jpg" alt></p><h1 id="影响范围"><a href="#影响范围" class="headerlink" title="影响范围"></a>影响范围</h1><p>LZO格式的表，单个HDFS文件很大</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>进行下面的设置，使用lzo自身的getSplits和isSplitable实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题还原&quot;&gt;&lt;a href=&quot;#问题还原&quot; class=&quot;headerlink&quot; title=&quot;问题还原&quot;&gt;&lt;/a&gt;问题还原&lt;/h1&gt;&lt;p&gt;查询LZO格式的表，如果单个HDFS文件很大，在读取表阶段，Hive没有切割大文件交给多个mapper处理，导致单个mapp</summary>
      
    
    
    
    
    <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
</feed>
